{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZf-fbj50Kqc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hzlmid2TRUhO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from typing import Set\n",
        "modules: Set[str] = set(sys.modules)\n",
        "%pip install -q pip --upgrade\n",
        "if not ('jupyternotify') in modules:\n",
        "    %pip install -q jupyternotify\n",
        "import numpy as np\n",
        "if np.__version__ != \"1.2.4\":\n",
        "    %pip install -q numpy==1.2.4\n",
        "if not ('folium' in modules):\n",
        "    %pip install -q folium==0.2.1\n",
        "    modules.add('folium')\n",
        "if not ('imgaug' in modules):\n",
        "    %pip install -q imgaug==0.2.6\n",
        "    modules.add('imgaug')\n",
        "if not ('tensorflow_text' in modules):\n",
        "    %pip install -q tensorflow_text==2.9.0\n",
        "    modules.add('tensorflow_text')\n",
        "if not ('seaborn' in modules):\n",
        "    %pip -q install -q seaborn\n",
        "    modules.add('seaborn')\n",
        "if not ('tqdm' in modules):\n",
        "    %pip install -q tqdm\n",
        "    modules.add('tqdm')\n",
        "if not ('matplotlib' in modules):\n",
        "    %pip install -q matplotlib\n",
        "    modules.add('matplotlib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1644854036589
        },
        "id": "MsfMr-Qod_nl",
        "outputId": "f28ea6ce-2590-4e1b-bdcf-c2b78ca6190a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "if (!(\"Notification\" in window)) {\n    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n    Notification.requestPermission(function (permission) {\n        if(!('permission' in Notification)) {\n            Notification.permission = permission;\n        }\n    })\n}\n",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# standart liberies:\n",
        "from typing import List, Optional, Tuple\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import statistics\n",
        "from functools import cache\n",
        "# NOT-standart liberies:\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext jupyternotify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1644854037100
        },
        "id": "SzaglEQ-kp2n",
        "outputId": "89ae89e9-984e-48a8-f7d9-fa82f971a609",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n",
            "Tensorflow version: 2.9.1\n",
            "tf text version: 2.9.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "print(f\"tf text version: {tf_text.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJGjWnfTc6DI",
        "outputId": "78801a62-70a2-4f5c-e611-2fc4ca33e08e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU info:/n\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvidia-smi' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "print('GPU info:/n')\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ZJ0-RZIx6j"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1644854038983
        },
        "id": "2mp2yFTEkp2u",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(0)\n",
        "random.seed(0)\n",
        "tf.keras.backend.set_floatx('float16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1644859230857
        },
        "id": "FqDAqgl5UV58",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "device: str\n",
        "if 'google.colab' in modules:\n",
        "    device = 'colab'\n",
        "else:\n",
        "    device = 'locally'\n",
        "curr_folder = os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "B5rtigmHckyv"
      },
      "source": [
        "# Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bWmh89nVckyw",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "set_size: int = 32\n",
        "batch_size: int = 16\n",
        "learning_rate: float = 0.001\n",
        "\n",
        "max_seq_len: int = 1024\n",
        "num_sets: int = (max_seq_len // set_size) - 1 # Because we dont predict the first set\n",
        "# number of sets in each sequence\n",
        "\n",
        "num_blocks: int = 4\n",
        "d_model: int = 128\n",
        "dff: int = 256\n",
        "num_heads: int = 8\n",
        "dropout_rate: float = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVR1oJRD0NtO"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1644857858611
        },
        "id": "pn1UfAXQwlQX",
        "outputId": "99c146f6-68c5-405a-f8b3-947c0d4df8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30279, 2)\n"
          ]
        }
      ],
      "source": [
        "if device == 'colab':  # If notebook is ran on colab\n",
        "    from google.colab import drive\n",
        "    if not os.path.isdir('/drive'):\n",
        "        drive.mount('/drive')\n",
        "    df: pd.DataFrame = pd.read_csv('/drive/MyDrive/final_project/wikipedia_articles.csv')\n",
        "else:  # If notebook is ran on my laptop\n",
        "    df: pd.DataFrame = pd.read_csv('wiki_data/articles.csv')\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW43lG6iUV6G",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "d3371b8b-a0a2-4318-fb38-e01a491b9d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 30279 data points\n",
            "The length of the longest text IN CHARACTERS is: 141803\n",
            "The length of the shortest text IN CHARACTERS is: 816\n"
          ]
        }
      ],
      "source": [
        "df: pd.Series = df['text']\n",
        "data_list: List[str] = df.to_list()\n",
        "DATA_SIZE = len(data_list)\n",
        "print(f\"There are {DATA_SIZE} data points\")\n",
        "string_lengths: List[int] = [len(data_point) for data_point in data_list]\n",
        "max_string_len = max(string_lengths)\n",
        "print(f\"The length of the longest text IN CHARACTERS is: {max_string_len}\")\n",
        "min_string_len = min(string_lengths)\n",
        "print(f\"The length of the shortest text IN CHARACTERS is: {min_string_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "87Putqz6kp20"
      },
      "source": [
        "## Creating the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KBhC3SPyMi9n"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "bert_tokenizer_params: dict = dict(lower_case=True)\n",
        "VOCAB_SIZE: int = 8192  # Always the same for all models\n",
        "\n",
        "if device == 'colab':  # If notebook is ran on colab\n",
        "    path = '/drive/MyDrive/final_project/vocab.txt'\n",
        "else:  # If notebook is ran on my laptop\n",
        "    path = 'C:/yoni/final_project/model/vocab.txt'\n",
        "\n",
        "\n",
        "reserved_tokens: List[str] = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", \"[MASK]\"]\n",
        "\n",
        "if os.path.exists(path):\n",
        "    with open(path, 'r') as f:\n",
        "        vocab: List[str] = f.read().split()\n",
        "else:\n",
        "    bert_vocab_args: dict = dict(\n",
        "        # The target vocabulary size\n",
        "        vocab_size = VOCAB_SIZE,\n",
        "        # Reserved tokens that must be included in the vocabulary\n",
        "        reserved_tokens=reserved_tokens,\n",
        "        # Arguments for `tf_text.BertTokenizer`\n",
        "        bert_tokenizer_params=bert_tokenizer_params,\n",
        "        # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "        learn_params={},\n",
        "    )\n",
        "    tensor_list: List = [tf.convert_to_tensor(data_point) for data_point in data_list]\n",
        "    data_set: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tensor_list)\n",
        "    # I already ran this code and saved the file to C:/yoni/final_project/model/vocab.txt\n",
        "    vocab: List[str] = tf_text.bert_vocab_from_dataset.bert_vocab_from_dataset(\n",
        "        data_set,\n",
        "        **bert_vocab_args,)\n",
        "    with open('C:/yoni/final_project/model/vocab.txt', 'w') as f:\n",
        "        for token in vocab:\n",
        "            f.write(token + ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IZ5uxlJEoi3",
        "outputId": "50274392-e68b-4d19-9cf4-0d03068dc73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the type of the items in vocab: <class 'str'>\n",
            "the first 15 items in vocab: ['[PAD]', '[UNK]', '[START]', '[END]', '[MASK]', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6']\n",
            " the length of vocab: 7882\n"
          ]
        }
      ],
      "source": [
        "print(f\"the type of the items in vocab: {type(vocab[0])}\")\n",
        "print(f\"the first 15 items in vocab: {vocab[:15]}\")\n",
        "print(f\" the length of vocab: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hndA44nhFw6I",
        "outputId": "f594d4c2-129c-4be8-d5c2-e34775c0680e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " the type of the items in tensor_vocab is: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            " the data type of the tensors in tensor_vocab is: <dtype: 'string'>\n"
          ]
        }
      ],
      "source": [
        "tensor_vocab: List[tf.Tensor] = [tf.convert_to_tensor(token_key, dtype=tf.string) for token_key in vocab]  # dtype = tf.String\n",
        "print(f\" the type of the items in tensor_vocab is: {type(tensor_vocab[0])}\")\n",
        "print(f\" the data type of the tensors in tensor_vocab is: {tensor_vocab[0].dtype}\")\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lD6QCFLAkp2_"
      },
      "source": [
        "## Creating the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HihwIsrKD7C9"
      },
      "outputs": [],
      "source": [
        "lookup_table = tf.lookup.StaticVocabularyTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tensor_vocab,\n",
        "        key_dtype=tf.string,\n",
        "        values=tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64),\n",
        "        value_dtype=tf.int64),\n",
        "    num_oov_buckets=1\n",
        ")\n",
        "tokenizer = tf_text.BertTokenizer(lookup_table, **bert_tokenizer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "A73Lbgg6kp3R"
      },
      "source": [
        "## Tokenizing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mLxV5l562-x2"
      },
      "outputs": [],
      "source": [
        "# START: int = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")  # The value of the start token\n",
        "# END: int = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")  # The value of the end token\n",
        "# starts = tf.cast(tf.Variable([START]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\n",
        "# ends = tf.cast(tf.Variable([END]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\n",
        "starts = tf.constant([2], dtype=tf.int32)\n",
        "ends = tf.constant([3], dtype=tf.int32)\n",
        "pad_int: int = int(tf.argmax(tf.constant(reserved_tokens) == \"[PAD]\"))\n",
        "pad_ten: tf.TensorSpec(dtype=tf.int32, shape=()) = tf.constant([pad_int], dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4gPxVc7tkp3X",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def tokenize_string(text: str) -> tf.Tensor:\n",
        "    \"\"\"Converts string to tensor\"\"\"\n",
        "    ragged: tf.RaggedTensor = tokenizer.tokenize(text)[0, :]\n",
        "    eager: tf.Tensor = ragged.to_tensor(default_value=0, shape=[None, 1])  # 0 is the value of the padding token\n",
        "    sqeezed: tf.Tensor = tf.squeeze(eager, axis=1)\n",
        "    typed: tf.Tensor = tf.cast(sqeezed, tf.int32)\n",
        "    edited: tf.Tensor = tf.concat([starts, typed, ends], axis=0)\n",
        "    return edited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "UzHmzHSukp3a",
        "outputId": "0c7869e0-6d48-4f10-b9b0-84b4344c8876",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenized_data: List[tf.Tensor] = [tokenize_string(data_point) for data_point in data_list] \n",
        "\n",
        "# tqdm is a progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "CNEdwxWFkp3d",
        "outputId": "36c5f35a-2398-442f-9736-bf7a08d4529c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30279\n",
            "(670,)\n",
            "tf.Tensor([   2 1011 7670   57   18 6423  617   33   61   44], shape=(10,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(len(tokenized_data))\n",
        "print(tokenized_data[0].shape)\n",
        "print(tokenized_data[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "D77Pm-iGkp3j",
        "outputId": "d08952c8-f333-4f21-8203-c49c6c2b7d9d",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25315\n",
            "178\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "lengths_tokenized: List[int] = [text.shape[0] for text in tokenized_data]\n",
        "print(max(lengths_tokenized))\n",
        "print(min(lengths_tokenized))\n",
        "sns.displot(lengths_tokenized);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EtjxgLWMkp3l"
      },
      "source": [
        "### chunk too long texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MubdHBEMkJNC"
      },
      "outputs": [],
      "source": [
        "def chunk_tensor(tensor: tf.Tensor, max_len: int = max_seq_len) -> List[tf.Tensor]:\n",
        "    \"\"\"Splits 1d tensor to chunks (1d tensors) of maximum size: max_len\"\"\"\n",
        "    return [tensor[i*max_len:(i+1)*max_len] for i in range(tensor.shape[0] // max_len)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DgU2Or2kp3m",
        "outputId": "292cd0f6-29ed-4cba-e627-4691f18955a7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72513\n",
            "(1024,)\n"
          ]
        }
      ],
      "source": [
        "chunked_data: List[tf.Tensor] = []\n",
        "for tensor in tokenized_data:\n",
        "    chunks = chunk_tensor(tensor, max_seq_len)\n",
        "    for chunk in chunks:\n",
        "        chunked_data.append(chunk)\n",
        "DATA_SIZE: int = len(chunked_data)\n",
        "print(DATA_SIZE)\n",
        "print(chunked_data[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Z2_P2Lnke8"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUOVoGBfkp3p",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def pad(tensor: tf.Tensor, pad_int: int) -> tf.Tensor:\n",
        "    \"\"\"Pads the tensor to the length of the longest text in the data set\"\"\"\n",
        "    padded: tf.Tensor = tf.pad(tensor=tensor, paddings=[[pad_int, max_seq_len - tensor.shape[0]]], mode='CONSTANT', constant_values=0)\n",
        "    # 0 is the padding token\n",
        "    return padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRa9_ryHkp3p",
        "outputId": "3f8d82e6-6067-4bc3-a34c-54f6001636e2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([   2    7 3852 ...    7   44 3909], shape=(1024,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "padded_data: List[tf.Tensor] = [pad(text, pad_int) for text in chunked_data]\n",
        "print(padded_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gbnub0UZkp3s"
      },
      "source": [
        "## Train test val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dz5c-Gskp3t",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "random.shuffle(padded_data)\n",
        "train_size: int = int(DATA_SIZE * 0.8)\n",
        "val_test_size: int = int(DATA_SIZE * 0.1)  # Both validation and test get 10% of the data\n",
        "\n",
        "train_tokenized: List[tf.Tensor] = padded_data[:train_size]\n",
        "val_tokenized: List[tf.Tensor] = padded_data[train_size:(val_test_size + train_size)]\n",
        "test_tokenized: List[tf.Tensor] = padded_data[(train_size + val_test_size):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYF0ns5Rkp3t",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def list_to_dataset(tokenized_list: List[tf.Tensor]) -> tf.data.Dataset:\n",
        "    \"\"\"Converts a list of tokenized texts after all preprocessing to a tf.data.Dataset\"\"\"\n",
        "    dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tokenized_list)\n",
        "    batched: tf.data.Dataset = dataset.batch(batch_size)\n",
        "    return batched\n",
        "\n",
        "train_set: tf.data.Dataset = list_to_dataset(train_tokenized)\n",
        "val_set: tf.data.Dataset = list_to_dataset(val_tokenized)\n",
        "test_set: tf.data.Dataset = list_to_dataset(test_tokenized)\n",
        "\n",
        "list_train_set = list(train_set)\n",
        "list_val_set = list(val_set)\n",
        "list_test_set = list(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzS72rcIvL0N"
      },
      "source": [
        "## Clear memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlkhQigKvOQT"
      },
      "outputs": [],
      "source": [
        "del train_tokenized, test_tokenized, val_tokenized\n",
        "del padded_data, chunked_data, tokenized_data, data_list, df, lengths_tokenized, \n",
        "del string_lengths, lookup_table\n",
        "del train_set, val_set, test_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vBLynXu0QI1"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9qUAZ00xiVd"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_V1b_16-kp3v"
      },
      "source": [
        "The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "$${PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$${PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "\n",
        "where $d_{model}$ is the model dimension, $pos$ is the position and $i$ is the index of the embedding.\n",
        "this is taken from the paper: attention is all you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx4hze4k0yls"
      },
      "outputs": [],
      "source": [
        "def create_positional_encoding(max_len: int, d_model: int) -> tf.Tensor:\n",
        "    \"\"\"Returns the positional encoding for a given a maximal sequence length and model dimension.\n",
        "    inputs: max_len: int, d_model: int\n",
        "    returns: tf.Tensor of shape (1, max_len, d_model) and dtype tf.keras.backend.floatx()\n",
        "    The 1 is for the batch dimension, the place in the batch dimension does not matter\"\"\"\n",
        "\n",
        "    def get_angles(positions: np.ndarray, timestamps: np.ndarray, d_model: int) -> np.ndarray:\n",
        "        \"\"\"Returns the angle in radians for given positions, timestamps and the dimension of the model\n",
        "        input: positions: np.ndarray of shape (max_len, 1), timestamps: np.ndarray of shape (1, d_model), d_model: int\n",
        "        output: np.ndarray of shape (max_len, d_model)\"\"\"\n",
        "        if tf.keras.backend.floatx() == \"float32\":\n",
        "            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float32(d_model)))\n",
        "        else:\n",
        "            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float16(d_model)))\n",
        "\n",
        "        return positions * angle_rates\n",
        "    \n",
        "    angle_rads = get_angles(np.arange(max_len)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)  # (max_len, d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # (max_len, d_model)\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # (max_len, d_model)\n",
        "\n",
        "    pos_encode = angle_rads[np.newaxis, ...]  # (1, max_len, d_model)\n",
        "\n",
        "    return tf.cast(pos_encode, dtype=tf.keras.backend.floatx())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ib5F3hnxrE9"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TXw70UlzcVi"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w68l26GzrqG"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQkjgUVVckzJ"
      },
      "outputs": [],
      "source": [
        "def create_masks(inp: tf.Tensor, tar: tf.Tensor, pad_ten: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"Creates all the masks needed for the model\n",
        "        input: inp: tf.Tensor of shape (batch_size, seq_len), tar: tf.Tensor of shape (batch_size, set_size)\n",
        "        Returns: Tuple of (padding_mask, look_ahead_mask)\n",
        "        padding_mask, look_ahead_mask: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n",
        "        \n",
        "        def create_padding_mask(seq: tf.Tensor) -> tf.Tensor:\n",
        "                \"\"\"Returns a padding mask for the given sequence.\n",
        "                input: seq: tf.Tensor of shape (batch_size, seq_len)\n",
        "                Returns: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n",
        "                seq = tf.cast(tf.math.equal(seq, pad_ten), tf.keras.backend.floatx())  \n",
        "                # For every item in the sequence, 1 if it is a padding token, 0 if it is not \n",
        "\n",
        "                # add extra dimensions to add the padding\n",
        "                \n",
        "                return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "        \n",
        "        # Encoder padding mask\n",
        "        padding_mask: tf.Tensor = create_padding_mask(inp)  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "        # Used in the 1st attention block in the decoder.\n",
        "        # It is used to pad and mask future tokens in the input received by\n",
        "        # the decoder.\n",
        "        set_size: int = tar.shape[1]\n",
        "\n",
        "        @cache\n",
        "        def create_look_ahead_mask(set_size: int) -> tf.Tensor:\n",
        "                mask = 1 - tf.linalg.band_part(tf.ones((set_size, set_size)), -1, 0)\n",
        "                mask = tf.cast(mask, dtype=tf.keras.backend.floatx())\n",
        "                return mask  # (seq_len, seq_len)\n",
        "\n",
        "        look_ahead_mask = create_look_ahead_mask(set_size)  # (seq_len, seq_len)\n",
        "        dec_target_padding_mask = create_padding_mask(tar)  # (batch_size, 1, 1, seq_len)\n",
        "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "        return padding_mask, look_ahead_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KHKaxz3xumc"
      },
      "source": [
        "## Layers and blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model: int, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.scale: tf.TensorSpec(shape=(), dtype=tf.keras.backend.floatx())\n",
        "        # scale = 1 / sqrt(d_model)\n",
        "        self.scale = tf.math.pow(tf.cast(d_model, tf.keras.backend.floatx()), -0.5)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
        "        \"\"\"Scaled Dot-Product Attention\n",
        "        input: \n",
        "        q: tf.Tensor of shape (batch_size, seq_len, d_model), \n",
        "        k: tf.Tensor of shape (batch_size, seq_len, d_model), \n",
        "        v: tf.Tensor of shape (batch_size, seq_len, d_model), \n",
        "        mask: Optional[tf.Tensor] of shape (batch_size, 1, 1, seq_len)\n",
        "        output: tf.Tensor of shape (batch_size, seq_len, d_model)\"\"\"\n",
        "        matmul_qk: tf.Tensor = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scaled_attention_logits: tf.Tensor = matmul_qk * self.scale  # (..., seq_len_q, seq_len_k)\n",
        "        # matmul_qk / sqrt(d_model)\n",
        "\n",
        "        # Masking\n",
        "        if mask is not None:\n",
        "            # noinspection PyTypeChecker\n",
        "            if tf.keras.backend.floatx() == 'float16':\n",
        "                # tf.float16.min is minus infinity\n",
        "                scaled_attention_logits += (mask * tf.float16.min)  # changed from -1e9 to prevent nan's\n",
        "            else:\n",
        "                scaled_attention_logits += (mask * -1e9) \n",
        "\n",
        "        # Normalize\n",
        "        attention_weights = self.softmax(scaled_attention_logits)\n",
        "        # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "        # Output\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 10, 128), dtype=float16, numpy=\n",
              "array([[[[0.669 , 0.2354, 0.877 , ..., 0.3428, 0.0547, 0.8877],\n",
              "         [0.669 , 0.2354, 0.877 , ..., 0.3428, 0.0547, 0.8877],\n",
              "         [0.669 , 0.2354, 0.877 , ..., 0.3428, 0.0547, 0.8877],\n",
              "         ...,\n",
              "         [0.669 , 0.2354, 0.877 , ..., 0.3428, 0.0547, 0.8877],\n",
              "         [0.669 , 0.2354, 0.877 , ..., 0.3428, 0.0547, 0.8877],\n",
              "         [0.669 , 0.2354, 0.877 , ..., 0.3428, 0.0547, 0.8877]]]],\n",
              "      dtype=float16)>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_sdpa = ScaledDotProductAttention(d_model=128)\n",
        "q = tf.random.uniform((1, 10, 128), dtype=tf.keras.backend.floatx())\n",
        "k = tf.random.uniform((1, 10, 128), dtype=tf.keras.backend.floatx())\n",
        "v = tf.random.uniform((1, 10, 128), dtype=tf.keras.backend.floatx())\n",
        "mask = tf.random.uniform((1, 1, 1, 10), dtype=tf.keras.backend.floatx())\n",
        "test_sdpa(q, k, v, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0I9SZI0kp31",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class MyMultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"U can use the built-in tf.keras.layers.multihead_attention but is caused a bug for me\"\"\"\n",
        "    def __init__(self, num_heads: int, d_model: int, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        if d_model % num_heads != 0:\n",
        "            raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\")\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.sdpa = ScaledDotProductAttention(d_model)\n",
        "\n",
        "    def split_heads(self, x: tf.Tensor, batch_size: int) -> tf.Tensor:\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v_k: tf.Tensor, q: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"inputs:\n",
        "        v_k: tf.Tensor of shape (batch_size, seq_len, d_model) in self attention keys and values are the same\n",
        "        q: tf.Tensor of shape (batch_size, seq_len, d_model)\n",
        "        mask: Optional[tf.Tensor] of shape (batch_size, seq_len)\"\"\"\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q: tf.Tensor = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k: tf.Tensor = self.wk(v_k)  # (batch_size, seq_len, d_model)\n",
        "        v: tf.Tensor = self.wv(v_k)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q: tf.Tensor = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k: tf.Tensor = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v: tf.Tensor = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape should be (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape should be (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention = self.sdpa(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n",
        "         # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "          # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointWiseFeedForwardNetwork(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model: int, dff: int, **kwargs): \n",
        "        super().__init__(**kwargs)\n",
        "        self.layer1 = tf.keras.layers.Dense(dff, activation='relu')  # (batch_size, seq_len, dff)\n",
        "        self.layer2 = tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Gets tensor of shape (batch_size, seq_len, d_model) and dtype tf.keras.beckend.floatx()\n",
        "        Returns tensor of shape (batch_size, seq_len, d_model) and dtype tf.keras.beckend.floatx()\"\"\"\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mEP3-Jx2n39"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model: int, num_heads: int, dff: int, drop_out_rate: float, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n",
        "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n",
        "\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_out_rate)\n",
        "\n",
        "    def call(self, x: tf.Tensor, training: bool, mask: tf.Tensor) -> tf.Tensor:\n",
        "        \n",
        "        attn_output = self.mha(x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout(attn_output, training=training)  # (batch_size, input_seq_len, d_model)\n",
        "        out1 = self.layer_norm(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout(ffn_output, training=training)  # (batch_size, input_seq_len, d_model)\n",
        "        out2 = self.layer_norm(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJWFrv0g281G"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n",
        "\n",
        "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n",
        "\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor, training):\n",
        "        # enc_output.shape should be (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1 = self.mha(x, x, look_ahead_mask)  # (batch_size, set_size, d_model)\n",
        "        attn1 = self.dropout(attn1, training=training)\n",
        "        out1 = self.layer_norm(attn1 + x)\n",
        "\n",
        "        attn2 = self.mha(enc_output, out1, padding_mask)  # (batch_size, set_size, d_model)\n",
        "        attn2 = self.dropout(attn2, training=training)\n",
        "        out2 = self.layer_norm(attn2 + out1)  # (batch_size, set_size, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, set_size, d_model)\n",
        "        ffn_output = self.dropout(ffn_output, training=training)\n",
        "        out3 = self.layer_norm(ffn_output + out2)  # (batch_size, set_size, d_model)\n",
        "\n",
        "        return out3\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdlS1kfM3k5d"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, pos_encoding: tf.Tensor, num_blocks: int, d_model: int, num_heads: int, dff: int, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_blocks = num_blocks\n",
        "        self.pos_encoding = pos_encoding\n",
        "\n",
        "        self.enc_blocks = [EncoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n",
        "        # the encoder \n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x: tf.Tensor, training, mask: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding position encoding.\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.keras.backend.floatx()))\n",
        "        \n",
        "        x += self.pos_encoding[:, :seq_len, :]  # (batch_size, input_seq_len, d_model)\n",
        "        x = self.dropout(x, training=training)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.enc_blocks[i](x, training, mask)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLE8js6O3p5-"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, pos_encoding, num_blocks: int, d_model: int, num_heads: int, dff: int,\n",
        "                 vocab_size: int, rate: float, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_blocks = num_blocks\n",
        "        self.pos_encoding = pos_encoding\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.dec_layers = [DecoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, training: bool,\n",
        "             look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, set_size, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.keras.backend.floatx()))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.dec_layers[i](x=x, enc_output=enc_output, look_ahead_mask=look_ahead_mask, \n",
        "                                   padding_mask=padding_mask, training=training)\n",
        "\n",
        "        # x.shape should be (batch_size, set_size, d_model)\n",
        "        return x\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-xe3K7LyD2l"
      },
      "source": [
        "## The full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NLhyE0T3tUs"
      },
      "outputs": [],
      "source": [
        "class SeTransformer(tf.keras.Model):\n",
        "    \"\"\"The base architecture of my models in this project.\"\"\"\n",
        "    def __init__(self, num_blocks: int, d_model: int, num_heads: int, dff: int,\n",
        "                 vocab_size: int, max_len: int, rate: float, pad_int: int, **kwargs):\n",
        "        super().__init__(**kwargs)  # calls tf.keras.Model's __init__ method\n",
        "        self.pad_int = pad_int\n",
        "        pos_encoding = create_positional_encoding(max_len, d_model)\n",
        "\n",
        "        self.encoder = Encoder(pos_encoding, num_blocks, d_model, num_heads, dff, rate)\n",
        "        self.decoder = Decoder(pos_encoding, num_blocks, d_model, num_heads, dff, vocab_size, rate)\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis = -1)  # there are {vocab_size} categories in the output\n",
        "\n",
        "    def call(self, inputs: List[tf.Tensor], training: bool) -> tf.Tensor:\n",
        "        inp, tar = inputs\n",
        "        # inp.shape should be (batch_size, max_seq_len) or (1, max_seq_len)\n",
        "        # tar.shape should be (batch_size, set_size) or (1, set_size)\n",
        "        padding_mask, look_ahead_mask = create_masks(inp, tar, self.pad_int)\n",
        "        x = self.embedding(inp)  # (batch_size, max_seq_len, d_model)\n",
        "        enc_output = self.encoder(x, training, padding_mask)  # (batch_size, max_seq_len, d_model)\n",
        "        \n",
        "        # dec_output.shape should be (batch_size, set_size, d_model)\n",
        "        dec_output = self.decoder(tar, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "        final_output = self.dense(dec_output)\n",
        "        # final_output.shape should be (batch_size, set_size, vocab_size)\n",
        "        softmaxed = self.softmax(final_output)\n",
        "        # softmaxed.shape should be (batch_size, set_size, vocab_size)\n",
        "        return softmaxed\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WZ41cb-1B_f",
        "outputId": "6ee5ceb5-2868-4ea0-b38a-5948f88a3704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 8, 7000)\n",
            "tf.Tensor(\n",
            "[[0.00012004 0.000142   0.0001636  ... 0.0001419  0.00017    0.0001329 ]\n",
            " [0.0001242  0.0001345  0.000156   ... 0.0001343  0.0001618  0.0001264 ]\n",
            " [0.0001252  0.0001411  0.0001599  ... 0.000142   0.0001662  0.0001293 ]\n",
            " ...\n",
            " [0.0001234  0.0001405  0.000162   ... 0.0001334  0.0001504  0.0001281 ]\n",
            " [0.000125   0.0001382  0.0001541  ... 0.0001297  0.0001563  0.0001316 ]\n",
            " [0.0001272  0.0001371  0.0001645  ... 0.0001369  0.0001576  0.000133  ]], shape=(8, 7000), dtype=float16)\n",
            "(1, 8, 7000)\n",
            "tf.Tensor(\n",
            "[[0.0001246 0.0001408 0.0001632 ... 0.0001395 0.000167  0.000132 ]\n",
            " [0.0001281 0.000141  0.0001625 ... 0.0001411 0.000167  0.0001312]\n",
            " [0.0001271 0.0001391 0.0001616 ... 0.0001415 0.000167  0.0001314]\n",
            " ...\n",
            " [0.0001261 0.0001366 0.0001599 ... 0.0001361 0.0001624 0.0001323]\n",
            " [0.0001279 0.0001377 0.0001607 ... 0.0001379 0.0001599 0.0001339]\n",
            " [0.0001292 0.0001377 0.0001639 ... 0.0001403 0.0001599 0.0001335]], shape=(8, 7000), dtype=float16)\n"
          ]
        }
      ],
      "source": [
        "sample_transformer = SeTransformer(\n",
        "    num_blocks=2, d_model=32, num_heads=4, dff=128,\n",
        "    vocab_size=7000,\n",
        "    max_len=1200, pad_int=0, rate=0.1)\n",
        "\n",
        "temp_input = tf.random.uniform((1, 1000), dtype=tf.int32, minval=0, maxval=6999)\n",
        "temp_target = tf.random.uniform((1, 8), dtype=tf.int32, minval=0, maxval=6999)\n",
        "\n",
        "train_out = sample_transformer([temp_input, temp_target], training=True)\n",
        "\n",
        "print(train_out.shape)  # (batch_size, set_size, vocab_size)\n",
        "print(train_out[0][0])\n",
        "\n",
        "non_train_out = sample_transformer([temp_input, temp_target], training=False)\n",
        "print(non_train_out.shape)  # (batch_size, set_size, vocab_size)\n",
        "print(non_train_out[0][0])\n",
        "\n",
        "del sample_transformer, temp_input, temp_target, train_out, non_train_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hFxiRfCekp38"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8jsueeI0kp3-"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ8vqihqkp3-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\yoni\\final_project\\model\\create_models.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/yoni/final_project/model/create_models.ipynb#ch0000001?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate, epsilon\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mepsilon())\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate, epsilon=tf.keras.backend.epsilon())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O783T1xMckzR"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uAxXGBtkp4A",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "model = SeTransformer(\n",
        "    num_blocks=num_blocks,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=max_seq_len,\n",
        "    rate=dropout_rate,\n",
        "    pad_int=pad_int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gjCRldwckzR"
      },
      "source": [
        "## Training helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR6qYltHhh_t"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n",
        "                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)])\n",
        "def train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred: tf.Tensor = model([inp, outp], training=True)\n",
        "        loss_val: tf.Tensor = tf.keras.losses.sparse_categorical_crossentropy(y_true = outp, y_pred = pred)\n",
        "    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    return loss_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NpdkcUkckzT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def validate(batch) -> float:\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n",
        "                                  tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)])\n",
        "    def val_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n",
        "        pred = model([inp, outp], training=False)\n",
        "        return tf.keras.losses.sparse_categorical_crossentropy(y_true = outp, y_pred = pred)\n",
        "    per_generation_loss: List[float] = []\n",
        "    for i in range(num_sets):\n",
        "        # The input is of size set_size-TAKE_TO_ACCOUNT\n",
        "        already_predicted: int = i * (set_size + 1)\n",
        "        start_from: int = max(0, already_predicted - max_seq_len)\n",
        "        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n",
        "        outp: tf.Tensor = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n",
        "        loss_val: tf.Tensor = val_step(inp, outp)\n",
        "        float_loss: float = loss_val.numpy().item()\n",
        "        per_generation_loss.append(float_loss)\n",
        "    return statistics.mean(per_generation_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZsEGz_vckzT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def check_point(folder_path: str, model: SeTransformer, val_loss: float, train_loss: float):\n",
        "    \"\"\"Saves the model at the end of each epoch\"\"\"\n",
        "    path: str = f\"{folder_path}/\"\n",
        "    tf.keras.models.save_model(model = model, filepath = path, save_format='tf')\n",
        "    with open(\"exmiperments.csv\", 'r') as f:\n",
        "        lines = f.readlines()[:-1]  # All but the last line\n",
        "    with open(\"exmiperments.csv\", 'w') as f:\n",
        "        c_writer = csv.writer(f, delimiter=',')\n",
        "        for line in lines:\n",
        "            c_writer.writerow(line)\n",
        "    fields = [date, val_loss, train_loss, set_size, num_blocks, d_model, dff,\n",
        "              num_heads, learning_rate, max_seq_len, dropout_rate, batch_size]\n",
        "    with open('experiments.csv', 'a') as f:\n",
        "        writer = csv.writer(f, delimiter=',')\n",
        "        writer.writerow(fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iuce_A94ckzU",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "date: str = datetime.datetime.now().strftime('%m%d-%H%M')\n",
        "if device == 'colab':\n",
        "    folder_path: str = \"/drive/MyDrive/final_project/checkpoints/\"\n",
        "else:\n",
        "    folder_path: str = \"C:/yoni/final_project/model/checkpoints/\"\n",
        "check_points_path = f\"{folder_path}{date}\"\n",
        "if not os.path.isdir(folder_path):\n",
        "    os.mkdir(folder_path)\n",
        "if not os.path.isdir(check_points_path):\n",
        "    os.mkdir(check_points_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00auA3LZwKvf"
      },
      "outputs": [],
      "source": [
        "train_loss, val_loss = float('inf'), float('inf')\n",
        "best_val_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWKTBdzkckzV",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "exp_path = \"/drive/MyDrive/final_project/experiments.csv\" if device == 'colab' else \"C:/yoni/final_project/model/experiments.csv\"\n",
        "if not os.pathexist(exp_path)\n",
        "with open(exp_path, 'a') as f:\n",
        "    fields = [date, val_loss, train_loss, set_size, num_blocks, d_model, dff,\n",
        "              num_heads, learning_rate, max_seq_len, dropout_rate, batch_size]\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN8rmbrNW0-Z"
      },
      "source": [
        "# The actual training loop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHktVH98kp4B",
        "outputId": "bd16ea4d-22d7-42d1-f603-27c4a82a6508",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "EPOCHS: int = 1000  # Train until the cloud disconnects or the model stops improving\n",
        "per_epoch_train_loss: List[float] = []\n",
        "per_epoch_val_loss: List[float] = []\n",
        "for epoch in range(EPOCHS):\n",
        "    per_batch_train_loss: List[float] = []\n",
        "    per_batch_val_loss: List[float] = []\n",
        "    for batch_num, train_batch in tqdm.tqdm(enumerate(list_train_set)):  # tqdm is a progress bar\n",
        "        per_generation_loss: List[float] = []\n",
        "        for i in range(num_sets):\n",
        "            # The input is of size set_size-TAKE_TO_ACCOUNT\n",
        "            already_predicted: int = i * (set_size + 1)\n",
        "            start_from: int = max(0, already_predicted - max_seq_len)\n",
        "            inp: tf.Tensor = train_batch[:, start_from:(i + 1) * set_size]\n",
        "            outp: tf.Tensor = train_batch[:, (i + 1) * set_size:(i + 2) * set_size]\n",
        "            loss_val: tf.Tensor = train_step(inp, outp)\n",
        "            train_loss: float = loss_val.numpy().item()\n",
        "            per_generation_loss.append(train_loss)\n",
        "            \n",
        "        per_batch_train_loss.append(statistics.mean(per_generation_loss))\n",
        "        if batch_num % 8 == 0:  # 8 is number of training batches/number of val batches\n",
        "            next_val_batch: tf.Tensor = list_val_set[batch_num // 8]\n",
        "            val_loss: float = validate(next_val_batch)\n",
        "            per_batch_val_loss.append(val_loss)\n",
        "    per_epoch_train_loss.append(statistics.mean(per_batch_train_loss))\n",
        "    per_epoch_val_loss.append(statistics.mean(per_batch_val_loss))\n",
        "    if per_epoch_val_loss[-1] > per_epoch_val_loss[-2]:\n",
        "        check_point(check_points_path, model, per_epoch_val_loss[-1], per_epoch_train_loss[-1])\n",
        "        print(\"Validation loss increased. Stopped training\")\n",
        "        %notify(\"Validation loss increased. Stopped training\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1gVcN35kp4C",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def plot_loss(loss: List[float], title: str):\n",
        "    plt.plot(loss)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(per_epoch_train_loss, \"Training Loss\")\n",
        "plot_loss(per_epoch_val_loss, \"Validation Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss = statistics.mean([validate(test_batch) for test_batch in list_test_set])\n",
        "print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5vBLynXu0QI1"
      ],
      "name": "create_models.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c667eb41e29edd16195f2712b5916a29b36d4e6f17fe7c0a7edbb7a73b8ba8a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
