{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZf-fbj50Kqc"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hzlmid2TRUhO"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "modules: set = set(sys.modules)\n",
    "if not ('jupyternotify') in modules:\n",
    "    %pip install -q jupyternotify\n",
    "import numpy as np\n",
    "if np.__version__ < \"1.2.4\":\n",
    "    %pip install -q numpy==1.2.4\n",
    "if not ('folium' in modules):\n",
    "    %pip install -q folium==0.2.1\n",
    "    modules.add('folium')\n",
    "if not ('imgaug' in modules):\n",
    "    %pip install -q imgaug==0.2.6\n",
    "    modules.add('imgaug')\n",
    "if not ('tensorflow_text' in modules):\n",
    "    %pip install -q tensorflow_text==2.9.0\n",
    "    modules.add('tensorflow_text')\n",
    "if not ('seaborn' in modules):\n",
    "    %pip -q install -q seaborn\n",
    "    modules.add('seaborn')\n",
    "if not ('tqdm' in modules):\n",
    "    %pip install -q tqdm\n",
    "    modules.add('tqdm')\n",
    "if not ('matplotlib' in modules):\n",
    "    %pip install -q matplotlib\n",
    "    modules.add('matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1644854036589
    },
    "id": "MsfMr-Qod_nl",
    "outputId": "43ba05d9-3427-4565-cdd0-5a9fc46b069f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "# standart liberies\n",
    "\n",
    "from typing import List, Tuple\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import functools\n",
    "import statistics\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import timeit\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_text as tf_text\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    import tqdm\n",
    "    import matplotlib.pyplot as plt\n",
    "    %load_ext jupyternotify\n",
    "    \n",
    "except ImportError:\n",
    "    if np.__version__ < \"1.2.4\":\n",
    "        %pip install -q numpy==1.2.4\n",
    "    if not ('folium' in modules):\n",
    "        %pip install -q folium==0.2.1\n",
    "        modules.add('folium')\n",
    "    if not ('imgaug' in modules):\n",
    "        %pip install -q imgaug==0.2.6\n",
    "        modules.add('imgaug')\n",
    "    if not ('tensorflow_text' in modules):\n",
    "        %pip install -q tensorflow_text==2.9.0\n",
    "        modules.add('tensorflow_text')\n",
    "        import tensorflow_text as tf_text\n",
    "    if not ('seaborn' in modules):\n",
    "        %pip -q install -q seaborn\n",
    "        import seaborn as sns\n",
    "        modules.add('seaborn')\n",
    "    if not ('tqdm' in modules):\n",
    "        %pip install -q tqdm\n",
    "        import tqdm\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1644854037100
    },
    "id": "SzaglEQ-kp2n",
    "outputId": "918d0566-5dba-401f-f1b6-1dc9870883d6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.1\n",
      "tf text version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"tf text version: {tf_text.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJGjWnfTc6DI",
    "outputId": "ef22ee1b-00d0-4653-cea4-6ceda2aa2bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU info:/n\n",
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('GPU info:/n')\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1644854038983
    },
    "id": "2mp2yFTEkp2u",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "tf.keras.backend.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1644859230857
    },
    "id": "FqDAqgl5UV58",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "device: str\n",
    "if 'google' in modules:\n",
    "    device = 'colab'\n",
    "else:\n",
    "    device = 'locally'\n",
    "curr_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "B5rtigmHckyv"
   },
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bWmh89nVckyw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_size: int = 32\n",
    "num_layers: int = 4\n",
    "d_model: int = 128\n",
    "dff: int = 256\n",
    "num_heads: int = 8\n",
    "learning_rate: float = 0.001\n",
    "max_seq_len: int = 1024\n",
    "dropout_rate: float = 0.1\n",
    "batch_size: int = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVR1oJRD0NtO"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1644857858611
    },
    "id": "pn1UfAXQwlQX",
    "outputId": "77d46a62-3bbd-4749-c550-09b909b9a141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
      "(30279, 2)\n"
     ]
    }
   ],
   "source": [
    "if device == 'colab':  # If notebook is ran on colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/drive')\n",
    "    df: pd.DataFrame = pd.read_csv('/drive/MyDrive/final_project/wikipedia_articles.csv')\n",
    "else:  # If notebook is ran on my laptop\n",
    "    df: pd.DataFrame = pd.read_csv('wiki_data/articles.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IW43lG6iUV6G",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "dade0356-aaae-47a7-ccb8-eade45ff9b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30279 data points\n",
      "The length of the longest text IN CHARACTERS is: 141803\n",
      "The length of the shortest text IN CHARACTERS is: 816\n"
     ]
    }
   ],
   "source": [
    "df: pd.Series = df['text']\n",
    "data_list: List[str] = df.to_list()\n",
    "DATA_SIZE = len(data_list)\n",
    "print(f\"There are {DATA_SIZE} data points\")\n",
    "string_lengths: List[int] = [len(data_point) for data_point in data_list]\n",
    "max_string_len = max(string_lengths)\n",
    "print(f\"The length of the longest text IN CHARACTERS is: {max_string_len}\")\n",
    "min_string_len = min(string_lengths)\n",
    "print(f\"The length of the shortest text IN CHARACTERS is: {min_string_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi1AAJcP56_o"
   },
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "87Putqz6kp20"
   },
   "source": [
    "## Creating the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jy4kcb37kp21",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tensor_list: List = [tf.convert_to_tensor(data_point) for data_point in data_list]\n",
    "# print(type(tensor_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KBhC3SPyMi9n"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "bert_tokenizer_params: dict = dict(lower_case=True)\n",
    "# reserved_tokens: List[str] = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", \"[MASK]\"]\n",
    "VOCAB_SIZE: int = 8192  # Always the same for all models\n",
    "#\n",
    "# bert_vocab_args: dict = dict(\n",
    "#     # The target vocabulary size\n",
    "#     vocab_size = VOCAB_SIZE,\n",
    "#     # Reserved tokens that must be included in the vocabulary\n",
    "#     reserved_tokens=reserved_tokens,\n",
    "#     # Arguments for `tf_text.BertTokenizer`\n",
    "#     bert_tokenizer_params=bert_tokenizer_params,\n",
    "#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "#     learn_params={},\n",
    "# )\n",
    "# data_set: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tensor_list)\n",
    "# #I already ran this code and saved the file to C:/yoni/final_project/model/look_up_table.txt\n",
    "# vocab: List[str] = bert_vocab_from_dataset.bert_vocab_from_dataset(\n",
    "#     data_set,\n",
    "#     **bert_vocab_args,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LMoFc73ZMD6k"
   },
   "outputs": [],
   "source": [
    "# len(vocab), vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "M3nFQUtfkp27",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# with open('C:/yoni/final_project/model/look_up_table.txt', 'w') as f:\n",
    "#     for token in vocab:\n",
    "#         f.write(token + ' ')\n",
    "if device == 'colab':  # If notebook is ran on colab\n",
    "    path = '/drive/MyDrive/final_project/look_up_table.txt'\n",
    "else:  # If notebook is ran on my laptop\n",
    "    device = 'local'\n",
    "    path = 'C:/yoni/final_project/model/look_up_table.txt'\n",
    "with open(path, 'r') as f:\n",
    "    vocab: List[str] = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4IZ5uxlJEoi3",
    "outputId": "3ecbbe9a-9f49-49dc-ff6f-38ebd1716d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of the items in vocab: <class 'str'>\n",
      "the first 15 items in vocab: ['[PAD]', '[UNK]', '[START]', '[END]', '[MASK]', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6']\n",
      " the length of vocab: 7882\n"
     ]
    }
   ],
   "source": [
    "print(f\"the type of the items in vocab: {type(vocab[0])}\")\n",
    "print(f\"the first 15 items in vocab: {vocab[:15]}\")\n",
    "print(f\" the length of vocab: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hndA44nhFw6I",
    "outputId": "d461678f-4433-4386-d035-bb066ae63d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the type of the items in tensor_vocab is: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      " the data type of the tensors in tensor_vocab is: <dtype: 'string'>\n"
     ]
    }
   ],
   "source": [
    "tensor_vocab: List[tf.Tensor] = [tf.convert_to_tensor(token_key, dtype=tf.string) for token_key in vocab]  # dtype = tf.String\n",
    "print(f\" the type of the items in tensor_vocab is: {type(tensor_vocab[0])}\")\n",
    "print(f\" the data type of the tensors in tensor_vocab is: {tensor_vocab[0].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HihwIsrKD7C9"
   },
   "outputs": [],
   "source": [
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tensor_vocab,\n",
    "        key_dtype=tf.string,\n",
    "        values=tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64),\n",
    "        value_dtype=tf.int64),\n",
    "    num_oov_buckets=1\n",
    ")\n",
    "tokenizer = tf_text.BertTokenizer(lookup_table, **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "lD6QCFLAkp2_"
   },
   "source": [
    "## Creating the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "A73Lbgg6kp3R"
   },
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mLxV5l562-x2"
   },
   "outputs": [],
   "source": [
    "# reserved_tokens: List[str] = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", \"[MASK]\"]\n",
    "# START: int = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")  # The value of the start token\n",
    "# END: int = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")  # The value of the end token\n",
    "# starts = tf.cast(tf.Variable([START]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\n",
    "# ends = tf.cast(tf.Variable([END]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\n",
    "starts = tf.constant([2], dtype=tf.int32)\n",
    "ends = tf.constant([3], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4gPxVc7tkp3X",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "async def tokenize(text: str) -> tf.Tensor:\n",
    "    \"\"\"Converts string to tensor. takes about 0.02 seconds per call\"\"\"\n",
    "    ragged: tf.RaggedTensor = tokenizer.tokenize(text)[0, :]\n",
    "    eager: tf.Tensor = ragged.to_tensor(default_value=0, shape=[None, 1])  # 0 is the value of the padding token\n",
    "    sqeezed: tf.Tensor = tf.squeeze(eager, axis=1)\n",
    "    typed: tf.Tensor = tf.cast(sqeezed, tf.int32)\n",
    "    edited: tf.Tensor = tf.concat([starts, typed, ends], axis=0)\n",
    "    return edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "UzHmzHSukp3a",
    "outputId": "01c5b2ee-9ce2-4f8d-f17c-d2669e557aa7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-33-1b1967cc04f5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmagic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'time'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtokenized_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0masyncio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenize_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#tokenized_data: List[tf.Tensor] = [tokenize(data_point) for data_point in tqdm.tqdm(data_list)]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.7/asyncio/runners.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mevents\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_running_loop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m         raise RuntimeError(\n\u001B[0;32m---> 34\u001B[0;31m             \"asyncio.run() cannot be called from a running event loop\")\n\u001B[0m\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcoroutines\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miscoroutine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmain\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# %time\n",
    "\n",
    "# tokenized_data: List[tf.Tensor] = [tokenize(data_point) for data_point in tqdm.tqdm(data_list)] \n",
    "# tqdm is a progress bar\n",
    "\n",
    "# replace with an asyncio implementation:\n",
    "\n",
    "async def tokenize_list(a_data_list: List[str]) -> List[tf.Tensor]:\n",
    "    return await asyncio.gather(*[tokenize(data_point) for data_point in a_data_list]) \n",
    "\n",
    "# copilot completion use tokenize_list:\n",
    "\n",
    "tokenized_data: List[tf.Tensor] = asyncio.run(tokenize_list(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "CNEdwxWFkp3d",
    "outputId": "f248a5b8-a783-42b9-af95-344b149bb7cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-29-27f55c75bb4d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenized_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenized_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenized_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'coroutine' has no len()"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_data))\n",
    "print(tokenized_data[0].shape)\n",
    "print(tokenized_data[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "D77Pm-iGkp3j",
    "outputId": "d08952c8-f333-4f21-8203-c49c6c2b7d9d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25315\n",
      "178\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths_tokenized: List[int] = [text.shape[0] for text in tokenized_data]\n",
    "print(max(lengths_tokenized))\n",
    "print(min(lengths_tokenized))\n",
    "sns.displot(lengths_tokenized);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hNxIc9lVMMI",
    "outputId": "49100905-c0af-4366-ae66-ce43b3306d80",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is a plot of the distribution of the lengths of the tokenized data (AKA how many tokens in each data point of the data set).\n",
    "We can see that there is very few data points with length over 8192 (2^13).\n",
    "A model that can handle long texts is very expensive (both in emory and runtime).\n",
    "Therefore I will filter out all texts longer than 8192 tokens.\n",
    "Note that 8192 is also a lot compared to the capacity of similar sized transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "EtjxgLWMkp3l"
   },
   "source": [
    "### Filter too long texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MubdHBEMkJNC"
   },
   "outputs": [],
   "source": [
    "def chunk_tensor(tensor: tf.Tensor, max_len: int = max_seq_len) -> List[tf.Tensor]:\n",
    "    \"\"\"Splits 1d tensor to chunks (1d tensors) of maximum size: max_len\"\"\"\n",
    "    return [tensor[x:x+max_seq_len] for x in range(0, tensor.shape[0], max_seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DgU2Or2kp3m",
    "outputId": "292cd0f6-29ed-4cba-e627-4691f18955a7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102768\n",
      "(670,)\n"
     ]
    }
   ],
   "source": [
    "chunked_data: List[tf.Tensor] = []\n",
    "for tensor in tokenized_data:\n",
    "    chunks = chunk_tensor(tensor, max_seq_len)\n",
    "    for chunk in chunks:\n",
    "        chunked_data.append(chunk)\n",
    "DATA_SIZE: int = len(chunked_data)\n",
    "print(DATA_SIZE)\n",
    "print(chunked_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUOVoGBfkp3p",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad(tensor: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Pads the tensor to the length of the longest text in the data set\"\"\"\n",
    "    padded: tf.Tensor = tf.pad(tensor=tensor, paddings=[[0, max_seq_len - tensor.shape[0]]], mode='CONSTANT', constant_values=0)\n",
    "    # 0 is the padding token\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRa9_ryHkp3p",
    "outputId": "3f8d82e6-6067-4bc3-a34c-54f6001636e2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   2 1011 7670 ...    0    0    0], shape=(1024,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "padded_data: List[tf.Tensor] = [pad(text) for text in chunked_data]\n",
    "print(padded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gbnub0UZkp3s"
   },
   "source": [
    "## Train test val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dz5c-Gskp3t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.shuffle(padded_data)\n",
    "train_size: int = int(DATA_SIZE * 0.8)\n",
    "val_test_size: int = int(DATA_SIZE * 0.1)  # Both validation and test get 10% of the data\n",
    "\n",
    "train_tokenized: List[tf.Tensor] = padded_data[:train_size]\n",
    "val_tokenized: List[tf.Tensor] = padded_data[train_size:(val_test_size + train_size)]\n",
    "test_tokenized: List[tf.Tensor] = padded_data[(train_size + val_test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYF0ns5Rkp3t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def list_to_dataset(tokenized_list: List[tf.Tensor]) -> tf.data.Dataset:\n",
    "    \"\"\"Converts a list of tokenized texts after all preprocessing to a tf.data.Dataset\"\"\"\n",
    "    dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tokenized_list)\n",
    "    batched: tf.data.Dataset = dataset.batch(batch_size)\n",
    "    return batched\n",
    "\n",
    "train_set: tf.data.Dataset = list_to_dataset(train_tokenized)\n",
    "val_set: tf.data.Dataset = list_to_dataset(val_tokenized)\n",
    "test_set: tf.data.Dataset = list_to_dataset(test_tokenized)\n",
    "\n",
    "list_train_set = list(train_set)\n",
    "list_val_set = list(val_set)\n",
    "list_test_set = list(test_set)\n",
    "\n",
    "del train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzS72rcIvL0N"
   },
   "source": [
    "## Clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlkhQigKvOQT"
   },
   "outputs": [],
   "source": [
    "del train_tokenized, test_tokenized, val_tokenized\n",
    "del padded_data, chunked_data, tokenized_data, data_list, df, lengths_tokenized, \n",
    "del string_lengths, lookup_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vBLynXu0QI1"
   },
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9qUAZ00xiVd"
   },
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_V1b_16-kp3v"
   },
   "source": [
    "The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
    "\n",
    "where $d_{model}$ is the model dimension, $pos$ is the position and $i$ is the index of the embedding.\n",
    "this is taken from the paper: attention is all you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF0_SIBZ0yGG"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos: int, i: int, d_model: int) -> tf.Tensor:\n",
    "    \"\"\"Returns the angle in radians for a given position and timestamp\"\"\"\n",
    "    if tf.keras.backend.floatx() == 'float32':\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    else:\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float16(d_model))\n",
    "\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx4hze4k0yls"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position: int, d_model: int) -> tf.Tensor:\n",
    "    \"\"\"Returns the positional encoding for a given position and timestamp\"\"\"\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encode = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encode, dtype=tf.keras.backend.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRzRM7eA01X0"
   },
   "outputs": [],
   "source": [
    "# n, d = 2048, 512\n",
    "# pos_encoding = positional_encoding(n, d)\n",
    "# print(pos_encoding.shape)\n",
    "# print(pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ib5F3hnxrE9"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TXw70UlzcVi"
   },
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-LR26WF1BTO"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq: tf.Tensor) -> tf.Tensor:\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.keras.backend.floatx())\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    \n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rph2Ed4kkp3y"
   },
   "source": [
    "Just to check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O3B5VNqzj-E"
   },
   "outputs": [],
   "source": [
    "# x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "# create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w68l26GzrqG"
   },
   "source": [
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEd1Hnn51RXV"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size: int) -> tf.Tensor:\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    mask = tf.cast(mask, dtype=tf.keras.backend.floatx())\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJeQH-hCz1DK"
   },
   "outputs": [],
   "source": [
    "# exm = tf.random.uniform((1, 3))\n",
    "# print(exm)\n",
    "# temp = create_look_ahead_mask(exm.shape[1])\n",
    "# print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQkjgUVVckzJ"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp: tf.Tensor, tar: tf.Tensor, set_size: int):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(set_size)\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KHKaxz3xumc"
   },
   "source": [
    "## Layers and blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAoOoRf5kp30",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Returns the attention weights.\n",
    "    Args:\n",
    "      q: query shape == (..., inp_len, depth)\n",
    "      k: key shape == (..., inp_len, depth)\n",
    "      v: value shape == (..., inp_len, depth_v)\n",
    "      mask: Float tensor with shape broadcast-able\n",
    "            to (..., inp_len, inp_len). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "    # print(f\"mask: {mask[0]}\")\n",
    "    # print(f\"q: {q[0]}\")\n",
    "    # print(f\"k: {k[0]}\")\n",
    "    # print(f\"v: {v[0]}\")\n",
    "    matmul_qk: tf.Tensor = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    # print(f\"matmul_qk: {matmul_qk[0]}\")\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth: tf.Tensor = tf.cast(tf.shape(k)[-1], tf.keras.backend.floatx())  # depth\n",
    "    # print(f\"depth: {depth}\")\n",
    "    \n",
    "    scaled_attention_logits: tf.Tensor = matmul_qk / tf.math.sqrt(depth)\n",
    "    # print(f\"scaled_attention_logits: {scaled_attention_logits[0]}\")\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        # noinspection PyTypeChecker\n",
    "        if tf.keras.backend.floatx() == 'float16':\n",
    "            scaled_attention_logits += (mask * tf.float16.min)  # changed from -1e9 to prevent nan's\n",
    "        else:\n",
    "            scaled_attention_logits += (mask * -1e9) \n",
    "        # print(f\"scaled_attention_logits: {scaled_attention_logits[0]}\")\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    # print(f\"attention_weights: {attention_weights[0]}\")\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    # print(f\"output: {output[0]}\")\n",
    "    should_print = False\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0I9SZI0kp31",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class My_multi_head_attention(tf.keras.layers.Layer):\n",
    "    # __slots__ = [\"num_heads\", \"d_model\", \"depth\", \"wq\", \"wk\", \"wv\", \"dense\"]\n",
    "    \"\"\"U can use the built-in tf.keras.layers.multihead_attention but is caused a bug for me\"\"\"\n",
    "    def __init__(self, num_heads: int, d_model: int):\n",
    "        super(My_multi_head_attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x: tf.Tensor, batch_size) -> tf.Tensor:\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v_k: tf.Tensor, q: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        # print(f\"v_k: {v_k.numpy()}\")\n",
    "\n",
    "        q: tf.Tensor = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        # print(f\"q1: {q.numpy()}\")\n",
    "        k: tf.Tensor = self.wk(v_k)  # (batch_size, seq_len, d_model)\n",
    "        # print(f\"k1: {k.numpy()}\")\n",
    "        v: tf.Tensor = self.wv(v_k)  # (batch_size, seq_len, d_model)\n",
    "        # print(f\"v1: {v.numpy()}\")\n",
    "\n",
    "        q: tf.Tensor = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        # print(f\"q2: {q.numpy()}\")\n",
    "        k: tf.Tensor = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        # print(f\"k2: {k.numpy()}\")\n",
    "        v: tf.Tensor = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        # print(f\"v2: {v.numpy()}\")\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # print(f\"scaled_attention1: {scaled_attention.numpy()}\")\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n",
    "         # (batch_size, seq_len_q, num_heads, depth)\n",
    "        # print(f\"scaled_attention2: {scaled_attention.numpy()}\")\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "          # (batch_size, seq_len_q, d_model)\n",
    "        # print(f\"concat_attention: {concat_attention.numpy()}\")\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        # print(f\"output: {output.numpy()}\")\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CQkOJpJ23uW"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model: int, dff: int) -> tf.keras.Model:\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mEP3-Jx2n39"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    # __slots__ = [\"mha\", \"ffn\", \"layer_norm\", \"dropout\"]\n",
    "    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float = 0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = My_multi_head_attention(num_heads = num_heads, d_model = d_model)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x: tf.Tensor, training, mask: tf.Tensor) -> tf.Tensor:\n",
    "        \n",
    "        # print(f\"x: {x.numpy()}\")\n",
    "        # print(f\"mask: {mask.numpy()}\")\n",
    "        attn_output = self.mha(x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        # print(f\"first mha outputs: {attn_output.numpy()}\")\n",
    "        attn_output = self.dropout(attn_output, training=training)\n",
    "        # print(f\"first dropout: {attn_output.numpy()}\")\n",
    "        out1 = self.layer_norm(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        # print(f\"out1: {out1.numpy()}\")\n",
    "        \n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        # print(f\"ffn_output: {ffn_output.numpy()}\")\n",
    "        ffn_output = self.dropout(ffn_output, training=training)\n",
    "        # print(f\"second dropout: {ffn_output.numpy()}\")\n",
    "        out2 = self.layer_norm(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        # print(f\"final output: {out2.numpy()}\")\n",
    "\n",
    "        return out2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJWFrv0g281G"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    # __slots__ = [\"mha\", \"ffn\", \"layer_norm\", \"dropout\"]\n",
    "    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float = 0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = My_multi_head_attention(num_heads = num_heads, d_model = d_model)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor, training):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1 = self.mha(x, x, look_ahead_mask)  # (batch_size, set_size, d_model)\n",
    "        attn1 = self.dropout(attn1, training=training)\n",
    "        out1 = self.layer_norm(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha(enc_output, out1, padding_mask)  # (batch_size, set_size, d_model)\n",
    "        attn2 = self.dropout(attn2, training=training)\n",
    "        out2 = self.layer_norm(attn2 + out1)  # (batch_size, set_size, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, set_size, d_model)\n",
    "        ffn_output = self.dropout(ffn_output, training=training)\n",
    "        out3 = self.layer_norm(ffn_output + out2)  # (batch_size, set_size, d_model)\n",
    "\n",
    "        return out3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdlS1kfM3k5d"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    # __slots__ = [\"d_model\", \"num_layers\", \"embedding\", \"pos_encoding\", \"enc_layers\", \"dropout\"]\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, input_vocab_size: int,\n",
    "                 maximum_position_encoding: int, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x: tf.Tensor, training, mask: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        # print(f\"embedding: {x.numpy()[0]}\")\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.keras.backend.floatx()))\n",
    "        # print(f\"sqrt: {x.numpy()[0]}\")\n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # print(f\"pos_encoding: {x.numpy()[0]}\")\n",
    "        x = self.dropout(x, training=training)\n",
    "        # print(f\"dropout: {x.numpy()[0]}\")\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "            # print(f\"enc_layers {i}: {x.numpy()[0]}\")\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLE8js6O3p5-"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    # __slots__ = [\"d_model\", \"num_layers\", \"embedding\", \"pos_encoding\", \"dec_layers\", \"dropout\"]\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int,\n",
    "                 maximum_position_encoding: int, rate: float = 0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, training: bool,\n",
    "             look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, set_size, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.keras.backend.floatx()))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x=x, enc_output=enc_output, look_ahead_mask=look_ahead_mask, \n",
    "                                   padding_mask=padding_mask, training=training)\n",
    "\n",
    "        # x.shape == (batch_size, set_size, d_model)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-xe3K7LyD2l"
   },
   "source": [
    "## The full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NLhyE0T3tUs"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    # __slots__ = [\"set_size\", \"batch_size\", \"num_layers\", \"d_model\",\n",
    "    #              \"num_heads\", \"dff\", \"vocab_size\", \"max_len\", \"rate\"]  # You can't save a model that uses slots\n",
    "    \"\"\"The base architecture of my models in this project.\"\"\"\n",
    "    def __init__(self, set_size: int, batch_size: int, num_layers: int, d_model: int, num_heads: int, dff: int,\n",
    "                 vocab_size: int, max_len: int, rate: float = 0.1):\n",
    "        super().__init__()  # Initializes the base classes of this class.\n",
    "        self.set_size: int = set_size\n",
    "        self.batch_size: int = batch_size\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                               vocab_size, max_len, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               vocab_size, max_len, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def call(self, inputs: List[tf.Tensor], training: bool) -> tf.Tensor:\n",
    "        inp, tar = inputs\n",
    "        # print(f\"input: {inp.numpy()[0][0]}\")\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar, self.set_size)\n",
    "        # print(f\"enc_padding_mask output: {enc_padding_mask.numpy()[0][0][0][0]}\")\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        \n",
    "        # print(f\"encoder output: {enc_output.numpy()[0][0][0]}\")\n",
    "        # dec_output.shape = (batch_size, set_size, d_model)\n",
    "        dec_output = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        # print(f\"decoder output: {dec_output.numpy()[0][0][0]}\")\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        # print(f\"final_output: {final_output.numpy()[0][0][0]}\")\n",
    "        softmaxed = self.softmax(final_output)\n",
    "        # print(f\"softmaxed: {softmaxed.numpy()[0][0][0]}\")\n",
    "        return softmaxed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WZ41cb-1B_f",
    "outputId": "6ee5ceb5-2868-4ea0-b38a-5948f88a3704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4, 8192)\n",
      "tf.Tensor(\n",
      "[[1.4842e-04 9.7454e-05 1.1861e-04 ... 1.1611e-04 1.2434e-04 1.1706e-04]\n",
      " [1.3661e-04 1.0204e-04 1.2231e-04 ... 1.2231e-04 1.3041e-04 1.2803e-04]\n",
      " [1.4734e-04 9.5904e-05 1.2481e-04 ... 1.1802e-04 1.1986e-04 1.1879e-04]\n",
      " [1.4484e-04 9.9003e-05 1.2088e-04 ... 1.1575e-04 1.2064e-04 1.2141e-04]], shape=(4, 8192), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    set_size=4, batch_size=batch_size, num_layers=2, d_model=32, num_heads=4, dff=128,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=max_seq_len)\n",
    "\n",
    "temp_input = tf.random.uniform((batch_size, max_seq_len), dtype=tf.int32, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((batch_size, 4), dtype=tf.int32, minval=0, maxval=200)\n",
    "\n",
    "fn_out = sample_transformer([temp_input, temp_target], training=True)\n",
    "\n",
    "print(fn_out.shape)  # (batch_size, max_seq_len, tvocab_size)\n",
    "print(fn_out[0])\n",
    "del sample_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hFxiRfCekp38"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8jsueeI0kp3-"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZ8vqihqkp3-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=tf.float16.min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O783T1xMckzR"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uAxXGBtkp4A",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    batch_size=batch_size,\n",
    "    set_size=set_size,\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=max_seq_len,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gjCRldwckzR"
   },
   "source": [
    "## Train step function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQg2vNCNjUIJ"
   },
   "source": [
    "Since I can't use model.compile and model.fit and I dont want this function to be compiled every time I run it, I use tf.function to pre-compile the function into tf.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaBpjXbnqSBD"
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, set_size], dtype = tf.int32)])\n",
    "def tokens_to_onehot(tokens: tf.Tensor):\n",
    "    \"\"\"Gets a tokens tensor of shape: (batch_size, set_size) and dtype: tf.int32\n",
    "     and returns a onehot encoding tensor of shape: (batch_size, set_size, VOCAB_SIZE) \n",
    "     and dtype: tf.int32\"\"\"\n",
    "    # probably a bottleneck\n",
    "    one_hot = tf.one_hot(tokens, depth = VOCAB_SIZE, axis=-1)\n",
    "    return tf.cast(one_hot, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBqe_-AS6KCa"
   },
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR6qYltHhh_t"
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n",
    "                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)])\n",
    "def train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred: tf.Tensor = model([inp, outp], training=True)\n",
    "        # print(f\"the shape of pred: {pred.shape}\\n the shape of outp: {outp.shape}\")\n",
    "        # print(pred[0][0])\n",
    "        # print(outp[0][0])\n",
    "        \n",
    "        expected: tf.Tensor = tokens_to_onehot(outp)\n",
    "        # assert pred.shape == expected.shape\n",
    "        loss_val: tf.Tensor = loss_func(y_true = expected, y_pred = pred)\n",
    "        # print(f\"exepted: {expected.numpy()}\") \n",
    "        # print(f\"inp[0]: {inp.numpy()[0]}\")\n",
    "        # print(f\"pred[0][0]: {pred.numpy()[0][0]}\")\n",
    "        # print(f\"loss_val: {loss_val.numpy()}\")\n",
    "    grads: tf.Tensor = tape.gradient(loss_val, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUM4zxGtckzS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n",
    "                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)])\n",
    "def val_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n",
    "    pred = model([inp, outp], training=False)\n",
    "    expected = tokens_to_onehot(outp)\n",
    "    loss: tf.Tensor = loss_func(y_true = expected, y_pred = pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NpdkcUkckzT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate(batch: tf.Tensor) -> float:\n",
    "    per_generation_loss: List[float] = []\n",
    "    for i in range(NUM_SETS):\n",
    "        # The input is of size set_size-TAKE_TO_ACCOUNT\n",
    "        already_predicted: int = i*(set_size+1)\n",
    "        start_from: int = max(0, already_predicted - max_seq_len)\n",
    "        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n",
    "        outp: tf.Tensor = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n",
    "        loss_val: tf.Tensor = val_step(inp, outp)\n",
    "        float_loss: float = loss_val.numpy().item()\n",
    "        per_generation_loss.append(float_loss)\n",
    "    return sum(per_generation_loss) / len(per_generation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8944E1hckzT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def delete_last_row(csv_path: str):\n",
    "    \"\"\"Deletes the last row of a csv file\"\"\"\n",
    "    with open(csv_path, 'r') as f:\n",
    "        lines = f.readlines()[:-1]\n",
    "    with open(csv_path, 'w') as f:\n",
    "        c_writer = csv.writer(f, delimiter=',')\n",
    "        for line in lines:\n",
    "            c_writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZsEGz_vckzT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_point(folder_path: str, model: Transformer, val_loss: float, train_loss: float):\n",
    "    \"\"\"Saves the model at the end of each epoch\"\"\"\n",
    "    path: str = f\"{folder_path}/\"\n",
    "    tf.keras.models.save_model(model = model, filepath = path, save_format='tf')\n",
    "    delete_last_row('experiments.csv')\n",
    "    fields = [date, val_loss, train_loss, set_size, num_layers, d_model, dff,\n",
    "              num_heads, learning_rate, max_seq_len, dropout_rate, batch_size]\n",
    "    with open('experiments.csv', 'a') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQc_k2ekckzU"
   },
   "source": [
    "## The training loop itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iuce_A94ckzU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "date: str = datetime.datetime.now().strftime('%m%d-%H%M')\n",
    "check_points_path = f\"/drive/MyDrive/checkpoints/{date}\"\n",
    "if not os.path.isdir(\"/drive/MyDrive/checkpoints\"):\n",
    "    os.mkdir(\"/drive/MyDrive/checkpoints\")\n",
    "if not os.path.isdir(check_points_path):\n",
    "    os.mkdir(check_points_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tDUBdOH0ckzV"
   },
   "source": [
    "## Add row to experiment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00auA3LZwKvf"
   },
   "outputs": [],
   "source": [
    "train_loss, val_loss = float('inf'), float('inf')\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWKTBdzkckzV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(r'experiments.csv', 'a') as f:\n",
    "    fields = [date, val_loss, train_loss, set_size, num_layers, d_model, dff,\n",
    "              num_heads, learning_rate, max_seq_len, dropout_rate, batch_size]\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN8rmbrNW0-Z"
   },
   "source": [
    "# The actual training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvpXKOY-rKgN",
    "outputId": "12088668-d6cf-4d27-d5a4-3ac1086492b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643, <tf.Tensor: shape=(16, 1024), dtype=int32, numpy=\n",
       " array([[1737,   50,  897, ...,  787,  307,   56],\n",
       "        [   2, 4200, 3656, ..., 5749,   46,   48],\n",
       "        [   5,   36, 2684, ..., 1751,  164,   51],\n",
       "        ...,\n",
       "        [   2,   35, 5157, ...,    7,   76, 1051],\n",
       "        [ 912, 2923,   53, ...,    0,    0,    0],\n",
       "        [1506,   51,   48, ..., 1334,  839,    6]], dtype=int32)>)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(list_val_set), list_val_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHktVH98kp4B",
    "outputId": "bd16ea4d-22d7-42d1-f603-27c4a82a6508",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r0it [00:00, ?it/s]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "10it [01:03,  3.22s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "20it [01:55,  3.18s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "30it [02:49,  3.14s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "40it [03:41,  3.14s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "50it [04:37,  3.35s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "60it [05:32,  3.17s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "70it [06:23,  3.08s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "80it [07:16,  3.10s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "90it [08:32,  3.33s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n",
      "100it [10:25,  4.18s/it]WARNING:absl:Found untraced functions such as embedding_56_layer_call_fn, embedding_56_layer_call_and_return_conditional_losses, dropout_211_layer_call_fn, dropout_211_layer_call_and_return_conditional_losses, embedding_57_layer_call_fn while saving (showing 5 of 136). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/2022-06-12/assets\n"
     ]
    }
   ],
   "source": [
    "EPOCHS: int = 100  # Train until the cloud disconnects\n",
    "NUM_SETS: int = (max_seq_len // set_size) - 1 # Because we dont predict the first set\n",
    "per_epoch_train_loss: List[float] = []\n",
    "per_epoch_val_loss: List[float] = []\n",
    "check_points_path = f\"./checkpoints/{datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "if not os.path.isdir(check_points_path):\n",
    "    os.mkdir(check_points_path)\n",
    "for epoch in range(EPOCHS):\n",
    "    per_batch_train_loss: List[float] = []\n",
    "    per_batch_val_loss: List[float] = []\n",
    "    for batch_num, train_batch in tqdm.tqdm(enumerate(list_train_set)):  # tqdm is a progress bar\n",
    "        per_generation_loss: List[float] = []\n",
    "        for i in range(NUM_SETS):\n",
    "            # The input is of size set_size-TAKE_TO_ACCOUNT\n",
    "            already_predicted: int = i * (set_size + 1)\n",
    "            start_from: int = max(0, already_predicted - max_seq_len)\n",
    "            inp: tf.Tensor = train_batch[:, start_from:(i + 1) * set_size]\n",
    "            outp: tf.Tensor = train_batch[:, (i + 1) * set_size:(i + 2) * set_size]\n",
    "            loss_val: tf.Tensor = train_step(inp, outp)\n",
    "            train_loss: float = loss_val.numpy().item()\n",
    "            # assert isinstance(train_loss, float)\n",
    "            \n",
    "            per_generation_loss.append(train_loss)\n",
    "            \n",
    "        per_batch_train_loss.append(statistics.mean(per_generation_loss))\n",
    "        if batch_num % 10 == 0:\n",
    "            next_val_batch: tf.Tensor = list_val_set[batch_num // 10]\n",
    "            val_loss: float = validate(next_val_batch)\n",
    "            per_batch_val_loss.append(val_loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                check_point(check_points_path, model, val_loss, train_loss)\n",
    "    per_epoch_train_loss.append(statistics.mean(per_batch_train_loss))\n",
    "    per_epoch_val_loss.append(statistics.mean(per_batch_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1gVcN35kp4C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5vBLynXu0QI1"
   ],
   "name": "final_project.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "cac4749ce6e64bfd07fafd5bf9c175e86cc05b1d81ce0d05824a22ecc489c963"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}