{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"ZZf-fbj50Kqc"}},{"cell_type":"code","source":"def install_libaries():\n    %pip install -q gensim==3.6.0\n    %pip install -q nlppreprocess","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:02:25.717592Z","iopub.execute_input":"2022-07-13T12:02:25.718686Z","iopub.status.idle":"2022-07-13T12:02:25.727554Z","shell.execute_reply.started":"2022-07-13T12:02:25.718622Z","shell.execute_reply":"2022-07-13T12:02:25.726457Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import sys\ndevice: str\nif \"google.colab\" in sys.modules.keys():\n    device = \"colab\"\nif \"kaggle_web_client\" in sys.modules.keys():\n    device = \"kaggle\"\nelse:\n    device = \"locally\"","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:02:27.406256Z","iopub.execute_input":"2022-07-13T12:02:27.407354Z","iopub.status.idle":"2022-07-13T12:02:27.417063Z","shell.execute_reply.started":"2022-07-13T12:02:27.407282Z","shell.execute_reply":"2022-07-13T12:02:27.414958Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# standart liberies:\nfrom typing import Optional, List, Set, Dict, Tuple\nimport datetime\nimport os\nimport random\nimport statistics\nimport math\nimport time\n# NON-standart liberies:\nimport wandb\nimport tensorflow as tf\nimport keras\nfrom keras import layers, Model\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport tqdm\nimport pandas as pd\ntry:\n    from keras.preprocessing.text import Tokenizer\n    from keras.preprocessing.sequence import pad_sequences \n    from nlppreprocess import NLP as nlp\n    from gensim.scripts.glove2word2vec import glove2word2vec\n    from gensim.models.keyedvectors import KeyedVectors\nexcept ModuleNotFoundError as e:\n    print(e)\n    install_libaries()\nexcept ImportError as e:\n    print(e)\n    install_libaries()","metadata":{"gather":{"logged":1644854036589},"id":"MsfMr-Qod_nl","outputId":"e09897f8-e611-4818-b6b6-adf1e370cccf","execution":{"iopub.status.busy":"2022-07-13T12:02:31.276875Z","iopub.execute_input":"2022-07-13T12:02:31.277318Z","iopub.status.idle":"2022-07-13T12:02:31.287605Z","shell.execute_reply.started":"2022-07-13T12:02:31.277278Z","shell.execute_reply":"2022-07-13T12:02:31.286237Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(f\"Python version: {sys.version}\")\nprint(f\"Tensorflow version: {tf.__version__}\")","metadata":{"gather":{"logged":1644854037100},"id":"SzaglEQ-kp2n","outputId":"5b7fd3b9-3369-431c-b516-bde95519988f","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:02:34.285888Z","iopub.execute_input":"2022-07-13T12:02:34.286987Z","iopub.status.idle":"2022-07-13T12:02:34.292631Z","shell.execute_reply.started":"2022-07-13T12:02:34.286945Z","shell.execute_reply":"2022-07-13T12:02:34.291718Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n[GCC 9.4.0]\nTensorflow version: 2.6.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Settings","metadata":{"id":"91ZJ0-RZIx6j"}},{"cell_type":"code","source":"tf.random.set_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n# keras.utils.set_random_seed(0)\n# tf.config.experimental.enable_op_determinism()\n# keras.backend.set_floatx(\"float16\")\nf_type = keras.backend.floatx()  # either tf.float16 or tf.float32","metadata":{"gather":{"logged":1644854038983},"id":"2mp2yFTEkp2u","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:02:35.025231Z","iopub.execute_input":"2022-07-13T12:02:35.026518Z","iopub.status.idle":"2022-07-13T12:02:35.033740Z","shell.execute_reply.started":"2022-07-13T12:02:35.026472Z","shell.execute_reply":"2022-07-13T12:02:35.032716Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Define a strategy - Accelerator optimization ","metadata":{"id":"QlubPo7LRyNm"}},{"cell_type":"code","source":"try:\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver();\n    tf.config.experimental_connect_to_cluster(resolver);\n    tf.tpu.experimental.initialize_tpu_system(resolver);\n    strategy = tf.distribute.TPUStrategy(resolver);\nexcept ValueError:\n    pass","metadata":{"id":"s0yqg12zqSt9","execution":{"iopub.status.busy":"2022-07-13T12:02:35.765438Z","iopub.execute_input":"2022-07-13T12:02:35.765878Z","iopub.status.idle":"2022-07-13T12:02:35.771836Z","shell.execute_reply.started":"2022-07-13T12:02:35.765841Z","shell.execute_reply":"2022-07-13T12:02:35.770853Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Data loading","metadata":{"id":"yVR1oJRD0NtO","_kg_hide-input":true}},{"cell_type":"code","source":"if device == \"colab\":  # If notebook is ran on colab\n    from google.colab import drive\n    drive.mount(\"/drive\")\n    df: pd.DataFrame = pd.read_csv(\"/drive/MyDrive/final_project/wikipedia_articles.csv\")\nelif device == \"kaggle\":\n    df: pd.DataFrame = pd.read_csv(\"../input/wikipedia-promotional-articles/promotional.csv\")\nelse:  # If notebook is ran on my laptop\n    df: pd.DataFrame = pd.read_csv(\"wiki_data/articles.csv\")\nprint(f\"the shape of the dataframe: {df.shape}\")\n\ntrain_df = df.sample(frac=0.8, random_state=0) #random state is a seed value\ntemp_val_df = df.drop(train_df.index)\ntest_df = temp_val_df.sample(frac=0.5, random_state=0)\nval_df = temp_val_df.drop(test_df.index)\n\ntrain_ser = train_df[\"text\"]\nval_ser = val_df[\"text\"]\ntest_ser = test_df[\"text\"]\n\ntrain_ser.shape, val_ser.shape, test_ser.shape","metadata":{"gather":{"logged":1644857858611},"id":"pn1UfAXQwlQX","outputId":"af20cbcb-3d71-4d4b-be9a-5850b9f9741c","execution":{"iopub.status.busy":"2022-07-13T12:22:15.732598Z","iopub.execute_input":"2022-07-13T12:22:15.733011Z","iopub.status.idle":"2022-07-13T12:22:18.667269Z","shell.execute_reply.started":"2022-07-13T12:22:15.732979Z","shell.execute_reply":"2022-07-13T12:22:18.666158Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"the shape of the dataframe: (23837, 7)\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"((19070,), (2383,), (2384,))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Creating a tokenizer","metadata":{}},{"cell_type":"code","source":"oov_token = \"[OOV]\"\ntokenizer = Tokenizer(num_words=8192, oov_token=oov_token)\ntokenizer.fit_on_texts(list((train_ser.apply(nlp().process).values)))\nlist_tokenized_train: List[List[int]] = tokenizer.texts_to_sequences(train_ser.values)\nlist_tokenized_val: List[List[int]] = tokenizer.texts_to_sequences(val_ser.values)\nlist_tokenized_test: List[List[int]] = tokenizer.texts_to_sequences(test_ser.values)\nvocab_size = tokenizer.get_config()[\"num_words\"]","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:28:21.040705Z","iopub.execute_input":"2022-07-13T12:28:21.041124Z","iopub.status.idle":"2022-07-13T12:29:06.604397Z","shell.execute_reply.started":"2022-07-13T12:28:21.041091Z","shell.execute_reply":"2022-07-13T12:29:06.603249Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"print(list_tokenized_train[0])\nprint(type(list_tokenized_train[0]))\npad_int: int = 0\nprint(any(pad_int in sublist for sublist in list_tokenized_train))\nprint(any(pad_int in sublist for sublist in list_tokenized_val))\nprint(any(pad_int in sublist for sublist in list_tokenized_test))\n# 0 is not used threfore it will be the padding token ","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:03:23.547741Z","iopub.execute_input":"2022-07-13T12:03:23.548525Z","iopub.status.idle":"2022-07-13T12:03:23.699115Z","shell.execute_reply.started":"2022-07-13T12:03:23.548470Z","shell.execute_reply":"2022-07-13T12:03:23.697622Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[2125, 6160, 157, 5206, 3277, 262, 65, 638, 5206, 6160, 1354, 831, 67, 829, 1, 281, 2354, 3, 389, 1394, 3406, 215, 5211, 84, 981, 1, 2734, 3406, 1111, 573, 2, 6160, 2125, 67, 453, 7, 60, 620, 60, 344, 60, 229, 3107, 4476, 2125, 638, 9, 807, 2, 60, 229, 3107, 1260, 1080, 956, 6588, 1, 2125, 480, 6061, 1774, 704, 60, 1260, 4515, 47, 550, 47, 1, 2125, 480, 6061, 1, 1774, 704, 60, 1260, 2125, 47, 1193]\n<class 'list'>\nFalse\nFalse\nFalse\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenizing the data","metadata":{"id":"A73Lbgg6kp3R"}},{"cell_type":"markdown","source":"### chunk too long texts","metadata":{"id":"EtjxgLWMkp3l"}},{"cell_type":"code","source":"max_seq_len: int = 256\n\ndef chunk_double_list(mat: List[List[int]], max_len: int = max_seq_len) -> List[List[int]]:\n    \"\"\"Splits token list to chunks (lists) of maximum size: max_len\"\"\"\n    chunked_mat = []\n    for l in mat:\n        chunked_mat += [l[i*max_len:(i+1)*max_len] for i in range(len(l) // max_len)]\n    return list(filter(lambda x: len(x) > 0, chunked_mat))","metadata":{"id":"MubdHBEMkJNC","execution":{"iopub.status.busy":"2022-07-13T12:03:23.702844Z","iopub.execute_input":"2022-07-13T12:03:23.703365Z","iopub.status.idle":"2022-07-13T12:03:23.711347Z","shell.execute_reply.started":"2022-07-13T12:03:23.703313Z","shell.execute_reply":"2022-07-13T12:03:23.710050Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"chunked_train: List[List[int]] = chunk_double_list(list_tokenized_train)\nchunked_val: List[List[int]] = chunk_double_list(list_tokenized_val)\nchunked_test: List[List[int]] = chunk_double_list(list_tokenized_test)","metadata":{"id":"9DgU2Or2kp3m","outputId":"596803b2-a306-4ef7-92c0-a497ecc507e3","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:03:23.712969Z","iopub.execute_input":"2022-07-13T12:03:23.713917Z","iopub.status.idle":"2022-07-13T12:03:24.518094Z","shell.execute_reply.started":"2022-07-13T12:03:23.713878Z","shell.execute_reply":"2022-07-13T12:03:24.516676Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Padding","metadata":{"id":"R8Z2_P2Lnke8"}},{"cell_type":"code","source":"chunked_train.sort(key = lambda l: len(l))  # sorting so that every batch will have similar sized texts, used when training\nchunked_val.sort(key = lambda l: len(l))\nchunked_test.sort(key = lambda l: len(l))\n\npadded_train: tf.Tensor = pad_sequences(chunked_train, padding=\"post\", value=pad_int)\npadded_val: tf.Tensor = pad_sequences(chunked_val, padding=\"post\", value=pad_int)\npadded_test: tf.Tensor = pad_sequences(chunked_test, padding=\"post\", value=pad_int)","metadata":{"id":"nRa9_ryHkp3p","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:03:24.519621Z","iopub.execute_input":"2022-07-13T12:03:24.519975Z","iopub.status.idle":"2022-07-13T12:03:25.801568Z","shell.execute_reply.started":"2022-07-13T12:03:24.519944Z","shell.execute_reply":"2022-07-13T12:03:25.800526Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Train test val split","metadata":{"id":"gbnub0UZkp3s"}},{"cell_type":"code","source":"batch_size: int = 128\n    \ndef ten_to_dataset(tokenized_ten: tf.Tensor) -> tf.data.Dataset:\n    \"\"\"Converts a list of tokenized texts after all preprocessing to a tf.data.Dataset\"\"\"\n    dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tokenized_ten)\n    dataset = dataset.batch(batch_size)\n    return dataset\n\ntrain_dataset = ten_to_dataset(padded_train)\nval_dataset = ten_to_dataset(padded_val)\ntest_dataset = ten_to_dataset(padded_test)\n\nlist_train_set = list(train_dataset)\nlist_val_set = list(val_dataset)\nlist_test_set = list(test_dataset)","metadata":{"id":"pTW48b8Qr6sv","execution":{"iopub.status.busy":"2022-07-13T12:03:25.803012Z","iopub.execute_input":"2022-07-13T12:03:25.803385Z","iopub.status.idle":"2022-07-13T12:03:26.156996Z","shell.execute_reply.started":"2022-07-13T12:03:25.803355Z","shell.execute_reply":"2022-07-13T12:03:26.155873Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"2022-07-13 12:03:25.842510: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Clear memory","metadata":{"id":"HzS72rcIvL0N"}},{"cell_type":"code","source":"del train_dataset, val_dataset, test_dataset\ndel chunked_train, chunked_val, chunked_test\ndel padded_train, padded_val, padded_test\ndel list_tokenized_train, list_tokenized_val, list_tokenized_test\ndel train_df, temp_val_df, val_df, test_df\ndel train_ser, val_ser, test_ser","metadata":{"id":"VlkhQigKvOQT","execution":{"iopub.status.busy":"2022-07-13T12:03:26.158453Z","iopub.execute_input":"2022-07-13T12:03:26.158803Z","iopub.status.idle":"2022-07-13T12:03:26.225912Z","shell.execute_reply.started":"2022-07-13T12:03:26.158773Z","shell.execute_reply":"2022-07-13T12:03:26.224677Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Model\n","metadata":{"id":"5vBLynXu0QI1"}},{"cell_type":"markdown","source":"## Positional encoding","metadata":{"id":"A9qUAZ00xiVd"}},{"cell_type":"markdown","source":"The formula for calculating the positional encoding is as follows:\n\n$${PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n$${PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n\nwhere $d_{model}$ is the model dimension, $pos$ is the position and $i$ is the index of the embedding.\nthis is taken from the paper: attention is all you need.","metadata":{"id":"_V1b_16-kp3v"}},{"cell_type":"code","source":"def create_positional_encoding(max_len: int, d_model: int) -> tf.Tensor:\n    \"\"\"Returns the positional encoding for a given a maximal sequence length and model dimension.\n    used in SeTransformer.__init__()\n    inputs: max_len: int, d_model: int\n    returns: tf.Tensor of shape (1, max_len, d_model) and dtype f_type\n    The 1 is for the batch dimension, the place in the batch dimension does not matter\"\"\"\n\n    def get_angles(positions: np.ndarray, timestamps: np.ndarray, d_model: int) -> np.ndarray:\n        \"\"\"Returns the angle in radians for given positions, timestamps and the dimension of the model\n        input: positions: np.ndarray of shape (max_len, 1), timestamps: np.ndarray of shape (1, d_model), d_model: int\n        output: np.ndarray of shape (max_len, d_model)\"\"\"\n        if f_type == \"float32\":\n            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float32(d_model)))\n        else:\n            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float16(d_model)))\n\n        return positions * angle_rates\n    \n    angle_rads = get_angles(np.arange(max_len)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)  # (max_len, d_model)\n\n    # apply sin to even indices in the array; 2i for i in range(d_model // 2)\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # (max_len, d_model)\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # (max_len, d_model)\n\n    pos_encode = angle_rads[np.newaxis, ...]  # (1, max_len, d_model)\n\n    return tf.cast(pos_encode, dtype=f_type)","metadata":{"id":"Wx4hze4k0yls","execution":{"iopub.status.busy":"2022-07-13T12:03:26.227673Z","iopub.execute_input":"2022-07-13T12:03:26.228946Z","iopub.status.idle":"2022-07-13T12:03:26.242301Z","shell.execute_reply.started":"2022-07-13T12:03:26.228896Z","shell.execute_reply":"2022-07-13T12:03:26.240861Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Masking","metadata":{"id":"6ib5F3hnxrE9"}},{"cell_type":"markdown","source":"Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.","metadata":{"id":"3TXw70UlzcVi"}},{"cell_type":"markdown","source":"The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n\nThis means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on.","metadata":{"id":"4w68l26GzrqG"}},{"cell_type":"code","source":"def create_masks(inp: tf.Tensor, tar: tf.Tensor, pad_ten: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n        \"\"\"Creates all the masks needed for the model\n        input: inp: tf.Tensor of shape (batch_size, seq_len), tar: tf.Tensor of shape (batch_size, set_size)\n        Returns: tuple of (padding_mask, look_ahead_mask)\n        padding_mask, look_ahead_mask: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n        \n        def create_padding_mask(seq: tf.Tensor) -> tf.Tensor:\n                \"\"\"Returns a padding mask for the given sequence.\n                input: seq: tf.Tensor of shape (batch_size, seq_len)\n                Returns: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n                seq = tf.cast(tf.math.equal(seq, pad_ten), f_type)  \n                # For every item in the sequence, 1 if it is a padding token, 0 if it is not \n\n                # add extra dimensions to add the padding\n                \n                return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n        \n        # Encoder padding mask\n        padding_mask: tf.Tensor = create_padding_mask(inp)  # (batch_size, 1, 1, seq_len)\n\n        # Used in the 1st attention block in the decoder.\n        # It is used to pad and mask future tokens in the input received by\n        # the decoder.\n        set_size: int = tar.shape[1]\n\n        def create_look_ahead_mask(set_size: int) -> tf.Tensor:\n                mask = 1 - tf.linalg.band_part(tf.ones((set_size, set_size)), -1, 0)\n                mask = tf.cast(mask, dtype=f_type)\n                return mask  # (seq_len, seq_len)\n\n        look_ahead_mask = create_look_ahead_mask(set_size)  # (seq_len, seq_len)\n        dec_target_padding_mask = create_padding_mask(tar)  # (batch_size, 1, 1, seq_len)\n        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) # (batch_size, 1, 1, seq_len)\n\n        return padding_mask, look_ahead_mask","metadata":{"id":"BQkjgUVVckzJ","execution":{"iopub.status.busy":"2022-07-13T12:03:26.247683Z","iopub.execute_input":"2022-07-13T12:03:26.248511Z","iopub.status.idle":"2022-07-13T12:03:26.260160Z","shell.execute_reply.started":"2022-07-13T12:03:26.248468Z","shell.execute_reply":"2022-07-13T12:03:26.259182Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Layers and blocks","metadata":{"id":"9KHKaxz3xumc"}},{"cell_type":"code","source":"class ScaledDotProductAttention(layers.Layer):\n    def __init__(self, d_model: int, **kwargs):\n        super(ScaledDotProductAttention, self).__init__(**kwargs)\n        # scale = 1 / sqrt(d_model)\n        self.scale = tf.math.pow(tf.cast(d_model, f_type), -0.5)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n        \"\"\"Scaled Dot-Product Attention\n        input: \n        q: tf.Tensor of shape (batch_size, seq_len, d_model), \n        k: tf.Tensor of shape (batch_size, seq_len, d_model), \n        v: tf.Tensor of shape (batch_size, seq_len, d_model), \n        mask: Optional[tf.Tensor] of shape (batch_size, 1, 1, seq_len)\n        output: tf.Tensor of shape (batch_size, seq_len, d_model)\"\"\"\n        matmul_qk: tf.Tensor = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n        # q @ transpose(k)\n\n        # Scaled Dot-Product Attention\n        scaled_attention_logits: tf.Tensor = matmul_qk * self.scale  # (..., seq_len_q, seq_len_k)\n        # matmul_qk / sqrt(d_model)\n\n        # Masking\n        if mask is not None:\n            # noinspection PyTypeChecker\n            if f_type == \"float16\":\n                # tf.float16.min is minus infinity\n                scaled_attention_logits += (mask * tf.float16.min)  # changed from -1e9 to prevent nan's\n            else:\n                scaled_attention_logits += (mask * -1e9) \n\n        # Normalize\n        attention_weights = self.softmax(scaled_attention_logits)\n        # (..., seq_len_q, seq_len_k)\n\n        # Output\n        output = tf.matmul(attention_weights, v)\n\n        return output","metadata":{"id":"gRXst-o2NjXL","execution":{"iopub.status.busy":"2022-07-13T12:03:26.262152Z","iopub.execute_input":"2022-07-13T12:03:26.262630Z","iopub.status.idle":"2022-07-13T12:03:26.276963Z","shell.execute_reply.started":"2022-07-13T12:03:26.262576Z","shell.execute_reply":"2022-07-13T12:03:26.275566Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class MyMultiHeadAttention(Model):\n    \"\"\"U can use the built-in layers.multihead_attention but is caused a bug for me\"\"\"\n    def __init__(self, num_heads: int, d_model: int, **kwargs):\n        super(MyMultiHeadAttention, self).__init__(**kwargs)\n        if d_model % num_heads != 0:\n            raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\")\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.depth = d_model // self.num_heads\n\n        self.wq = layers.Dense(d_model)\n        self.wk = layers.Dense(d_model)\n        self.wv = layers.Dense(d_model)\n\n        self.dense = layers.Dense(d_model)\n        self.sdpa = ScaledDotProductAttention(d_model)\n\n        \n    def split_heads(self, x: tf.Tensor, batch_size: int) -> tf.Tensor:\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    \n    def call(self, v_k: tf.Tensor, q: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n        \"\"\"inputs:\n        v_k: tf.Tensor of shape (batch_size, seq_len, d_model) in self attention keys and values are the same\n        q: tf.Tensor of shape (batch_size, seq_len, d_model)\n        mask: Optional[tf.Tensor] of shape (batch_size, seq_len)\"\"\"\n        batch_size = tf.shape(q)[0]\n\n        q: tf.Tensor = self.wq(q)  # (batch_size, seq_len, d_model)\n        k: tf.Tensor = self.wk(v_k)  # (batch_size, seq_len, d_model)\n        v: tf.Tensor = self.wv(v_k)  # (batch_size, seq_len, d_model)\n\n        q: tf.Tensor = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k: tf.Tensor = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v: tf.Tensor = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape should be (batch_size, num_heads, seq_len_q, depth)\n        scaled_attention = self.sdpa(q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n         # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n          # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output","metadata":{"id":"_0I9SZI0kp31","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:03:26.278798Z","iopub.execute_input":"2022-07-13T12:03:26.279488Z","iopub.status.idle":"2022-07-13T12:03:26.297017Z","shell.execute_reply.started":"2022-07-13T12:03:26.279451Z","shell.execute_reply":"2022-07-13T12:03:26.295851Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class PointWiseFeedForward(Model):\n    def __init__(self, d_model: int, dff: int, **kwargs): \n        super(PointWiseFeedForward, self).__init__(**kwargs)\n        self.layer1 = layers.Dense(dff, activation=\"relu\")  # (batch_size, seq_len, dff)\n        self.layer2 = layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    \n    def call(self, x: tf.Tensor) -> tf.Tensor:\n        \"\"\"Gets and returns tensor of shape (batch_size, seq_len, d_model) and dtype keras.beckend.floatx()\"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return x","metadata":{"id":"QOVbpMxtNjXQ","execution":{"iopub.status.busy":"2022-07-13T12:03:26.298249Z","iopub.execute_input":"2022-07-13T12:03:26.298874Z","iopub.status.idle":"2022-07-13T12:03:26.312835Z","shell.execute_reply.started":"2022-07-13T12:03:26.298832Z","shell.execute_reply":"2022-07-13T12:03:26.311657Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(Model):\n    def __init__(self, d_model: int, num_heads: int, dff: int, drop_out_rate: float, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n\n        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n        self.ffn = PointWiseFeedForward(d_model, dff)\n\n        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout = layers.Dropout(drop_out_rate)\n\n    def call(self, x: tf.Tensor, training: bool, mask: tf.Tensor) -> tf.Tensor:\n        \n        attn_output = self.mha(x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout(attn_output, training=training)  # (batch_size, input_seq_len, d_model)\n        # out1 = self.layer_norm(x + attn_output)  # (batch_size, input_seq_len, d_model)\n        # might be data leak\n        out1 = self.layer_norm(attn_output)  # (batch_size, input_seq_len, d_model)\n        \n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout(ffn_output, training=training)  # (batch_size, input_seq_len, d_model)\n        out2 = self.layer_norm(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2","metadata":{"id":"8mEP3-Jx2n39","execution":{"iopub.status.busy":"2022-07-13T12:03:26.314708Z","iopub.execute_input":"2022-07-13T12:03:26.315203Z","iopub.status.idle":"2022-07-13T12:03:26.327776Z","shell.execute_reply.started":"2022-07-13T12:03:26.315147Z","shell.execute_reply":"2022-07-13T12:03:26.326488Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(Model):\n    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float, **kwargs):\n        super(DecoderBlock, self).__init__(**kwargs)\n\n        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n\n        self.ffn = PointWiseFeedForward(d_model, dff)\n\n        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout = layers.Dropout(rate)\n\n    def call(self, x: tf.Tensor, enc_output: tf.Tensor, look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor, training):\n        # enc_output.shape should be (batch_size, input_seq_len, d_model)\n\n        attn1 = self.mha(x, x, look_ahead_mask)  # (batch_size, set_size, d_model)\n        attn1 = self.dropout(attn1, training=training)  # (batch_size, set_size, d_model)\n        # out1 = self.layer_norm(attn1 + x)\n        # might be data leak\n        out1 = self.layer_norm(attn1)  # (batch_size, set_size, d_model)\n\n        attn2 = self.mha(enc_output, out1, padding_mask)  # (batch_size, set_size, d_model)\n        attn2 = self.dropout(attn2, training=training)  # (batch_size, set_size, d_model)\n        out2 = self.layer_norm(attn2 + out1)  # (batch_size, set_size, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, set_size, d_model)\n        ffn_output = self.dropout(ffn_output, training=training)\n        out3 = self.layer_norm(ffn_output + out2)  # (batch_size, set_size, d_model)\n\n        return out3 ","metadata":{"id":"xJWFrv0g281G","execution":{"iopub.status.busy":"2022-07-13T12:03:26.329259Z","iopub.execute_input":"2022-07-13T12:03:26.329625Z","iopub.status.idle":"2022-07-13T12:03:26.344242Z","shell.execute_reply.started":"2022-07-13T12:03:26.329592Z","shell.execute_reply":"2022-07-13T12:03:26.343297Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class Encoder(Model):\n    def __init__(self, pos_encoding: tf.Tensor, num_blocks: int, d_model: int, num_heads: int, dff: int, rate=0.1, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n        self.d_model = d_model\n        self.num_blocks = num_blocks\n        self.pos_encoding = pos_encoding\n\n        self.enc_blocks = [EncoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n        # the encoder \n        self.dropout = layers.Dropout(rate)\n        self.scale = tf.math.sqrt(tf.cast(self.d_model, f_type))\n\n    def call(self, x: tf.Tensor, training, mask: tf.Tensor) -> tf.Tensor:\n\n        seq_len = tf.shape(x)[1]\n\n        # adding position encoding.\n        # assert not tf.math.is_nan(x[0][0][0])\n        x *= self.scale\n        # assert not tf.math.is_nan(x[0][0][0])\n        \n        x += self.pos_encoding[:, :seq_len, :]  # (batch_size, input_seq_len, d_model)\n        # assert not tf.math.is_nan(x[0][0][0])\n        x = self.dropout(x, training=training)  # (batch_size, input_seq_len, d_model)\n        # assert not tf.math.is_nan(x[0][0][0])\n\n        for block in self.enc_blocks:\n            x = block(x, training, mask)  # (batch_size, input_seq_len, d_model)\n            # assert not tf.math.is_nan(x[0][0][0])\n\n        return x  # (batch_size, input_seq_len, d_model)  ","metadata":{"id":"NdlS1kfM3k5d","execution":{"iopub.status.busy":"2022-07-13T12:03:26.345994Z","iopub.execute_input":"2022-07-13T12:03:26.346766Z","iopub.status.idle":"2022-07-13T12:03:26.361310Z","shell.execute_reply.started":"2022-07-13T12:03:26.346715Z","shell.execute_reply":"2022-07-13T12:03:26.359929Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, pos_encoding, num_blocks: int, d_model: int, num_heads: int, dff: int,\n                 vocab_size: int, rate: float, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n\n        self.scale = tf.math.sqrt(tf.cast(d_model, f_type))\n        self.num_blocks = num_blocks\n        self.pos_encoding = pos_encoding\n\n        self.embedding = layers.Embedding(vocab_size, d_model)\n        self.dec_blocks = [DecoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n        self.dropout = layers.Dropout(rate)\n\n    def call(self, tar: tf.Tensor, enc_output: tf.Tensor, training: bool,\n             look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor) -> tf.Tensor:\n\n        seq_len = tf.shape(tar)[1]\n\n        x = self.embedding(tar)  # (batch_size, set_size, d_model)\n        x *= self.scale\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for block in self.dec_blocks:\n            x = block(x=x, enc_output=enc_output, look_ahead_mask=look_ahead_mask,\n                      padding_mask=padding_mask, training=training)\n\n        # x.shape should be (batch_size, set_size, d_model)\n        return x","metadata":{"id":"QLE8js6O3p5-","execution":{"iopub.status.busy":"2022-07-13T12:03:26.363072Z","iopub.execute_input":"2022-07-13T12:03:26.363935Z","iopub.status.idle":"2022-07-13T12:03:26.379375Z","shell.execute_reply.started":"2022-07-13T12:03:26.363879Z","shell.execute_reply":"2022-07-13T12:03:26.377688Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class EmbeddingTransposed(layers.Layer):\n    def __init__(self, tied_to: layers.Embedding = None, activation: Optional[str] = None, **kwargs):\n        super(EmbeddingTransposed, self).__init__(trainable=tied_to._trainable, **kwargs)\n        self.tied_to = tied_to\n        self.activation = keras.activations.get(activation)\n\n    def build(self, input_shape):\n        self.custom_weights = self.tied_to.weights[0]\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], keras.backend.int_shape(self.tied_to.weights[0])[0]\n\n    def call(self, inputs, mask=None):\n        output = keras.backend.dot(inputs, keras.backend.transpose(self.custom_weights))\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def get_config(self):\n        config = {\"activation\": keras.activations.serialize(self.activation)}\n        base_config = super(EmbeddingTransposed, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"id":"6uaYfyRdEtBN","execution":{"iopub.status.busy":"2022-07-13T12:03:26.381177Z","iopub.execute_input":"2022-07-13T12:03:26.381633Z","iopub.status.idle":"2022-07-13T12:03:26.396521Z","shell.execute_reply.started":"2022-07-13T12:03:26.381571Z","shell.execute_reply":"2022-07-13T12:03:26.395508Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## The full model","metadata":{"id":"1-xe3K7LyD2l"}},{"cell_type":"code","source":"def count_layers(my_model) -> int:\n    \"\"\"Counts the layers of a keras model recursizely\"\"\"\n    if not isinstance(my_model, keras.Model): \n        if isinstance(my_model, layers.Layer):\n            return 1\n        return 0\n    return sum([count_layers(sub_model) for sub_model in my_model.layers])","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:03:26.398117Z","iopub.execute_input":"2022-07-13T12:03:26.398758Z","iopub.status.idle":"2022-07-13T12:03:26.413036Z","shell.execute_reply.started":"2022-07-13T12:03:26.398722Z","shell.execute_reply":"2022-07-13T12:03:26.412148Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def line_prepender(_from: str, to: str, add: str):\n    with open(_from, 'r') as f:\n        content = f.read()\n    with open(to, \"w\") as f:\n        f.write(add + '\\n' + content)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:03:26.414665Z","iopub.execute_input":"2022-07-13T12:03:26.415033Z","iopub.status.idle":"2022-07-13T12:03:26.425778Z","shell.execute_reply.started":"2022-07-13T12:03:26.415001Z","shell.execute_reply":"2022-07-13T12:03:26.424479Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif device == \"kaggle\":\n    glove_txt_path = \"../input/glove6b/glove.6B.300d.txt\"\nelse:\n    !wget http://nlp.stanford.edu/data/glove.6B.300d.zip\n    !apt install unzip\n    !unzip \"glove.6B.300d.zip\"\n    glove_txt_path = \"glove.6B.300d.txt\"\npro_path = \"./edited_glove.txt\"\nout_put_path = \"./gensim_glove_vectors.txt\"\n\nglove2word2vec(glove_input_file=glove_txt_path, word2vec_output_file=out_put_path)\nline_prepender(glove_txt_path, pro_path, \"400000 300\")\nkeyed_vectors = KeyedVectors.load_word2vec_format(pro_path, binary=False)\nglove_embedding = keyed_vectors.get_keras_embedding()","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:03:26.427736Z","iopub.execute_input":"2022-07-13T12:03:26.428166Z","iopub.status.idle":"2022-07-13T12:03:42.997287Z","shell.execute_reply.started":"2022-07-13T12:03:26.428131Z","shell.execute_reply":"2022-07-13T12:03:42.995147Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"CPU times: user 5.55 s, sys: 10.3 s, total: 15.8 s\nWall time: 16.6 s\n","output_type":"stream"}]},{"cell_type":"code","source":"class SeTransformer(Model):\n    \"\"\"The base architecture of my models in this project.\"\"\"\n    def __init__(self, num_blocks: int, d_model: int, num_heads: int, dff: int,\n                 vocab_size: int, max_len: int, rate: float, pad_int: int, **kwargs):\n        super(SeTransformer, self).__init__(**kwargs)  # calls keras.Model's __init__ method with kwarg as key worg arguments\n        self.pad_int = pad_int\n        pos_encoding = create_positional_encoding(max_len, d_model)\n        self.encoder = Encoder(pos_encoding, num_blocks, d_model, num_heads, dff, rate)\n        self.decoder = Decoder(pos_encoding, num_blocks, d_model, num_heads, dff, vocab_size, rate)\n        self.embedding = glove_embedding\n        self.emb_trans = EmbeddingTransposed(self.embedding, \"softmax\")\n\n\n    def get_layer_count(self) -> int:\n        return count_layers(self)\n    \n    \n    def summary(self, **kwargs) -> None:\n        super().summary(**kwargs)\n        print(f\"The model have {self.get_layer_count()} layers\")\n        \n    \n    def count_params(self) -> int:\n        \"\"\"counts trainable parameters\n        Raises an error if caleed before building the model\"\"\"\n        param_count: int = self.encoder.count_params() + self.decoder.count_params() + self.embedding.count_params()\n        return param_count\n    \n    \n    def build_graph(self) -> keras.Model:\n        \"\"\"Returns a functional keras model identical to the model\"\"\"\n        inp = layers.Input(shape=(batch_size, max_seq_len))\n        tar = layers.Input(shape=(batch_size, set_size))\n        return keras.Model(inputs=[[inp, tar], True], outputs=self.call([inp, tar], True))\n    \n    \n    def call(self, inputs: List[tf.Tensor], training: bool) -> tf.Tensor:\n        inp, tar = inputs\n        # inp.shape should be (batch_size, max_seq_len)\n        # tar.shape should be (batch_size, set_size)\n        x = self.embedding(inp)  # (batch_size, max_seq_len, d_model)\n        padding_mask, look_ahead_mask = create_masks(inp, tar, self.pad_int)\n        enc_output = self.encoder(x, training, padding_mask)  # (batch_size, max_seq_len, d_model)\n        dec_output = self.decoder(tar, enc_output, training, look_ahead_mask, padding_mask)  # (batch_size, set_size, d_model)\n        final_output = self.emb_trans(dec_output)  # (batch_size, set_size, vocab_size)\n        return final_output","metadata":{"id":"4NLhyE0T3tUs","execution":{"iopub.status.busy":"2022-07-13T12:08:27.340524Z","iopub.execute_input":"2022-07-13T12:08:27.340990Z","iopub.status.idle":"2022-07-13T12:08:27.359542Z","shell.execute_reply.started":"2022-07-13T12:08:27.340955Z","shell.execute_reply":"2022-07-13T12:08:27.358056Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{"id":"hFxiRfCekp38"}},{"cell_type":"code","source":"# model_art = wandb.use_artifact(f\"{model_collection_name}:latest\")\n# model_path = model_art.get_path(\"model.pb\").download()\n# model = tf.saved_model.load(model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyper-Parameters","metadata":{"id":"B5rtigmHckyv"}},{"cell_type":"code","source":"set_size: int = 2\nlearning_rate: float = 0.005\n\nnum_sets: int = (max_seq_len // set_size) - 1 # Because we dont predict the first set\n# number of sets in each sequence\n\nnum_blocks: int = 16\nd_model: int = 300\ndff: int = 1024\nnum_heads: int = 30\ndropout_rate: float = 0.1","metadata":{"id":"bWmh89nVckyw","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:14:12.769552Z","iopub.execute_input":"2022-07-13T12:14:12.770222Z","iopub.status.idle":"2022-07-13T12:14:12.775943Z","shell.execute_reply.started":"2022-07-13T12:14:12.770166Z","shell.execute_reply":"2022-07-13T12:14:12.775096Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## Weights and Biases","metadata":{}},{"cell_type":"code","source":"if device == \"kaggle\":\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        os.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"WANDB_API_KEY\")\n    except Exception:\n        print(\"please enter your weights and biases API key\")\n!wandb login","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:14:16.638329Z","iopub.execute_input":"2022-07-13T12:14:16.638921Z","iopub.status.idle":"2022-07-13T12:14:21.965791Z","shell.execute_reply.started":"2022-07-13T12:14:16.638889Z","shell.execute_reply":"2022-07-13T12:14:21.964483Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myonikremer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":"# try: \n#     artifect = use_artifact(artifact, use_as=None)\n#     art = wandb.use_artifact(...)\n#     wandb.run.link_artifact(art, \"yonikremer/final_project_owned/version0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not \"run\" in globals():\n    run = wandb.init(\n        project=\"final_project_owned\",\n        entity=\"yonikremer\",\n        name=datetime.datetime.today().strftime(\"run from %d/%m/%Y\"),\n        settings=wandb.Settings(start_method=\"thread\"),\n        config = {\"set size\": set_size,\n                  \"batch size\": batch_size,\n                  \"learning rate\": learning_rate,\n                  \"max seq len\": max_seq_len,\n                  \"num blocks\": num_blocks,\n                  \"model dimention\": d_model,\n                  \"dff\": dff,\n                  \"num heads\": num_heads,\n                  \"dropout rate\": dropout_rate\n                  })\n    config = wandb.config","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:14:37.517216Z","iopub.execute_input":"2022-07-13T12:14:37.517670Z","iopub.status.idle":"2022-07-13T12:14:44.865507Z","shell.execute_reply.started":"2022-07-13T12:14:37.517621Z","shell.execute_reply":"2022-07-13T12:14:44.862291Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myonikremer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.12.21 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.20"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220713_121439-1zlywzf4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/yonikremer/final_project_owned/runs/1zlywzf4\" target=\"_blank\">run from 13/07/2022</a></strong> to <a href=\"https://wandb.ai/yonikremer/final_project_owned\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create the model","metadata":{"id":"O783T1xMckzR"}},{"cell_type":"code","source":"model = SeTransformer(\n    num_blocks=num_blocks,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    vocab_size=vocab_size,\n    max_len=max_seq_len,\n    rate=dropout_rate,\n    pad_int=pad_int)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, epsilon=keras.backend.epsilon())\nloss_func = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nacc = keras.metrics.SparseCategoricalAccuracy(dtype=f_type)\n\nmodel.compile(optimizer=optimizer,\n    loss=loss_func,\n    metrics=[acc]\n )","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:40:19.188891Z","iopub.execute_input":"2022-07-13T12:40:19.189303Z","iopub.status.idle":"2022-07-13T12:40:21.648125Z","shell.execute_reply.started":"2022-07-13T12:40:19.189270Z","shell.execute_reply":"2022-07-13T12:40:21.647177Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"temp_input = tf.random.uniform((1, 200), dtype=tf.int32, minval=1, maxval=6999)\ntemp_target = tf.random.uniform((1, 8), dtype=tf.int32, minval=1, maxval=6999)\ntemp_target2 = temp_target + 3\n\ntrain_pred1 = model([temp_input, temp_target], training=False)\ntrain_pred2 = model([temp_input, temp_target], training=False)\ntrain_pred3 = model([temp_input, temp_target2], training=False)\n\ntry:\n    tf.debugging.assert_equal(train_pred1, train_pred2)\n    print(\"the model is determenistic\")\n    try:\n        tf.debugging.assert_equal(train_pred1, train_pred3)\n    except tf.errors.InvalidArgumentError:\n        print(\"WARNING: model output might depends on the target\")\nexcept tf.errors.InvalidArgumentError:\n    print(\"The model is not determenistic and have sone random noise\")\nparam_count: int = model.count_params()\nprint(f\"The model has {param_count:,} = {round(param_count * (10**-6), 1)}M trainable parameters\")\nrun.config[\"parameters\"] = param_count\n\n# stats = FlopCoKeras(model)\n# flops_per_call: int = stats.total_flops\n# macs_per_call: int = stats.total_macs\n\n# # (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * (3 for forward and backward pass) * (number of examples in dataset) \n# training_flops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n# print(f\"FLOPs per call: {flops_per_call:,} = {(flops_per_call * (10 ** -6)):,}M\")\n# print(f\"MACs per call: {macs_per_call:,} = {(macs_per_call * (10 ** -6)):,}M\")\ndel temp_input, temp_target, temp_target2, train_pred1, train_pred2, train_pred3","metadata":{"id":"-uAxXGBtkp4A","pycharm":{"name":"#%%\n"},"outputId":"898c89bb-118b-4c84-e398-4171a2904b42","execution":{"iopub.status.busy":"2022-07-13T12:41:21.293381Z","iopub.execute_input":"2022-07-13T12:41:21.293801Z","iopub.status.idle":"2022-07-13T12:41:26.502135Z","shell.execute_reply.started":"2022-07-13T12:41:21.293768Z","shell.execute_reply":"2022-07-13T12:41:26.499448Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"the model is determenistic\nWARNING: model output might depends on the target\nThe model has 153,738,368 = 153.7M trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"# keras.utils.plot_model(\n#     model.build_graph(),\n#     show_shapes=True,\n#     show_dtype=True,\n#     show_layer_names=False,\n#     expand_nested=True,\n#     layer_range=None,\n#     show_layer_activations=True\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary(line_length=125, positions=[0.5, 0.66, 0.83, 1], expand_nested=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training helper functions","metadata":{"id":"5gjCRldwckzR"}},{"cell_type":"code","source":"pad_ten = tf.constant(pad_int, dtype=tf.int32)\npad_ten","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:49:36.742597Z","iopub.execute_input":"2022-07-13T12:49:36.743015Z","iopub.status.idle":"2022-07-13T12:49:38.535073Z","shell.execute_reply.started":"2022-07-13T12:49:36.742982Z","shell.execute_reply":"2022-07-13T12:49:38.533802Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(), dtype=int32, numpy=0>"},"metadata":{}}]},{"cell_type":"code","source":"@tf.function\ndef contains_pad(inp: tf.Tensor):\n    bool_ten = tf.math.equal(inp, pad_ten)\n    nonzero_count = tf.math.count_nonzero(bool_ten)\n    return nonzero_count > 0","metadata":{"id":"HZf9y_1nCtEa","execution":{"iopub.status.busy":"2022-07-13T12:49:42.796816Z","iopub.execute_input":"2022-07-13T12:49:42.797698Z","iopub.status.idle":"2022-07-13T12:49:44.516794Z","shell.execute_reply.started":"2022-07-13T12:49:42.797648Z","shell.execute_reply":"2022-07-13T12:49:44.515570Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{"id":"iRP0mGA1NjXi"}},{"cell_type":"code","source":"@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\ndef train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    with tf.GradientTape() as tape:\n        pred: tf.Tensor = model([inp, outp], training=True) \n        loss_val: tf.Tensor = loss_func(y_true = outp, y_pred = pred)\n    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    return tf.math.reduce_mean(loss_val), acc(outp, pred)","metadata":{"id":"UR6qYltHhh_t","execution":{"iopub.status.busy":"2022-07-13T12:49:46.826796Z","iopub.execute_input":"2022-07-13T12:49:46.827253Z","iopub.status.idle":"2022-07-13T12:49:48.777951Z","shell.execute_reply.started":"2022-07-13T12:49:46.827220Z","shell.execute_reply":"2022-07-13T12:49:48.777121Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\ndef train(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=f_type):\n    per_gen_loss: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    per_gen_acc: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    i = 0\n    while i < num_sets:\n        # The input is of size set_size-TAKE_TO_ACCOUNT\n        already_predicted: int = i * (set_size + 1)\n        start_from: int = max(0, already_predicted - max_seq_len)\n        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n            break\n        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n        loss_val, acc_val = train_step(inp, outp)\n        one_hot_loss = tf.one_hot([i], num_sets, dtype = f_type) * loss_val\n        one_hot_acc = tf.one_hot([i], num_sets, dtype = f_type) * acc_val\n        per_gen_loss += one_hot_loss\n        per_gen_acc += one_hot_acc\n        i += 1\n    return tf.math.reduce_mean(per_gen_loss[:i]), tf.math.reduce_mean(per_gen_acc[:i])\n    ","metadata":{"id":"sSQ8nWVwMuG_","execution":{"iopub.status.busy":"2022-07-13T12:49:50.361242Z","iopub.execute_input":"2022-07-13T12:49:50.362096Z","iopub.status.idle":"2022-07-13T12:49:51.993378Z","shell.execute_reply.started":"2022-07-13T12:49:50.362051Z","shell.execute_reply":"2022-07-13T12:49:51.992398Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain(tf.random.uniform(shape=[batch_size, max_seq_len], minval=0, maxval=vocab_size-1, dtype=tf.int32))","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:56:01.178791Z","iopub.execute_input":"2022-07-13T12:56:01.179274Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2022-07-13 12:56:02.014627: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Validate","metadata":{"id":"N9vu221UNjXk"}},{"cell_type":"code","source":"@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\ndef val_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    pred = model([inp, outp], training=False)\n    loss_val = loss_func(y_true = outp, y_pred = pred)\n    return tf.math.reduce_mean(loss_val), acc(outp, pred)","metadata":{"id":"gPYi_RT4Ytv_","execution":{"iopub.status.busy":"2022-07-13T12:42:55.999402Z","iopub.execute_input":"2022-07-13T12:42:55.999851Z","iopub.status.idle":"2022-07-13T12:42:57.772366Z","shell.execute_reply.started":"2022-07-13T12:42:55.999815Z","shell.execute_reply":"2022-07-13T12:42:57.771255Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\ndef validate(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=f_type):\n    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    per_gen_acc: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    i = 0\n    while i < num_sets:\n        # The input is of size set_size-TAKE_TO_ACCOUNT\n        already_predicted: int = i * (set_size + 1)\n        start_from: int = max(0, already_predicted - max_seq_len)\n        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n            break\n        loss_val, acc_val = val_step(inp, outp)\n        one_hot_loss = tf.one_hot([i], num_sets, dtype=f_type) * loss_val\n        one_hot_acc = tf.one_hot([i], num_sets, dtype=f_type) * acc_val\n        per_gen_loss += one_hot_loss\n        per_gen_acc += one_hot_acc\n        i += 1\n    return tf.math.reduce_mean(per_gen_loss[:i]), tf.math.reduce_mean(per_gen_acc[:i])","metadata":{"id":"7NpdkcUkckzT","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:43:03.482918Z","iopub.execute_input":"2022-07-13T12:43:03.483367Z","iopub.status.idle":"2022-07-13T12:43:05.054057Z","shell.execute_reply.started":"2022-07-13T12:43:03.483327Z","shell.execute_reply":"2022-07-13T12:43:05.053122Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"%%time\nvalidate(tf.random.uniform(shape=[batch_size, max_seq_len], minval=0, maxval=vocab_size-1, dtype=tf.int32))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Callbacks","metadata":{"id":"cEcysTey1bEk"}},{"cell_type":"code","source":"best_loss = float(\"inf\")\nbest_model = None","metadata":{"id":"Iuce_A94ckzU","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:43:13.225564Z","iopub.execute_input":"2022-07-13T12:43:13.225983Z","iopub.status.idle":"2022-07-13T12:43:15.086506Z","shell.execute_reply.started":"2022-07-13T12:43:13.225949Z","shell.execute_reply":"2022-07-13T12:43:15.085241Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def check_point(model: SeTransformer, train_loss, train_acc, val_loss, val_acc, test_loss = None, test_acc = None):\n    \"\"\"Saves the model at the end of each epoch\"\"\"\n    # (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * \n    # * (3 for forward and backward pass) * (number of examples in dataset)\n    global best_model\n    global last_save_time\n    last_save_time = time.time()\n    best_model = model\n    num_ops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n    keras.models.save_model(model = model, filepath = \"model.pb\", save_format=\"tf\", overwrite=True)\n    wandb.log({\"model train loss\": train_loss, \"model train acc\": train_acc, \"model val loss\": val_loss, \"model val acc\": val_acc, \n              \"model test loss\": test_loss, \"model test acc\": test_acc})\n    art = wandb.Artifact(f\"{wandb.run.id}-best model\", type=\"my_model\", description = f\"the model after {num_ops:,} operations\")\n    art.add_file(\"model.pb\")\n    run.log_artifact(artifact)\n    print(\"Saved checkpoint\")","metadata":{"id":"LZsEGz_vckzT","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T12:43:20.317717Z","iopub.execute_input":"2022-07-13T12:43:20.318143Z","iopub.status.idle":"2022-07-13T12:43:22.167212Z","shell.execute_reply.started":"2022-07-13T12:43:20.318111Z","shell.execute_reply":"2022-07-13T12:43:22.166299Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"def on_val_batch_end(train_loss: float, train_acc: float ,val_loss: float, val_acc: float) -> bool:\n    \"\"\"A callback after every val batch\n    returns True if the model should stop training and False else\"\"\"\n    global best_loss\n    global best_model\n    global last_save_time\n    if time.time() - last_save_time > 1800.0 and val_loss < math.log(vocab_size) and val_loss < best_loss:\n        best_loss = val_loss\n        # If the last save is more than a half hour (1800 sec) ago\n        # and if the predictions are better than randon and \n        check_point(model, train_loss, train_acc, val_loss, val_acc)\n        return False\n    elif train_loss < 0.01:\n        title: str = \"Over fitting or data leak\"\n        message = f\"Training loss is {train_loss} and val loss is {val_loss} in the latest batch\"\n        wandb.alert(title=title, text=message)\n        print(title)\n        print(message)\n        return True\n    elif time.time() - last_save_time > 18000.0 and train_loss >= math.log(vocab_size):\n        # if the prob of every token is 1/vocab_size, the loss is\n        # -ln(1/vocab_size) = ln(vocab_size) \n        # by the logrithem rule log(a^x)=xlog(a) where x = -1\n        # if after 5 hours of training, the model predictions are still random\n        title: str = \"Under fitting\"\n        message = f\"train loss: {train_loss} train acc: {train_acc}, val loss: {val_loss}, val acc: {val_acc} in the latest batch\"\n        wandb.alert(title=title, text=message)\n        print(title)\n        print(message)\n        return True\n    return Flase","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:43:25.339544Z","iopub.execute_input":"2022-07-13T12:43:25.340007Z","iopub.status.idle":"2022-07-13T12:43:27.567086Z","shell.execute_reply.started":"2022-07-13T12:43:25.339970Z","shell.execute_reply":"2022-07-13T12:43:27.565853Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"## The actual training loop!","metadata":{"id":"RN8rmbrNW0-Z"}},{"cell_type":"code","source":"def train_loop():\n    epochs: int = 1000000  # Train until the cloud disconnects or the model stops improving\n    per_epoch_train_loss: List[float] = []\n    per_epoch_val_loss: List[float] = []\n    per_epoch_train_acc: List[float] = []\n    per_epoch_val_acc: List[float] = []\n    print(f\"number of train batches per epoch: {len(list_train_set)}\")\n    last_save_time = time.time()\n    for epoch in range(epochs):\n        print(f\"epoch number: {epoch}\")\n        per_batch_train_loss: List[float] = []\n        per_batch_val_loss: List[float] = []\n        per_batch_train_acc: List[float] = []\n        per_batch_val_acc: List[float] = []\n        for batch_num in tqdm.tqdm(range(len(list_train_set))):  # tqdm is a progress bar\n            train_loss, train_acc = train(list_train_set[batch_num])\n            float_train_loss = keras.backend.eval(train_loss).item()\n            per_batch_train_loss.append(float_train_loss)\n            float_train_acc = keras.backend.eval(train_acc).item()\n            per_batch_train_acc.append(float_train_acc)\n            wandb.log({\"epoch\": epoch, \"batch\": batch_num, \"batch train loss\": float_train_loss, \"batch train_acc\": float_train_acc})\n            if batch_num % 8 == 0:  # 8 = #training batches/#val batches\n                # because training set is 80% of the data and val set is 10%\n                next_val_batch: tf.Tensor = list_val_set[batch_num // 8]\n                val_loss, val_acc = validate(next_val_batch)\n                float_val_loss = keras.backend.eval(val_loss).item()\n                per_batch_val_loss.append(float_val_loss)\n                float_val_acc = keras.backend.eval(val_acc).item()\n                per_batch_val_acc.append(float_val_acc)\n                wandb.log({\"epoch\": epoch, \"batch\": batch_num, \"batch val loss\": float_val_loss, \"batch train_acc\": float_val_acc})\n                on_val_batch_end(float_train_loss, float_train_acc, float_val_loss, float_val_acc)\n        epoch_train_loss = statistics.mean(per_batch_train_loss)\n        epoch_train_acc = statistics.mean(per_batch_train_acc)\n        epoch_val_loss = statistics.mean(per_batch_val_loss)\n        epoch_val_acc = statistics.mean(per_batch_val_acc)\n        per_epoch_train_loss.append(epoch_train_loss)\n        per_epoch_train_acc.append(epoch_train_acc)\n        per_epoch_val_loss.append(epoch_val_loss)\n        per_epoch_val_acc.append(epoch_val_acc)\n        print(f\"train loss: {epoch_train_loss}\")\n        print(f\"train acc: {epoch_train_acc}\")\n        print(f\"val loss: {epoch_val_loss}\")\n        print(f\"val acc: {epoch_val_acc}\")\n        if len(per_epoch_val_loss) > 1:\n            if epoch_val_loss >= per_epoch_val_loss[-2]:\n                print(\"Validation loss increased. Stopped training\")\n                return epoch_train_loss, epoch_val_loss, epoch_train_acc, epoch_val_acc\n        print(\"Saved checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:43:32.522306Z","iopub.execute_input":"2022-07-13T12:43:32.522734Z","iopub.status.idle":"2022-07-13T12:43:34.330128Z","shell.execute_reply.started":"2022-07-13T12:43:32.522702Z","shell.execute_reply":"2022-07-13T12:43:34.328917Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"!export AUTOGRAPH_VERBOSITY=10\nif \"strategy\" in globals():\n    with strategy.scope():\n        train_loss, val_loss, train_acc, val_acc = train_loop()\nelse:\n    train_loss, val_loss, train_acc, val_acc = train_loop()","metadata":{"id":"g5MukK3ynG7v","outputId":"dca87c06-0140-41d9-c5bd-1e71ea9c9289","execution":{"iopub.status.busy":"2022-07-13T12:44:02.816885Z","iopub.execute_input":"2022-07-13T12:44:02.817326Z","iopub.status.idle":"2022-07-13T12:44:05.805716Z","shell.execute_reply.started":"2022-07-13T12:44:02.817287Z","shell.execute_reply":"2022-07-13T12:44:05.803765Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"number of train batches per epoch: 295\nepoch number: 0\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/295 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function train at 0x7f0553964200> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: name 'fscope' is not defined\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/295 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_135/291670250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_135/2331907470.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mper_batch_val_acc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_train_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# tqdm is a progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_train_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mfloat_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mper_batch_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: in user code:\n\n    /tmp/ipykernel_135/1550445663.py:3 contains_pad  *\n        bool_ten = tf.math.equal(inp, pad_ten)\n\n    NameError: name 'pad_ten' is not defined\n"],"ename":"NameError","evalue":"in user code:\n\n    /tmp/ipykernel_135/1550445663.py:3 contains_pad  *\n        bool_ten = tf.math.equal(inp, pad_ten)\n\n    NameError: name 'pad_ten' is not defined\n","output_type":"error"}]},{"cell_type":"markdown","source":"## After training","metadata":{"id":"Z-iopedNNjXs"}},{"cell_type":"code","source":"if best_model:\n    global model\n    model = tf.lite.TFLiteConverter.from_saved_model(\"model.pb\")\n    test_loss, test_acc = statistics.mean([validate(test_batch) for test_batch in tqdm.tqdm(list_test_set)])\n    print(f\"Test loss: {test_loss}\")\n    print(f\"Test acc: {test_acc}\")\n    wandb.log({\"test loss\": test_loss, \"text acc\": test_acc})","metadata":{"id":"1VtC8sP_NjXu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}