{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"ZZf-fbj50Kqc"}},{"cell_type":"code","source":"import sys\ndevice: str\nif 'google.colab' in sys.modules.keys():\n    device = 'colab'\nif 'kaggle_web_client' in sys.modules.keys():\n    device = 'kaggle'\nelse:\n    device = 'locally'","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:02:10.003173Z","iopub.execute_input":"2022-07-11T13:02:10.003749Z","iopub.status.idle":"2022-07-11T13:02:10.010143Z","shell.execute_reply.started":"2022-07-11T13:02:10.003707Z","shell.execute_reply":"2022-07-11T13:02:10.008759Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%time\nif device == 'kaggle':\n    !conda create -y -q --name create_models_venv numpy tensorflow pandas matplotlib seaborn\n    !conda init -y -q create_models_venv\n    !conda activate -y -q create_models_venv\n    !pip install -q tensorflow-text\n    !pip install -q wandb\n    !pip install -q tqdm\n    !pip install -q kaggle\nelif device == 'colab':\n    %pip install -q tensorflow-text\n    %pip install -q tqdm\n    %pip install -q wandb --upgrade\n    %pip install -q flopco-keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if device == 'kaggle':\n    !pip install -q tensorflow-text\n    !pip install -q wandb\n    !pip install -q tqdm","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:02:52.597316Z","iopub.execute_input":"2022-07-11T13:02:52.597781Z","iopub.status.idle":"2022-07-11T13:03:22.598460Z","shell.execute_reply.started":"2022-07-11T13:02:52.597743Z","shell.execute_reply":"2022-07-11T13:03:22.597268Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\nCommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init <SHELL_NAME>\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install flopco-keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda remove -y cudatoolkit\n!conda install -y cudnn","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:07:08.872259Z","iopub.execute_input":"2022-07-11T13:07:08.872714Z","iopub.status.idle":"2022-07-11T13:14:30.225970Z","shell.execute_reply.started":"2022-07-11T13:07:08.872679Z","shell.execute_reply":"2022-07-11T13:14:30.224476Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting package metadata (repodata.json): done\nSolving environment: failed\n\nPackagesNotFoundError: The following packages are missing from the target environment:\n  - cudatoolkit\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - cudnn\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    cudatoolkit-11.1.74        |       h6bb024c_0        1.19 GB  nvidia\n    cudnn-8.0.4                |       cuda11.1_0       713.6 MB  nvidia\n    ------------------------------------------------------------\n                                           Total:        1.88 GB\n\nThe following NEW packages will be INSTALLED:\n\n  cudatoolkit        nvidia/linux-64::cudatoolkit-11.1.74-h6bb024c_0\n  cudnn              nvidia/linux-64::cudnn-8.0.4-cuda11.1_0\n\nThe following packages will be UPDATED:\n\n  openssl                                 1.1.1o-h166bdaf_0 --> 1.1.1q-h166bdaf_0\n\n\n\nDownloading and Extracting Packages\ncudatoolkit-11.1.74  | 1.19 GB   | ##################################### | 100% \ncudnn-8.0.4          | 713.6 MB  | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: / By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n\ndone\n","output_type":"stream"}]},{"cell_type":"code","source":"# standart liberies:\nfrom typing import Optional, List, Set, Dict, Tuple\nimport datetime\nimport os\nimport random\nimport statistics\nimport math\nimport time\n# NON-standart liberies:\n# from flopco_keras import FlopCoKeras  # flop counter for keras at https://github.com/evgps/flopco-keras\nimport wandb\nimport tensorflow as tf\nfrom tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\nfrom tensorflow_text import FastWordpieceTokenizer\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport tqdm\nimport matplotlib.pyplot as plt","metadata":{"gather":{"logged":1644854036589},"id":"MsfMr-Qod_nl","outputId":"e09897f8-e611-4818-b6b6-adf1e370cccf","execution":{"iopub.status.busy":"2022-07-11T13:17:49.412873Z","iopub.execute_input":"2022-07-11T13:17:49.413432Z","iopub.status.idle":"2022-07-11T13:17:57.101369Z","shell.execute_reply.started":"2022-07-11T13:17:49.413389Z","shell.execute_reply":"2022-07-11T13:17:57.099796Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2022-07-11 13:17:51.398820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-07-11 13:17:51.398908: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Python version: {sys.version}\")\nprint(f\"Tensorflow version: {tf.__version__}\")","metadata":{"gather":{"logged":1644854037100},"id":"SzaglEQ-kp2n","outputId":"5b7fd3b9-3369-431c-b516-bde95519988f","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:18:05.183692Z","iopub.execute_input":"2022-07-11T13:18:05.184546Z","iopub.status.idle":"2022-07-11T13:18:05.190867Z","shell.execute_reply.started":"2022-07-11T13:18:05.184502Z","shell.execute_reply":"2022-07-11T13:18:05.189667Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n[GCC 9.4.0]\nTensorflow version: 2.9.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Settings","metadata":{"id":"91ZJ0-RZIx6j"}},{"cell_type":"code","source":"tf.random.set_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\ntf.keras.utils.set_random_seed(0)\ntf.config.experimental.enable_op_determinism()\n# tf.keras.backend.set_floatx('float16')","metadata":{"gather":{"logged":1644854038983},"id":"2mp2yFTEkp2u","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:18:09.850269Z","iopub.execute_input":"2022-07-11T13:18:09.851486Z","iopub.status.idle":"2022-07-11T13:18:09.858761Z","shell.execute_reply.started":"2022-07-11T13:18:09.851431Z","shell.execute_reply":"2022-07-11T13:18:09.857716Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Define a strategy - Accelerator optimization ","metadata":{"id":"QlubPo7LRyNm"}},{"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver();\ntf.config.experimental_connect_to_cluster(resolver);\ntf.tpu.experimental.initialize_tpu_system(resolver);\nstrategy = tf.distribute.TPUStrategy(resolver);","metadata":{"id":"s0yqg12zqSt9","execution":{"iopub.status.busy":"2022-07-11T13:18:13.337469Z","iopub.execute_input":"2022-07-11T13:18:13.338213Z","iopub.status.idle":"2022-07-11T13:18:13.649919Z","shell.execute_reply.started":"2022-07-11T13:18:13.338161Z","shell.execute_reply":"2022-07-11T13:18:13.648484Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_8498/1622343764.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_connect_to_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_tpu_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    202\u001b[0m           \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m           discovery_url=discovery_url)\n\u001b[0m\u001b[1;32m    205\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud_tpu_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/cloud_tpu_client/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tpu, zone, project, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtpu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please provide a TPU Name to connect to.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Please provide a TPU Name to connect to."],"ename":"ValueError","evalue":"Please provide a TPU Name to connect to.","output_type":"error"}]},{"cell_type":"markdown","source":"# Data loading","metadata":{"id":"yVR1oJRD0NtO","_kg_hide-input":true}},{"cell_type":"code","source":"if device == 'colab':  # If notebook is ran on colab\n    from google.colab import drive\n    drive.mount('/drive')\n    df: pd.DataFrame = pd.read_csv('/drive/MyDrive/final_project/wikipedia_articles.csv')\nelif device == 'kaggle':\n    df: pd.DataFrame = pd.read_csv('../input/wikipedia-promotional-articles/promotional.csv')\nelse:  # If notebook is ran on my laptop\n    df: pd.DataFrame = pd.read_csv('wiki_data/articles.csv')\nprint(f\"the shape of the dataframe: {df.shape}\")","metadata":{"gather":{"logged":1644857858611},"id":"pn1UfAXQwlQX","outputId":"af20cbcb-3d71-4d4b-be9a-5850b9f9741c","execution":{"iopub.status.busy":"2022-07-11T13:18:34.287518Z","iopub.execute_input":"2022-07-11T13:18:34.288190Z","iopub.status.idle":"2022-07-11T13:18:36.913668Z","shell.execute_reply.started":"2022-07-11T13:18:34.288141Z","shell.execute_reply":"2022-07-11T13:18:36.912201Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"the shape of the dataframe: (23837, 7)\n","output_type":"stream"}]},{"cell_type":"code","source":"df: pd.Series = df['text']\ndata_list: List[str] = df.to_list()\nDATA_SIZE = len(data_list)\nprint(f\"There are {DATA_SIZE} data points\")\nstring_lengths: List[int] = [len(data_point) for data_point in data_list]\nmax_string_len = max(string_lengths)\nprint(f\"The length of the longest text IN CHARACTERS is: {max_string_len}\")\nmin_string_len = min(string_lengths)\nprint(f\"The length of the shortest text IN CHARACTERS is: {min_string_len}\")","metadata":{"id":"IW43lG6iUV6G","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"outputId":"3fad9123-c6f0-4cf6-f625-6d22d0241bda","collapsed":false,"execution":{"iopub.status.busy":"2022-07-11T13:18:37.152585Z","iopub.execute_input":"2022-07-11T13:18:37.153349Z","iopub.status.idle":"2022-07-11T13:18:37.184637Z","shell.execute_reply.started":"2022-07-11T13:18:37.153271Z","shell.execute_reply":"2022-07-11T13:18:37.183352Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"There are 23837 data points\nThe length of the longest text IN CHARACTERS is: 147041\nThe length of the shortest text IN CHARACTERS is: 38\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating the vocabulary","metadata":{"id":"87Putqz6kp20"}},{"cell_type":"code","source":"# %%time\nbert_tokenizer_params: dict = dict(lower_case=True)\nVOCAB_SIZE: int = 8192  # Always the same for all models\n\nif device == 'colab':  # If notebook is ran on colab\n    path = '/drive/MyDrive/final_project/vocab.txt'\nif device == 'kaggle':\n    path = \"../input/my-lookup-table/look_up_table.txt\"\nelse:  # If notebook is ran on my laptop\n    path = 'C:/yoni/final_project/model/vocab.txt'\n\n\nreserved_tokens: List[str] = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", \"[MASK]\"]\n\nif os.path.exists(path):\n    with open(path, 'r') as f:\n        vocab: List[str] = f.read().split()\nelse:\n    bert_vocab_args: dict = dict(\n        # The target vocabulary size\n        vocab_size = VOCAB_SIZE,\n        # Reserved tokens that must be included in the vocabulary\n        reserved_tokens=reserved_tokens,\n        # Arguments for `tf_text.BertTokenizer`\n        bert_tokenizer_params=bert_tokenizer_params,\n        # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n        learn_params={},\n    )\n    tensor_list: list = [tf.convert_to_tensor(data_point) for data_point in data_list]\n    data_set: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tensor_list)\n    # I already ran this code and saved the file to C:/yoni/final_project/model/vocab.txt\n    vocab: List[str] = bert_vocab_from_dataset(\n        data_set,\n        **bert_vocab_args,)\n    with open('C:/yoni/final_project/model/vocab.txt', 'w') as f:\n        for token in vocab:\n            f.write(token + ' ')","metadata":{"id":"KBhC3SPyMi9n","execution":{"iopub.status.busy":"2022-07-11T13:18:54.971119Z","iopub.execute_input":"2022-07-11T13:18:54.972000Z","iopub.status.idle":"2022-07-11T13:18:54.995138Z","shell.execute_reply.started":"2022-07-11T13:18:54.971954Z","shell.execute_reply":"2022-07-11T13:18:54.993675Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(f\"the type of the items in vocab: {type(vocab[0])}\")\nprint(f\"the first 15 items in vocab: {vocab[:15]}\")\nvocab_size = len(vocab)\nprint(f\" the length of vocab: {vocab_size}\")","metadata":{"id":"4IZ5uxlJEoi3","outputId":"a6886ce0-ea09-47e8-8ad6-3cc9e0bb8d25","execution":{"iopub.status.busy":"2022-07-11T13:18:58.971354Z","iopub.execute_input":"2022-07-11T13:18:58.971783Z","iopub.status.idle":"2022-07-11T13:18:58.978104Z","shell.execute_reply.started":"2022-07-11T13:18:58.971751Z","shell.execute_reply":"2022-07-11T13:18:58.976770Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"the type of the items in vocab: <class 'str'>\nthe first 15 items in vocab: ['[PAD]', '[UNK]', '[START]', '[END]', '[MASK]', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6']\n the length of vocab: 7882\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating the tokenizer","metadata":{"id":"lD6QCFLAkp2_"}},{"cell_type":"code","source":"from tensorflow_text import FastWordpieceTokenizer\ntokenizer = FastWordpieceTokenizer(vocab, support_detokenization=True, token_out_type=tf.int32)","metadata":{"id":"HihwIsrKD7C9","execution":{"iopub.status.busy":"2022-07-11T13:30:58.201936Z","iopub.execute_input":"2022-07-11T13:30:58.202716Z","iopub.status.idle":"2022-07-11T13:30:58.319398Z","shell.execute_reply.started":"2022-07-11T13:30:58.202673Z","shell.execute_reply":"2022-07-11T13:30:58.318149Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"tokenizer.tokenize(\"my string\")","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:31:01.278730Z","iopub.execute_input":"2022-07-11T13:31:01.279287Z","iopub.status.idle":"2022-07-11T13:31:01.291026Z","shell.execute_reply.started":"2022-07-11T13:31:01.279229Z","shell.execute_reply":"2022-07-11T13:31:01.289711Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 488, 5450], dtype=int32)>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizing the data","metadata":{"id":"A73Lbgg6kp3R"}},{"cell_type":"code","source":"# START: int = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")  # The value of the start token\n# END: int = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")  # The value of the end token\n# starts = tf.cast(tf.Variable([START]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\n# ends = tf.cast(tf.Variable([END]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\nstarts = tf.constant([2], dtype=tf.int32)\nends = tf.constant([3], dtype=tf.int32)\npad_int: int = int(tf.argmax(tf.constant(reserved_tokens) == \"[PAD]\"))\npad_ten: tf.TensorSpec(dtype=tf.int32, shape=()) = tf.constant([pad_int], dtype=tf.int32)","metadata":{"id":"mLxV5l562-x2","execution":{"iopub.status.busy":"2022-07-11T13:31:08.124884Z","iopub.execute_input":"2022-07-11T13:31:08.126766Z","iopub.status.idle":"2022-07-11T13:31:08.137668Z","shell.execute_reply.started":"2022-07-11T13:31:08.126701Z","shell.execute_reply":"2022-07-11T13:31:08.136508Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def tokenize_string(text: str) -> tf.Tensor:\n    \"\"\"Converts string to tensor\"\"\"\n#     ragged: tf.RaggedTensor = tokenizer.tokenize(text)\n#     eager: tf.Tensor = ragged.to_tensor(default_value=0, shape=[None, 1])  # 0 is the value of the padding token\n#     sqeezed: tf.Tensor = tf.squeeze(eager, axis=1)\n#     typed: tf.Tensor = tf.cast(sqeezed, tf.int32)\n    tokens = tokenizer.tokenize(text)\n    edited: tf.Tensor = tf.concat([starts, tokens, ends], axis=0)\n    return edited","metadata":{"id":"4gPxVc7tkp3X","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:31:40.432049Z","iopub.execute_input":"2022-07-11T13:31:40.432856Z","iopub.status.idle":"2022-07-11T13:31:40.438907Z","shell.execute_reply.started":"2022-07-11T13:31:40.432807Z","shell.execute_reply":"2022-07-11T13:31:40.437974Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"tokenized_data: List[tf.Tensor] = [tokenize_string(data_point) for data_point in tqdm.tqdm(data_list)] \n# tqdm is a progress bar\nprint(f\"An example of a shape of tokens tensor: {tokenized_data[0].shape}\")\nprint(f\"examples of tokens tensor: {tokenized_data[0][:10]}\")\nlengths_tokenized: List[int] = [text.shape[0] for text in tokenized_data]\nprint(f\"The maximum length of a tokens tensor: {max(lengths_tokenized)}\")\nprint(f\"The minimum length of a tokens tensor: {min(lengths_tokenized)}\")\nsns.displot(lengths_tokenized);","metadata":{"id":"UzHmzHSukp3a","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:33:33.309309Z","iopub.execute_input":"2022-07-11T13:33:33.310396Z","iopub.status.idle":"2022-07-11T13:33:51.428418Z","shell.execute_reply.started":"2022-07-11T13:33:33.310344Z","shell.execute_reply":"2022-07-11T13:33:51.427141Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"100%|██████████| 23837/23837 [00:16<00:00, 1454.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"An example of a shape of tokens tensor: (782,)\nexamples of tokens tensor: [   2    9    1  137    1    9    6 7461    7    9]\nThe maximum length of a tokens tensor: 33335\nThe minimum length of a tokens tensor: 9\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/23837 [04:22<?, ?it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 360x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"### chunk too long texts","metadata":{"id":"EtjxgLWMkp3l"}},{"cell_type":"code","source":"max_seq_len: int = 256\ndef chunk_tensor(tensor: tf.Tensor, max_len: int = max_seq_len) -> List[tf.Tensor]:\n    \"\"\"Splits 1d tensor to chunks (1d tensors) of maximum size: max_len\"\"\"\n    return [tensor[i*max_len:(i+1)*max_len] for i in range(tensor.shape[0] // max_len)]","metadata":{"id":"MubdHBEMkJNC","execution":{"iopub.status.busy":"2022-07-11T13:34:26.079322Z","iopub.execute_input":"2022-07-11T13:34:26.080026Z","iopub.status.idle":"2022-07-11T13:34:26.090925Z","shell.execute_reply.started":"2022-07-11T13:34:26.079952Z","shell.execute_reply":"2022-07-11T13:34:26.089208Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"chunked_data: List[tf.Tensor] = []\nfor tensor in tokenized_data:\n    chunks = chunk_tensor(tensor, max_seq_len)\n    for chunk in chunks:\n        chunked_data.append(chunk)\nDATA_SIZE: int = len(chunked_data)\nprint(f\"The data set size after chunking {DATA_SIZE}\")\nprint(f\"An example of a chunked shape{chunked_data[0].shape}\")","metadata":{"id":"9DgU2Or2kp3m","outputId":"596803b2-a306-4ef7-92c0-a497ecc507e3","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:34:28.969872Z","iopub.execute_input":"2022-07-11T13:34:28.970394Z","iopub.status.idle":"2022-07-11T13:34:42.398406Z","shell.execute_reply.started":"2022-07-11T13:34:28.970354Z","shell.execute_reply":"2022-07-11T13:34:42.397057Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"The data set size after chunking 80656\nAn example of a chunked shape(256,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Padding","metadata":{"id":"R8Z2_P2Lnke8"}},{"cell_type":"code","source":"def pad(tensor: tf.Tensor, pad_int: int) -> tf.Tensor:\n    \"\"\"Pads the tensor to the length of the longest text in the data set\"\"\"\n    padded: tf.Tensor = tf.pad(tensor=tensor, paddings=[[pad_int, max_seq_len - tensor.shape[0]]], mode='CONSTANT', constant_values=0)\n    # 0 is the padding token\n    return padded","metadata":{"id":"XUOVoGBfkp3p","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:34:46.529585Z","iopub.execute_input":"2022-07-11T13:34:46.530039Z","iopub.status.idle":"2022-07-11T13:34:46.537873Z","shell.execute_reply.started":"2022-07-11T13:34:46.530002Z","shell.execute_reply":"2022-07-11T13:34:46.536235Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"padded_data: List[tf.Tensor] = [pad(text, pad_int) for text in chunked_data]\nchunked_data.sort(key = lambda t: t.shape[0])  # sorting so that every batch will have similar sized texts, used when training","metadata":{"id":"nRa9_ryHkp3p","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:34:50.078198Z","iopub.execute_input":"2022-07-11T13:34:50.079483Z","iopub.status.idle":"2022-07-11T13:34:58.922241Z","shell.execute_reply.started":"2022-07-11T13:34:50.079433Z","shell.execute_reply":"2022-07-11T13:34:58.921166Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Train test val split","metadata":{"id":"gbnub0UZkp3s"}},{"cell_type":"code","source":"batch_size: int = 128\n\ndef list_to_dataset(tokenized_list: List[tf.Tensor]) -> tf.data.Dataset:\n    \"\"\"Converts a list of tokenized texts after all preprocessing to a tf.data.Dataset\"\"\"\n    dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tokenized_list)\n    batched: tf.data.Dataset = dataset.batch(batch_size)\n    return batched\n\nbatched_data_ten = list_to_dataset(padded_data)\nbatched_data_list = list(batched_data_ten)\nrandom.shuffle(batched_data_list)\nif batched_data_list[-1].shape[0] != batch_size:\n    batched_data_list = batched_data_list[:-1]\ndata_size = len(batched_data_list)\ntrain_size: int = int(data_size * 0.8) \nval_test_size: int = int(data_size * 0.1)  # Both validation and test get 10% of the data\nlist_train_set: List[tf.Tensor] = batched_data_list[:train_size]\nlist_val_set: List[tf.Tensor] = batched_data_list[train_size:(train_size + val_test_size)]\nlist_test_set: List[tf.Tensor] = batched_data_list[(train_size + val_test_size)]","metadata":{"id":"pTW48b8Qr6sv","execution":{"iopub.status.busy":"2022-07-11T13:35:56.815435Z","iopub.execute_input":"2022-07-11T13:35:56.816780Z","iopub.status.idle":"2022-07-11T13:35:59.513742Z","shell.execute_reply.started":"2022-07-11T13:35:56.816727Z","shell.execute_reply":"2022-07-11T13:35:59.512542Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"2022-07-11 13:35:58.794763: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 82591744 exceeds 10% of free system memory.\n2022-07-11 13:35:58.998102: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 82591744 exceeds 10% of free system memory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Clear memory","metadata":{"id":"HzS72rcIvL0N"}},{"cell_type":"code","source":"del batched_data_list, train_size, val_test_size, data_size\ndel padded_data, chunked_data, tokenized_data, data_list, df, lengths_tokenized\ndel lookup_table, reserved_tokens\ndel bert_tokenizer_params, ends, starts, vocab, tensor_vocab\ndel chunk, chunks","metadata":{"id":"VlkhQigKvOQT","execution":{"iopub.status.busy":"2022-07-11T13:36:24.569782Z","iopub.execute_input":"2022-07-11T13:36:24.570261Z","iopub.status.idle":"2022-07-11T13:36:24.743338Z","shell.execute_reply.started":"2022-07-11T13:36:24.570223Z","shell.execute_reply":"2022-07-11T13:36:24.742071Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# Model\n","metadata":{"id":"5vBLynXu0QI1"}},{"cell_type":"markdown","source":"## Positional encoding","metadata":{"id":"A9qUAZ00xiVd"}},{"cell_type":"markdown","source":"The formula for calculating the positional encoding is as follows:\n\n$${PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n$${PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n\nwhere $d_{model}$ is the model dimension, $pos$ is the position and $i$ is the index of the embedding.\nthis is taken from the paper: attention is all you need.","metadata":{"id":"_V1b_16-kp3v"}},{"cell_type":"code","source":"def create_positional_encoding(max_len: int, d_model: int) -> tf.Tensor:\n    \"\"\"Returns the positional encoding for a given a maximal sequence length and model dimension.\n    inputs: max_len: int, d_model: int\n    returns: tf.Tensor of shape (1, max_len, d_model) and dtype tf.keras.backend.floatx()\n    The 1 is for the batch dimension, the place in the batch dimension does not matter\"\"\"\n\n    def get_angles(positions: np.ndarray, timestamps: np.ndarray, d_model: int) -> np.ndarray:\n        \"\"\"Returns the angle in radians for given positions, timestamps and the dimension of the model\n        input: positions: np.ndarray of shape (max_len, 1), timestamps: np.ndarray of shape (1, d_model), d_model: int\n        output: np.ndarray of shape (max_len, d_model)\"\"\"\n        if tf.keras.backend.floatx() == \"float32\":\n            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float32(d_model)))\n        else:\n            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float16(d_model)))\n\n        return positions * angle_rates\n    \n    angle_rads = get_angles(np.arange(max_len)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)  # (max_len, d_model)\n\n    # apply sin to even indices in the array; 2i for i in range(d_model // 2)\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # (max_len, d_model)\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # (max_len, d_model)\n\n    pos_encode = angle_rads[np.newaxis, ...]  # (1, max_len, d_model)\n\n    return tf.cast(pos_encode, dtype=tf.keras.backend.floatx())","metadata":{"id":"Wx4hze4k0yls","execution":{"iopub.status.busy":"2022-07-11T13:36:35.669624Z","iopub.execute_input":"2022-07-11T13:36:35.670379Z","iopub.status.idle":"2022-07-11T13:36:35.681919Z","shell.execute_reply.started":"2022-07-11T13:36:35.670335Z","shell.execute_reply":"2022-07-11T13:36:35.680763Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"## Masking","metadata":{"id":"6ib5F3hnxrE9"}},{"cell_type":"markdown","source":"Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.","metadata":{"id":"3TXw70UlzcVi"}},{"cell_type":"markdown","source":"The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n\nThis means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on.","metadata":{"id":"4w68l26GzrqG"}},{"cell_type":"code","source":"def create_masks(inp: tf.Tensor, tar: tf.Tensor, pad_ten: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n        \"\"\"Creates all the masks needed for the model\n        input: inp: tf.Tensor of shape (batch_size, seq_len), tar: tf.Tensor of shape (batch_size, set_size)\n        Returns: tuple of (padding_mask, look_ahead_mask)\n        padding_mask, look_ahead_mask: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n        \n        def create_padding_mask(seq: tf.Tensor) -> tf.Tensor:\n                \"\"\"Returns a padding mask for the given sequence.\n                input: seq: tf.Tensor of shape (batch_size, seq_len)\n                Returns: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n                seq = tf.cast(tf.math.equal(seq, pad_ten), tf.keras.backend.floatx())  \n                # For every item in the sequence, 1 if it is a padding token, 0 if it is not \n\n                # add extra dimensions to add the padding\n                \n                return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n        \n        # Encoder padding mask\n        padding_mask: tf.Tensor = create_padding_mask(inp)  # (batch_size, 1, 1, seq_len)\n\n        # Used in the 1st attention block in the decoder.\n        # It is used to pad and mask future tokens in the input received by\n        # the decoder.\n        set_size: int = tar.shape[1]\n\n        def create_look_ahead_mask(set_size: int) -> tf.Tensor:\n                mask = 1 - tf.linalg.band_part(tf.ones((set_size, set_size)), -1, 0)\n                mask = tf.cast(mask, dtype=tf.keras.backend.floatx())\n                return mask  # (seq_len, seq_len)\n\n        look_ahead_mask = create_look_ahead_mask(set_size)  # (seq_len, seq_len)\n        dec_target_padding_mask = create_padding_mask(tar)  # (batch_size, 1, 1, seq_len)\n        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) # (batch_size, 1, 1, seq_len)\n\n        return padding_mask, look_ahead_mask","metadata":{"id":"BQkjgUVVckzJ","execution":{"iopub.status.busy":"2022-07-11T13:36:35.827644Z","iopub.execute_input":"2022-07-11T13:36:35.828136Z","iopub.status.idle":"2022-07-11T13:36:35.839781Z","shell.execute_reply.started":"2022-07-11T13:36:35.828096Z","shell.execute_reply":"2022-07-11T13:36:35.838723Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## Layers and blocks","metadata":{"id":"9KHKaxz3xumc"}},{"cell_type":"code","source":"class ScaledDotProductAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model: int, **kwargs):\n        super().__init__(**kwargs)\n        self.scale: tf.TensorSpec(shape=(), dtype=tf.keras.backend.floatx())\n        # scale = 1 / sqrt(d_model)\n        self.scale = tf.math.pow(tf.cast(d_model, tf.keras.backend.floatx()), -0.5)\n        self.softmax = tf.keras.layers.Softmax(axis=-1)\n\n    def call(self, q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n        \"\"\"Scaled Dot-Product Attention\n        input: \n        q: tf.Tensor of shape (batch_size, seq_len, d_model), \n        k: tf.Tensor of shape (batch_size, seq_len, d_model), \n        v: tf.Tensor of shape (batch_size, seq_len, d_model), \n        mask: Optional[tf.Tensor] of shape (batch_size, 1, 1, seq_len)\n        output: tf.Tensor of shape (batch_size, seq_len, d_model)\"\"\"\n        matmul_qk: tf.Tensor = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n        # Scaled Dot-Product Attention\n        scaled_attention_logits: tf.Tensor = matmul_qk * self.scale  # (..., seq_len_q, seq_len_k)\n        # matmul_qk / sqrt(d_model)\n\n        # Masking\n        if mask is not None:\n            # noinspection PyTypeChecker\n            if tf.keras.backend.floatx() == 'float16':\n                # tf.float16.min is minus infinity\n                scaled_attention_logits += (mask * tf.float16.min)  # changed from -1e9 to prevent nan's\n            else:\n                scaled_attention_logits += (mask * -1e9) \n\n        # Normalize\n        attention_weights = self.softmax(scaled_attention_logits)\n        # (..., seq_len_q, seq_len_k)\n\n        # Output\n        output = tf.matmul(attention_weights, v)\n\n        return output","metadata":{"id":"gRXst-o2NjXL","execution":{"iopub.status.busy":"2022-07-11T13:36:35.918401Z","iopub.execute_input":"2022-07-11T13:36:35.919192Z","iopub.status.idle":"2022-07-11T13:36:35.931750Z","shell.execute_reply.started":"2022-07-11T13:36:35.919135Z","shell.execute_reply":"2022-07-11T13:36:35.930373Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"class MyMultiHeadAttention(tf.keras.layers.Layer):\n    \"\"\"U can use the built-in tf.keras.layers.multihead_attention but is caused a bug for me\"\"\"\n    def __init__(self, num_heads: int, d_model: int, **kwargs):\n        super().__init__(**kwargs)\n        if d_model % num_heads != 0:\n            raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\")\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        self.sdpa = ScaledDotProductAttention(d_model)\n\n    def split_heads(self, x: tf.Tensor, batch_size: int) -> tf.Tensor:\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v_k: tf.Tensor, q: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n        \"\"\"inputs:\n        v_k: tf.Tensor of shape (batch_size, seq_len, d_model) in self attention keys and values are the same\n        q: tf.Tensor of shape (batch_size, seq_len, d_model)\n        mask: Optional[tf.Tensor] of shape (batch_size, seq_len)\"\"\"\n        batch_size = tf.shape(q)[0]\n\n        q: tf.Tensor = self.wq(q)  # (batch_size, seq_len, d_model)\n        k: tf.Tensor = self.wk(v_k)  # (batch_size, seq_len, d_model)\n        v: tf.Tensor = self.wv(v_k)  # (batch_size, seq_len, d_model)\n\n        q: tf.Tensor = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k: tf.Tensor = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v: tf.Tensor = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape should be (batch_size, num_heads, seq_len_q, depth)\n        scaled_attention = self.sdpa(q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n         # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n          # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output","metadata":{"id":"_0I9SZI0kp31","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:36:36.008646Z","iopub.execute_input":"2022-07-11T13:36:36.009547Z","iopub.status.idle":"2022-07-11T13:36:36.024827Z","shell.execute_reply.started":"2022-07-11T13:36:36.009478Z","shell.execute_reply":"2022-07-11T13:36:36.023878Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"class PointWiseFeedForwardNetwork(tf.keras.layers.Layer):\n    def __init__(self, d_model: int, dff: int, **kwargs): \n        super().__init__(**kwargs)\n        self.layer1 = tf.keras.layers.Dense(dff, activation='relu')  # (batch_size, seq_len, dff)\n        self.layer2 = tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    \n    def call(self, x: tf.Tensor) -> tf.Tensor:\n        \"\"\"Gets tensor of shape (batch_size, seq_len, d_model) and dtype tf.keras.beckend.floatx()\n        Returns tensor of shape (batch_size, seq_len, d_model) and dtype tf.keras.beckend.floatx()\"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return x","metadata":{"id":"QOVbpMxtNjXQ","execution":{"iopub.status.busy":"2022-07-11T13:36:36.100349Z","iopub.execute_input":"2022-07-11T13:36:36.101610Z","iopub.status.idle":"2022-07-11T13:36:36.109334Z","shell.execute_reply.started":"2022-07-11T13:36:36.101558Z","shell.execute_reply":"2022-07-11T13:36:36.107925Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(tf.keras.layers.Layer):\n    def __init__(self, d_model: int, num_heads: int, dff: int, drop_out_rate: float, **kwargs):\n        super().__init__(**kwargs)\n\n        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout = tf.keras.layers.Dropout(drop_out_rate)\n\n    def call(self, x: tf.Tensor, training: bool, mask: tf.Tensor) -> tf.Tensor:\n        \n        attn_output = self.mha(x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout(attn_output, training=training)  # (batch_size, input_seq_len, d_model)\n        # out1 = self.layer_norm(x + attn_output)  # (batch_size, input_seq_len, d_model)\n        # might be data leak\n        out1 = self.layer_norm(attn_output)  # (batch_size, input_seq_len, d_model)\n        \n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout(ffn_output, training=training)  # (batch_size, input_seq_len, d_model)\n        out2 = self.layer_norm(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2","metadata":{"id":"8mEP3-Jx2n39","execution":{"iopub.status.busy":"2022-07-11T13:36:36.194955Z","iopub.execute_input":"2022-07-11T13:36:36.196527Z","iopub.status.idle":"2022-07-11T13:36:36.207816Z","shell.execute_reply.started":"2022-07-11T13:36:36.196478Z","shell.execute_reply":"2022-07-11T13:36:36.206745Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(tf.keras.layers.Layer):\n    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float, **kwargs):\n        super().__init__(**kwargs)\n\n        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n\n        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x: tf.Tensor, enc_output: tf.Tensor, look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor, training):\n        # enc_output.shape should be (batch_size, input_seq_len, d_model)\n\n        attn1 = self.mha(x, x, look_ahead_mask)  # (batch_size, set_size, d_model)\n        attn1 = self.dropout(attn1, training=training)  # (batch_size, set_size, d_model)\n        # out1 = self.layer_norm(attn1 + x)\n        # might be data leak\n        out1 = self.layer_norm(attn1)  # (batch_size, set_size, d_model)\n\n        attn2 = self.mha(enc_output, out1, padding_mask)  # (batch_size, set_size, d_model)\n        attn2 = self.dropout(attn2, training=training)  # (batch_size, set_size, d_model)\n        out2 = self.layer_norm(attn2 + out1)  # (batch_size, set_size, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, set_size, d_model)\n        ffn_output = self.dropout(ffn_output, training=training)\n        out3 = self.layer_norm(ffn_output + out2)  # (batch_size, set_size, d_model)\n\n        return out3 ","metadata":{"id":"xJWFrv0g281G","execution":{"iopub.status.busy":"2022-07-11T13:36:36.277810Z","iopub.execute_input":"2022-07-11T13:36:36.278267Z","iopub.status.idle":"2022-07-11T13:36:36.288889Z","shell.execute_reply.started":"2022-07-11T13:36:36.278226Z","shell.execute_reply":"2022-07-11T13:36:36.287571Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, pos_encoding: tf.Tensor, num_blocks: int, d_model: int, num_heads: int, dff: int, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n\n        self.d_model = d_model\n        self.num_blocks = num_blocks\n        self.pos_encoding = pos_encoding\n\n        self.enc_blocks = [EncoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n        # the encoder \n        self.dropout = tf.keras.layers.Dropout(rate)\n        self.scale = tf.math.sqrt(tf.cast(self.d_model, tf.keras.backend.floatx()))\n\n    def call(self, x: tf.Tensor, training, mask: tf.Tensor) -> tf.Tensor:\n\n        seq_len = tf.shape(x)[1]\n\n        # adding position encoding.\n        # assert not tf.math.is_nan(x[0][0][0])\n        x *= self.scale\n        # assert not tf.math.is_nan(x[0][0][0])\n        \n        x += self.pos_encoding[:, :seq_len, :]  # (batch_size, input_seq_len, d_model)\n        # assert not tf.math.is_nan(x[0][0][0])\n        x = self.dropout(x, training=training)  # (batch_size, input_seq_len, d_model)\n        # assert not tf.math.is_nan(x[0][0][0])\n\n        for i in range(self.num_blocks):\n            x = self.enc_blocks[i](x, training, mask)  # (batch_size, input_seq_len, d_model)\n            # assert not tf.math.is_nan(x[0][0][0])\n\n        return x  # (batch_size, input_seq_len, d_model)  ","metadata":{"id":"NdlS1kfM3k5d","execution":{"iopub.status.busy":"2022-07-11T13:36:36.504029Z","iopub.execute_input":"2022-07-11T13:36:36.505352Z","iopub.status.idle":"2022-07-11T13:36:36.516912Z","shell.execute_reply.started":"2022-07-11T13:36:36.505297Z","shell.execute_reply":"2022-07-11T13:36:36.515702Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, pos_encoding, num_blocks: int, d_model: int, num_heads: int, dff: int,\n                 vocab_size: int, rate: float, **kwargs):\n        super().__init__(**kwargs)\n\n        self.scale = tf.math.sqrt(tf.cast(d_model, tf.keras.backend.floatx()))\n        self.num_blocks = num_blocks\n        self.pos_encoding = pos_encoding\n\n        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n        self.dec_layers = [DecoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, tar: tf.Tensor, enc_output: tf.Tensor, training: bool,\n             look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor) -> tf.Tensor:\n\n        seq_len = tf.shape(tar)[1]\n\n        x = self.embedding(tar)  # (batch_size, set_size, d_model)\n        x *= self.scale\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_blocks):\n            x = self.dec_layers[i](x=x, enc_output=enc_output, look_ahead_mask=look_ahead_mask, \n                                   padding_mask=padding_mask, training=training)\n\n        # x.shape should be (batch_size, set_size, d_model)\n        return x","metadata":{"id":"QLE8js6O3p5-","execution":{"iopub.status.busy":"2022-07-11T13:36:36.601570Z","iopub.execute_input":"2022-07-11T13:36:36.602739Z","iopub.status.idle":"2022-07-11T13:36:36.613830Z","shell.execute_reply.started":"2022-07-11T13:36:36.602688Z","shell.execute_reply":"2022-07-11T13:36:36.612519Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"class EmbeddingTransposed(tf.keras.layers.Layer):\n    def __init__(self, tied_to: tf.keras.layers.Embedding = None, activation: Optional[str] = None, **kwargs):\n        super(EmbeddingTransposed, self).__init__(**kwargs)\n        self.tied_to = tied_to\n        self.activation = tf.keras.activations.get(activation)\n\n    def build(self, input_shape):\n        self.custom_weights = self.tied_to.weights[0]\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], tf.keras.backend.int_shape(self.tied_to.weights[0])[0]\n\n    def call(self, inputs, mask=None):\n        output = tf.keras.backend.dot(inputs, tf.keras.backend.transpose(self.custom_weights))\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def get_config(self):\n        config = {\"activation\": tf.keras.activations.serialize(self.activation)}\n        base_config = super(EmbeddingTransposed, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"id":"6uaYfyRdEtBN","execution":{"iopub.status.busy":"2022-07-11T13:36:36.702640Z","iopub.execute_input":"2022-07-11T13:36:36.703114Z","iopub.status.idle":"2022-07-11T13:36:36.714748Z","shell.execute_reply.started":"2022-07-11T13:36:36.703075Z","shell.execute_reply":"2022-07-11T13:36:36.713320Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## The full model","metadata":{"id":"1-xe3K7LyD2l"}},{"cell_type":"code","source":"class SeTransformer(tf.keras.Model):\n    \"\"\"The base architecture of my models in this project.\"\"\"\n    def __init__(self, num_blocks: int, d_model: int, num_heads: int, dff: int,\n                 vocab_size: int, max_len: int, rate: float, pad_int: int, **kwargs):\n        super().__init__(**kwargs)  # calls tf.keras.Model's __init__ method\n        self.pad_int = pad_int\n        self.vocab_size = vocab_size\n        pos_encoding = create_positional_encoding(max_len, d_model)\n        self.d_model = d_model\n        self.encoder = Encoder(pos_encoding, num_blocks, d_model, num_heads, dff, rate)\n        self.decoder = Decoder(pos_encoding, num_blocks, d_model, num_heads, dff, vocab_size, rate)\n        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n        self.emb_trans = EmbeddingTransposed(self.embedding, \"softmax\")\n\n\n    def count_params(self) -> int:\n        \"\"\"counts trainable parameters\n        Raises an error if caleed before building the model\"\"\"\n        param_count: int = self.encoder.count_params() + self.decoder.count_params() + self.embedding.count_params()\n        return param_count\n\n\n    def call(self, inputs: list, training: bool) -> tf.Tensor:\n        inp, tar = inputs\n        # inp.shape should be (batch_size, max_seq_len)\n        # tar.shape should be (batch_size, set_size)\n        x = self.embedding(inp)  # (batch_size, max_seq_len, d_model)\n        padding_mask, look_ahead_mask = create_masks(inp, tar, self.pad_int)\n        enc_output = self.encoder(x, training, padding_mask)  # (batch_size, max_seq_len, d_model)\n        dec_output = self.decoder(tar, enc_output, training, look_ahead_mask, padding_mask)  # (batch_size, set_size, d_model)\n        final_output = self.emb_trans(dec_output)  # (batch_size, set_size, vocab_size)\n        return final_output","metadata":{"id":"4NLhyE0T3tUs","execution":{"iopub.status.busy":"2022-07-11T13:36:36.793934Z","iopub.execute_input":"2022-07-11T13:36:36.794816Z","iopub.status.idle":"2022-07-11T13:36:36.808159Z","shell.execute_reply.started":"2022-07-11T13:36:36.794766Z","shell.execute_reply":"2022-07-11T13:36:36.807112Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"sample_transformer = SeTransformer(\n    num_blocks=2, d_model=32, num_heads=4, dff=128,\n    vocab_size=7000,\n    max_len=1200, pad_int=0, rate=0.1)\n\ntemp_input = tf.random.uniform((1, 200), dtype=tf.int32, minval=5, maxval=6999)\ntemp_target = tf.random.uniform((1, 8), dtype=tf.int32, minval=5, maxval=6999)\ntemp_target2 = temp_target + 3\n\ntrain_out = sample_transformer([temp_input, temp_target], training=True)\ntrain_out2 = sample_transformer([temp_input, temp_target2], training=True)\n\ntry:\n    tf.debugging.assert_equal(train_out, train_out2)\nexcept tf.errors.InvalidArgumentError:\n    print(\"WARNING: model output might depends on the target\")\n\nnon_train_out = sample_transformer([temp_input, temp_target], training=False)\n\ndel sample_transformer, train_out, non_train_out, temp_target2, train_out2","metadata":{"id":"3WZ41cb-1B_f","outputId":"36a9e5ad-0737-4a6c-ef71-b85b208ca4e7","execution":{"iopub.status.busy":"2022-07-11T13:36:36.878500Z","iopub.execute_input":"2022-07-11T13:36:36.878960Z","iopub.status.idle":"2022-07-11T13:36:37.598212Z","shell.execute_reply.started":"2022-07-11T13:36:36.878919Z","shell.execute_reply":"2022-07-11T13:36:37.597320Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"WARNING: model output depends on the target\nThe attribuites of the model are: ['_SCALAR_UPRANKING_ON', '_TF_MODULE_IGNORED_PROPERTIES', '__call__', '__class__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_activity_regularizer', '_add_trackable', '_add_trackable_child', '_add_variable_with_custom_getter', '_assert_compile_was_called', '_assert_weights_created', '_auto_track_sub_layers', '_autocast', '_autographed_call', '_base_model_initialized', '_build_input_shape', '_call_accepts_kwargs', '_call_arg_was_passed', '_call_fn_arg_defaults', '_call_fn_arg_positions', '_call_fn_args', '_call_full_argspec', '_callable_losses', '_captured_weight_regularizer', '_cast_single_input', '_check_call_args', '_check_sample_weight_warning', '_checkpoint', '_checkpoint_dependencies', '_clear_losses', '_cluster_coordinator', '_compile_was_called', '_compiled_trainable_state', '_compute_dtype', '_compute_dtype_object', '_compute_output_and_mask_jointly', '_configure_steps_per_execution', '_dedup_weights', '_default_training_arg', '_deferred_dependencies', '_delete_tracking', '_deserialization_dependencies', '_deserialize_from_proto', '_distribution_strategy', '_dtype', '_dtype_policy', '_dynamic', '_eager_losses', '_expects_mask_arg', '_expects_training_arg', '_export_to_saved_model_graph', '_flatten', '_flatten_layers', '_flatten_modules', '_functional_construction_call', '_gather_children_attribute', '_gather_saveables_for_checkpoint', '_get_call_arg_value', '_get_callback_model', '_get_cell_name', '_get_compile_args', '_get_existing_metric', '_get_input_masks', '_get_legacy_saved_model_children', '_get_node_attribute_at_index', '_get_optimizer', '_get_save_spec', '_get_trainable_state', '_get_unnested_name_scope', '_handle_activity_regularization', '_handle_deferred_dependencies', '_handle_weight_regularization', '_in_multi_worker_mode', '_inbound_nodes', '_inbound_nodes_value', '_infer_output_signature', '_init_batch_counters', '_init_call_fn_args', '_init_set_name', '_initial_weights', '_input_spec', '_instrument_layer_creation', '_instrumented_keras_api', '_instrumented_keras_layer_class', '_instrumented_keras_model_class', '_is_compiled', '_is_graph_network', '_is_layer', '_is_model_for_instrumentation', '_jit_compile', '_keras_api_names', '_keras_api_names_v1', '_keras_tensor_symbolic_call', '_layout_map', '_list_extra_dependencies_for_serialization', '_list_functions_for_serialization', '_lookup_dependency', '_losses', '_map_resources', '_maybe_build', '_maybe_cast_inputs', '_maybe_create_attribute', '_maybe_initialize_trackable', '_maybe_load_initial_epoch_from_ckpt', '_maybe_load_initial_step_from_ckpt', '_metrics', '_metrics_lock', '_must_restore_from_config', '_name', '_name_based_attribute_restore', '_name_based_restores', '_name_scope', '_name_scope_on_declaration', '_no_dependency', '_non_trainable_weights', '_obj_reference_counts', '_obj_reference_counts_dict', '_object_identifier', '_outbound_nodes', '_outbound_nodes_value', '_predict_counter', '_preload_simple_restoration', '_preserve_input_structure_in_config', '_reset_compile_cache', '_restore_from_checkpoint_position', '_restore_from_tensors', '_run_eagerly', '_saved_model_arg_spec', '_saved_model_inputs_spec', '_self_name_based_restores', '_self_saveable_object_factories', '_self_setattr_tracking', '_self_tracked_trackables', '_self_unconditional_checkpoint_dependencies', '_self_unconditional_deferred_dependencies', '_self_unconditional_dependency_names', '_self_update_uid', '_serialize_to_proto', '_serialize_to_tensors', '_set_call_arg_value', '_set_connectivity_metadata', '_set_dtype_policy', '_set_inputs', '_set_mask_keras_history_checked', '_set_mask_metadata', '_set_save_spec', '_set_trainable_state', '_set_training_mode', '_setattr_tracking', '_should_cast_single_input', '_should_compute_mask', '_should_eval', '_single_restoration_from_checkpoint_position', '_split_out_first_arg', '_stateful', '_steps_per_execution', '_supports_masking', '_test_counter', '_tf_api_names', '_tf_api_names_v1', '_thread_local', '_track_trackable', '_trackable_children', '_trackable_saved_model_saver', '_tracking_metadata', '_train_counter', '_trainable', '_trainable_weights', '_training_state', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_undeduplicated_weights', '_update_uid', '_updated_config', '_updates', '_use_input_spec_as_call_signature', '_validate_compile', '_validate_target_and_loss', 'activity_regularizer', 'add_loss', 'add_metric', 'add_update', 'add_variable', 'add_weight', 'build', 'built', 'call', 'compile', 'compiled_loss', 'compiled_metrics', 'compute_dtype', 'compute_loss', 'compute_mask', 'compute_metrics', 'compute_output_shape', 'compute_output_signature', 'count_params', 'd_model', 'decoder', 'distribute_strategy', 'dtype', 'dtype_policy', 'dynamic', 'emb_trans', 'embedding', 'encoder', 'evaluate', 'evaluate_generator', 'finalize_state', 'fit', 'fit_generator', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_layer', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_weights', 'history', 'inbound_nodes', 'input', 'input_mask', 'input_names', 'input_shape', 'input_spec', 'inputs', 'layers', 'load_weights', 'losses', 'make_predict_function', 'make_test_function', 'make_train_function', 'metrics', 'metrics_names', 'name', 'name_scope', 'non_trainable_variables', 'non_trainable_weights', 'optimizer', 'outbound_nodes', 'output', 'output_mask', 'output_names', 'output_shape', 'outputs', 'pad_int', 'predict', 'predict_function', 'predict_generator', 'predict_on_batch', 'predict_step', 'reset_metrics', 'reset_states', 'run_eagerly', 'save', 'save_spec', 'save_weights', 'set_weights', 'state_updates', 'stateful', 'stop_training', 'submodules', 'summary', 'supports_masking', 'test_function', 'test_on_batch', 'test_step', 'to_json', 'to_yaml', 'train_function', 'train_on_batch', 'train_step', 'train_tf_function', 'trainable', 'trainable_variables', 'trainable_weights', 'updates', 'variable_dtype', 'variables', 'vocab_size', 'weights', 'with_name_scope']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training the model","metadata":{"id":"hFxiRfCekp38"}},{"cell_type":"code","source":"# model_art = wandb.use_artifact(f\"{model_collection_name}:latest\")\n# model_path = model_art.get_path(\"model.pb\").download()\n# model = tf.saved_model.load(model_path)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:36:37.600358Z","iopub.execute_input":"2022-07-11T13:36:37.601409Z","iopub.status.idle":"2022-07-11T13:36:37.606022Z","shell.execute_reply.started":"2022-07-11T13:36:37.601370Z","shell.execute_reply":"2022-07-11T13:36:37.604964Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"## Hyper-Parameters","metadata":{"id":"B5rtigmHckyv"}},{"cell_type":"code","source":"set_size: int = 2\nlearning_rate: float = 0.005\n\nnum_sets: int = (max_seq_len // set_size) - 1 # Because we dont predict the first set\n# number of sets in each sequence\n\nnum_blocks: int = 16\nd_model: int = 512\ndff: int = 1024\nnum_heads: int = 32\ndropout_rate: float = 0.1","metadata":{"id":"bWmh89nVckyw","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:36:37.608003Z","iopub.execute_input":"2022-07-11T13:36:37.608491Z","iopub.status.idle":"2022-07-11T13:36:37.619404Z","shell.execute_reply.started":"2022-07-11T13:36:37.608445Z","shell.execute_reply":"2022-07-11T13:36:37.618455Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"## Weights and Biases","metadata":{}},{"cell_type":"code","source":"if device == 'kaggle':\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_API_KEY\")\n    except Exception:\n        print(\"please enter your weights and biases API key\")\n!wandb login","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:37:55.399043Z","iopub.execute_input":"2022-07-11T13:37:55.399556Z","iopub.status.idle":"2022-07-11T13:37:58.407253Z","shell.execute_reply.started":"2022-07-11T13:37:55.399519Z","shell.execute_reply":"2022-07-11T13:37:58.405797Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myonikremer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":"# try: \n#     artifect = use_artifact(artifact, use_as=None)\n#     art = wandb.use_artifact(...)\n#     wandb.run.link_artifact(art, 'yonikremer/final_project_owned/version0')","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:36:37.656864Z","iopub.status.idle":"2022-07-11T13:36:37.657702Z","shell.execute_reply.started":"2022-07-11T13:36:37.657439Z","shell.execute_reply":"2022-07-11T13:36:37.657467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(\n    project=\"final_project_owned\",\n    entity=\"yonikremer\",\n    name=datetime.datetime.today().strftime(f\"run from %d/%m/%Y\"),\n    settings=wandb.Settings(start_method=\"thread\"),\n    config = {\"set size\": set_size,\n              \"batch size\": batch_size,\n              \"learning rate\": learning_rate,\n              \"max seq len\": max_seq_len,\n              \"num blocks\": num_blocks,\n              \"model dimention\": d_model,\n              \"dff\": dff,\n              \"num heads\": num_heads,\n              \"dropout rate\": dropout_rate\n              })\nconfig = wandb.config","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:38:38.664243Z","iopub.execute_input":"2022-07-11T13:38:38.665135Z","iopub.status.idle":"2022-07-11T13:38:41.522902Z","shell.execute_reply.started":"2022-07-11T13:38:38.665092Z","shell.execute_reply":"2022-07-11T13:38:41.521868Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myonikremer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.12.21 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.18"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220711_133839-2pb64ete</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/yonikremer/final_project_owned/runs/2pb64ete\" target=\"_blank\">run from 11/07/2022</a></strong> to <a href=\"https://wandb.ai/yonikremer/final_project_owned\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create the model","metadata":{"id":"O783T1xMckzR"}},{"cell_type":"code","source":"# with strategy.scope():\nmodel = SeTransformer(\n    num_blocks=num_blocks,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    vocab_size=vocab_size,\n    max_len=max_seq_len,\n    rate=dropout_rate,\n    pad_int=pad_int)\n\nloss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\noptimizer = tf.keras.optimizers.Adam(learning_rate, epsilon=tf.keras.backend.epsilon())\ntemp_input = tf.random.uniform((batch_size, max_seq_len), dtype=tf.int32, minval=5, maxval=6999)\ntemp_target = tf.random.uniform((batch_size, set_size), dtype=tf.int32, minval=5, maxval=6999)\nmodel.compile(optimizer=optimizer, loss=loss_func)\n\ntemp_input = tf.random.uniform((batch_size, max_seq_len), dtype=tf.int32, minval=5, maxval=6999)\ntemp_target = tf.random.uniform((batch_size, set_size), dtype=tf.int32, minval=5, maxval=6999)\n\nmodel([temp_input, temp_target], False)\n\nparam_count: int = model.count_params()\nprint(f\"The model has {param_count:,} = {param_count * (10**-6):,}M trainable parameters\")\nrun.config[\"parameters\"] = param_count\n# stats = FlopCoKeras(model)\n# flops_per_call: int = stats.total_flops\n# macs_per_call: int = stats.total_macs\n\n# (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * (3 for forward and backward pass) * (number of examples in dataset) \n# training_flops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n# print(f\"FLOPs per call: {flops_per_call:,} = {(flops_per_call * (10 ** -6)):,}M\")\n# print(f\"MACs per call: {macs_per_call:,} = {(macs_per_call * (10 ** -6)):,}M\")\n\ntf.keras.utils.plot_model(\n    model,\n    show_shapes=True,\n    show_dtype=True,\n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=True,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=True\n)\n\ndel temp_input, temp_target","metadata":{"id":"-uAxXGBtkp4A","pycharm":{"name":"#%%\n"},"outputId":"898c89bb-118b-4c84-e398-4171a2904b42","execution":{"iopub.status.busy":"2022-07-11T13:43:08.046250Z","iopub.execute_input":"2022-07-11T13:43:08.046739Z","iopub.status.idle":"2022-07-11T13:43:31.019037Z","shell.execute_reply.started":"2022-07-11T13:43:08.046701Z","shell.execute_reply":"2022-07-11T13:43:31.017398Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stderr","text":"  0%|          | 0/128 [06:29<?, ?it/s]\n2022-07-11 13:43:08.557223: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 33554432 exceeds 10% of free system memory.\n2022-07-11 13:43:08.568995: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 33554432 exceeds 10% of free system memory.\n2022-07-11 13:43:08.590114: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 33554432 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"The model has 12,461,056 = 12.461056M trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(model.trainable_weights))\nprint(len(model.trainable_weights))\nprint(type(model.trainable_weights[0]))\nprint(model.trainable_weights[0])","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:58:02.140334Z","iopub.execute_input":"2022-07-11T13:58:02.141749Z","iopub.status.idle":"2022-07-11T13:58:02.622826Z","shell.execute_reply.started":"2022-07-11T13:58:02.141700Z","shell.execute_reply":"2022-07-11T13:58:02.621376Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"<class 'list'>\n226\n<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\nse_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_312/kernel:0\nse_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_312/bias:0\nse_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_313/kernel:0\nse_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_313/bias:0\nse_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_314/kernel:0\nse_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_314/bias:0\nse_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_315/kernel:0\nse_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_315/bias:0\nse_transformer_4/encoder_4/encoder_block_26/point_wise_feed_forward_network_52/dense_316/kernel:0\nse_transformer_4/encoder_4/encoder_block_26/point_wise_feed_forward_network_52/dense_316/bias:0\nse_transformer_4/encoder_4/encoder_block_26/point_wise_feed_forward_network_52/dense_317/kernel:0\nse_transformer_4/encoder_4/encoder_block_26/point_wise_feed_forward_network_52/dense_317/bias:0\nse_transformer_4/encoder_4/encoder_block_26/layer_normalization_52/gamma:0\nse_transformer_4/encoder_4/encoder_block_26/layer_normalization_52/beta:0\nse_transformer_4/encoder_4/encoder_block_27/my_multi_head_attention_53/dense_318/kernel:0\nse_transformer_4/encoder_4/encoder_block_27/my_multi_head_attention_53/dense_318/bias:0\nse_transformer_4/encoder_4/encoder_block_27/my_multi_head_attention_53/dense_319/kernel:0\nse_transformer_4/encoder_4/encoder_block_27/my_multi_head_attention_53/dense_319/bias:0\nse_transformer_4/encoder_4/encoder_block_27/my_multi_head_attention_53/dense_320/kernel:0\nse_transformer_4/encoder_4/encoder_block_27/my_multi_head_attention_53/dense_320/bias:0\nse_transformer_4/encoder_4/encoder_block_27/my_multi_head_attention_53/dense_321/kernel:0\nse_transformer_4/encoder_4/encoder_block_27/my_multi_head_attention_53/dense_321/bias:0\nse_transformer_4/encoder_4/encoder_block_27/point_wise_feed_forward_network_53/dense_322/kernel:0\nse_transformer_4/encoder_4/encoder_block_27/point_wise_feed_forward_network_53/dense_322/bias:0\nse_transformer_4/encoder_4/encoder_block_27/point_wise_feed_forward_network_53/dense_323/kernel:0\nse_transformer_4/encoder_4/encoder_block_27/point_wise_feed_forward_network_53/dense_323/bias:0\nse_transformer_4/encoder_4/encoder_block_27/layer_normalization_53/gamma:0\nse_transformer_4/encoder_4/encoder_block_27/layer_normalization_53/beta:0\nse_transformer_4/encoder_4/encoder_block_28/my_multi_head_attention_54/dense_324/kernel:0\nse_transformer_4/encoder_4/encoder_block_28/my_multi_head_attention_54/dense_324/bias:0\nse_transformer_4/encoder_4/encoder_block_28/my_multi_head_attention_54/dense_325/kernel:0\nse_transformer_4/encoder_4/encoder_block_28/my_multi_head_attention_54/dense_325/bias:0\nse_transformer_4/encoder_4/encoder_block_28/my_multi_head_attention_54/dense_326/kernel:0\nse_transformer_4/encoder_4/encoder_block_28/my_multi_head_attention_54/dense_326/bias:0\nse_transformer_4/encoder_4/encoder_block_28/my_multi_head_attention_54/dense_327/kernel:0\nse_transformer_4/encoder_4/encoder_block_28/my_multi_head_attention_54/dense_327/bias:0\nse_transformer_4/encoder_4/encoder_block_28/point_wise_feed_forward_network_54/dense_328/kernel:0\nse_transformer_4/encoder_4/encoder_block_28/point_wise_feed_forward_network_54/dense_328/bias:0\nse_transformer_4/encoder_4/encoder_block_28/point_wise_feed_forward_network_54/dense_329/kernel:0\nse_transformer_4/encoder_4/encoder_block_28/point_wise_feed_forward_network_54/dense_329/bias:0\nse_transformer_4/encoder_4/encoder_block_28/layer_normalization_54/gamma:0\nse_transformer_4/encoder_4/encoder_block_28/layer_normalization_54/beta:0\nse_transformer_4/encoder_4/encoder_block_29/my_multi_head_attention_55/dense_330/kernel:0\nse_transformer_4/encoder_4/encoder_block_29/my_multi_head_attention_55/dense_330/bias:0\nse_transformer_4/encoder_4/encoder_block_29/my_multi_head_attention_55/dense_331/kernel:0\nse_transformer_4/encoder_4/encoder_block_29/my_multi_head_attention_55/dense_331/bias:0\nse_transformer_4/encoder_4/encoder_block_29/my_multi_head_attention_55/dense_332/kernel:0\nse_transformer_4/encoder_4/encoder_block_29/my_multi_head_attention_55/dense_332/bias:0\nse_transformer_4/encoder_4/encoder_block_29/my_multi_head_attention_55/dense_333/kernel:0\nse_transformer_4/encoder_4/encoder_block_29/my_multi_head_attention_55/dense_333/bias:0\nse_transformer_4/encoder_4/encoder_block_29/point_wise_feed_forward_network_55/dense_334/kernel:0\nse_transformer_4/encoder_4/encoder_block_29/point_wise_feed_forward_network_55/dense_334/bias:0\nse_transformer_4/encoder_4/encoder_block_29/point_wise_feed_forward_network_55/dense_335/kernel:0\nse_transformer_4/encoder_4/encoder_block_29/point_wise_feed_forward_network_55/dense_335/bias:0\nse_transformer_4/encoder_4/encoder_block_29/layer_normalization_55/gamma:0\nse_transformer_4/encoder_4/encoder_block_29/layer_normalization_55/beta:0\nse_transformer_4/encoder_4/encoder_block_30/my_multi_head_attention_56/dense_336/kernel:0\nse_transformer_4/encoder_4/encoder_block_30/my_multi_head_attention_56/dense_336/bias:0\nse_transformer_4/encoder_4/encoder_block_30/my_multi_head_attention_56/dense_337/kernel:0\nse_transformer_4/encoder_4/encoder_block_30/my_multi_head_attention_56/dense_337/bias:0\nse_transformer_4/encoder_4/encoder_block_30/my_multi_head_attention_56/dense_338/kernel:0\nse_transformer_4/encoder_4/encoder_block_30/my_multi_head_attention_56/dense_338/bias:0\nse_transformer_4/encoder_4/encoder_block_30/my_multi_head_attention_56/dense_339/kernel:0\nse_transformer_4/encoder_4/encoder_block_30/my_multi_head_attention_56/dense_339/bias:0\nse_transformer_4/encoder_4/encoder_block_30/point_wise_feed_forward_network_56/dense_340/kernel:0\nse_transformer_4/encoder_4/encoder_block_30/point_wise_feed_forward_network_56/dense_340/bias:0\nse_transformer_4/encoder_4/encoder_block_30/point_wise_feed_forward_network_56/dense_341/kernel:0\nse_transformer_4/encoder_4/encoder_block_30/point_wise_feed_forward_network_56/dense_341/bias:0\nse_transformer_4/encoder_4/encoder_block_30/layer_normalization_56/gamma:0\nse_transformer_4/encoder_4/encoder_block_30/layer_normalization_56/beta:0\nse_transformer_4/encoder_4/encoder_block_31/my_multi_head_attention_57/dense_342/kernel:0\nse_transformer_4/encoder_4/encoder_block_31/my_multi_head_attention_57/dense_342/bias:0\nse_transformer_4/encoder_4/encoder_block_31/my_multi_head_attention_57/dense_343/kernel:0\nse_transformer_4/encoder_4/encoder_block_31/my_multi_head_attention_57/dense_343/bias:0\nse_transformer_4/encoder_4/encoder_block_31/my_multi_head_attention_57/dense_344/kernel:0\nse_transformer_4/encoder_4/encoder_block_31/my_multi_head_attention_57/dense_344/bias:0\nse_transformer_4/encoder_4/encoder_block_31/my_multi_head_attention_57/dense_345/kernel:0\nse_transformer_4/encoder_4/encoder_block_31/my_multi_head_attention_57/dense_345/bias:0\nse_transformer_4/encoder_4/encoder_block_31/point_wise_feed_forward_network_57/dense_346/kernel:0\nse_transformer_4/encoder_4/encoder_block_31/point_wise_feed_forward_network_57/dense_346/bias:0\nse_transformer_4/encoder_4/encoder_block_31/point_wise_feed_forward_network_57/dense_347/kernel:0\nse_transformer_4/encoder_4/encoder_block_31/point_wise_feed_forward_network_57/dense_347/bias:0\nse_transformer_4/encoder_4/encoder_block_31/layer_normalization_57/gamma:0\nse_transformer_4/encoder_4/encoder_block_31/layer_normalization_57/beta:0\nse_transformer_4/encoder_4/encoder_block_32/my_multi_head_attention_58/dense_348/kernel:0\nse_transformer_4/encoder_4/encoder_block_32/my_multi_head_attention_58/dense_348/bias:0\nse_transformer_4/encoder_4/encoder_block_32/my_multi_head_attention_58/dense_349/kernel:0\nse_transformer_4/encoder_4/encoder_block_32/my_multi_head_attention_58/dense_349/bias:0\nse_transformer_4/encoder_4/encoder_block_32/my_multi_head_attention_58/dense_350/kernel:0\nse_transformer_4/encoder_4/encoder_block_32/my_multi_head_attention_58/dense_350/bias:0\nse_transformer_4/encoder_4/encoder_block_32/my_multi_head_attention_58/dense_351/kernel:0\nse_transformer_4/encoder_4/encoder_block_32/my_multi_head_attention_58/dense_351/bias:0\nse_transformer_4/encoder_4/encoder_block_32/point_wise_feed_forward_network_58/dense_352/kernel:0\nse_transformer_4/encoder_4/encoder_block_32/point_wise_feed_forward_network_58/dense_352/bias:0\nse_transformer_4/encoder_4/encoder_block_32/point_wise_feed_forward_network_58/dense_353/kernel:0\nse_transformer_4/encoder_4/encoder_block_32/point_wise_feed_forward_network_58/dense_353/bias:0\nse_transformer_4/encoder_4/encoder_block_32/layer_normalization_58/gamma:0\nse_transformer_4/encoder_4/encoder_block_32/layer_normalization_58/beta:0\nse_transformer_4/encoder_4/encoder_block_33/my_multi_head_attention_59/dense_354/kernel:0\nse_transformer_4/encoder_4/encoder_block_33/my_multi_head_attention_59/dense_354/bias:0\nse_transformer_4/encoder_4/encoder_block_33/my_multi_head_attention_59/dense_355/kernel:0\nse_transformer_4/encoder_4/encoder_block_33/my_multi_head_attention_59/dense_355/bias:0\nse_transformer_4/encoder_4/encoder_block_33/my_multi_head_attention_59/dense_356/kernel:0\nse_transformer_4/encoder_4/encoder_block_33/my_multi_head_attention_59/dense_356/bias:0\nse_transformer_4/encoder_4/encoder_block_33/my_multi_head_attention_59/dense_357/kernel:0\nse_transformer_4/encoder_4/encoder_block_33/my_multi_head_attention_59/dense_357/bias:0\nse_transformer_4/encoder_4/encoder_block_33/point_wise_feed_forward_network_59/dense_358/kernel:0\nse_transformer_4/encoder_4/encoder_block_33/point_wise_feed_forward_network_59/dense_358/bias:0\nse_transformer_4/encoder_4/encoder_block_33/point_wise_feed_forward_network_59/dense_359/kernel:0\nse_transformer_4/encoder_4/encoder_block_33/point_wise_feed_forward_network_59/dense_359/bias:0\nse_transformer_4/encoder_4/encoder_block_33/layer_normalization_59/gamma:0\nse_transformer_4/encoder_4/encoder_block_33/layer_normalization_59/beta:0\nse_transformer_4/decoder_4/embedding_8/embeddings:0\nse_transformer_4/decoder_4/decoder_block_26/my_multi_head_attention_60/dense_360/kernel:0\nse_transformer_4/decoder_4/decoder_block_26/my_multi_head_attention_60/dense_360/bias:0\nse_transformer_4/decoder_4/decoder_block_26/my_multi_head_attention_60/dense_361/kernel:0\nse_transformer_4/decoder_4/decoder_block_26/my_multi_head_attention_60/dense_361/bias:0\nse_transformer_4/decoder_4/decoder_block_26/my_multi_head_attention_60/dense_362/kernel:0\nse_transformer_4/decoder_4/decoder_block_26/my_multi_head_attention_60/dense_362/bias:0\nse_transformer_4/decoder_4/decoder_block_26/my_multi_head_attention_60/dense_363/kernel:0\nse_transformer_4/decoder_4/decoder_block_26/my_multi_head_attention_60/dense_363/bias:0\nse_transformer_4/decoder_4/decoder_block_26/point_wise_feed_forward_network_60/dense_364/kernel:0\nse_transformer_4/decoder_4/decoder_block_26/point_wise_feed_forward_network_60/dense_364/bias:0\nse_transformer_4/decoder_4/decoder_block_26/point_wise_feed_forward_network_60/dense_365/kernel:0\nse_transformer_4/decoder_4/decoder_block_26/point_wise_feed_forward_network_60/dense_365/bias:0\nse_transformer_4/decoder_4/decoder_block_26/layer_normalization_60/gamma:0\nse_transformer_4/decoder_4/decoder_block_26/layer_normalization_60/beta:0\nse_transformer_4/decoder_4/decoder_block_27/my_multi_head_attention_61/dense_366/kernel:0\nse_transformer_4/decoder_4/decoder_block_27/my_multi_head_attention_61/dense_366/bias:0\nse_transformer_4/decoder_4/decoder_block_27/my_multi_head_attention_61/dense_367/kernel:0\nse_transformer_4/decoder_4/decoder_block_27/my_multi_head_attention_61/dense_367/bias:0\nse_transformer_4/decoder_4/decoder_block_27/my_multi_head_attention_61/dense_368/kernel:0\nse_transformer_4/decoder_4/decoder_block_27/my_multi_head_attention_61/dense_368/bias:0\nse_transformer_4/decoder_4/decoder_block_27/my_multi_head_attention_61/dense_369/kernel:0\nse_transformer_4/decoder_4/decoder_block_27/my_multi_head_attention_61/dense_369/bias:0\nse_transformer_4/decoder_4/decoder_block_27/point_wise_feed_forward_network_61/dense_370/kernel:0\nse_transformer_4/decoder_4/decoder_block_27/point_wise_feed_forward_network_61/dense_370/bias:0\nse_transformer_4/decoder_4/decoder_block_27/point_wise_feed_forward_network_61/dense_371/kernel:0\nse_transformer_4/decoder_4/decoder_block_27/point_wise_feed_forward_network_61/dense_371/bias:0\nse_transformer_4/decoder_4/decoder_block_27/layer_normalization_61/gamma:0\nse_transformer_4/decoder_4/decoder_block_27/layer_normalization_61/beta:0\nse_transformer_4/decoder_4/decoder_block_28/my_multi_head_attention_62/dense_372/kernel:0\nse_transformer_4/decoder_4/decoder_block_28/my_multi_head_attention_62/dense_372/bias:0\nse_transformer_4/decoder_4/decoder_block_28/my_multi_head_attention_62/dense_373/kernel:0\nse_transformer_4/decoder_4/decoder_block_28/my_multi_head_attention_62/dense_373/bias:0\nse_transformer_4/decoder_4/decoder_block_28/my_multi_head_attention_62/dense_374/kernel:0\nse_transformer_4/decoder_4/decoder_block_28/my_multi_head_attention_62/dense_374/bias:0\nse_transformer_4/decoder_4/decoder_block_28/my_multi_head_attention_62/dense_375/kernel:0\nse_transformer_4/decoder_4/decoder_block_28/my_multi_head_attention_62/dense_375/bias:0\nse_transformer_4/decoder_4/decoder_block_28/point_wise_feed_forward_network_62/dense_376/kernel:0\nse_transformer_4/decoder_4/decoder_block_28/point_wise_feed_forward_network_62/dense_376/bias:0\nse_transformer_4/decoder_4/decoder_block_28/point_wise_feed_forward_network_62/dense_377/kernel:0\nse_transformer_4/decoder_4/decoder_block_28/point_wise_feed_forward_network_62/dense_377/bias:0\nse_transformer_4/decoder_4/decoder_block_28/layer_normalization_62/gamma:0\nse_transformer_4/decoder_4/decoder_block_28/layer_normalization_62/beta:0\nse_transformer_4/decoder_4/decoder_block_29/my_multi_head_attention_63/dense_378/kernel:0\nse_transformer_4/decoder_4/decoder_block_29/my_multi_head_attention_63/dense_378/bias:0\nse_transformer_4/decoder_4/decoder_block_29/my_multi_head_attention_63/dense_379/kernel:0\nse_transformer_4/decoder_4/decoder_block_29/my_multi_head_attention_63/dense_379/bias:0\nse_transformer_4/decoder_4/decoder_block_29/my_multi_head_attention_63/dense_380/kernel:0\nse_transformer_4/decoder_4/decoder_block_29/my_multi_head_attention_63/dense_380/bias:0\nse_transformer_4/decoder_4/decoder_block_29/my_multi_head_attention_63/dense_381/kernel:0\nse_transformer_4/decoder_4/decoder_block_29/my_multi_head_attention_63/dense_381/bias:0\nse_transformer_4/decoder_4/decoder_block_29/point_wise_feed_forward_network_63/dense_382/kernel:0\nse_transformer_4/decoder_4/decoder_block_29/point_wise_feed_forward_network_63/dense_382/bias:0\nse_transformer_4/decoder_4/decoder_block_29/point_wise_feed_forward_network_63/dense_383/kernel:0\nse_transformer_4/decoder_4/decoder_block_29/point_wise_feed_forward_network_63/dense_383/bias:0\nse_transformer_4/decoder_4/decoder_block_29/layer_normalization_63/gamma:0\nse_transformer_4/decoder_4/decoder_block_29/layer_normalization_63/beta:0\nse_transformer_4/decoder_4/decoder_block_30/my_multi_head_attention_64/dense_384/kernel:0\nse_transformer_4/decoder_4/decoder_block_30/my_multi_head_attention_64/dense_384/bias:0\nse_transformer_4/decoder_4/decoder_block_30/my_multi_head_attention_64/dense_385/kernel:0\nse_transformer_4/decoder_4/decoder_block_30/my_multi_head_attention_64/dense_385/bias:0\nse_transformer_4/decoder_4/decoder_block_30/my_multi_head_attention_64/dense_386/kernel:0\nse_transformer_4/decoder_4/decoder_block_30/my_multi_head_attention_64/dense_386/bias:0\nse_transformer_4/decoder_4/decoder_block_30/my_multi_head_attention_64/dense_387/kernel:0\nse_transformer_4/decoder_4/decoder_block_30/my_multi_head_attention_64/dense_387/bias:0\nse_transformer_4/decoder_4/decoder_block_30/point_wise_feed_forward_network_64/dense_388/kernel:0\nse_transformer_4/decoder_4/decoder_block_30/point_wise_feed_forward_network_64/dense_388/bias:0\nse_transformer_4/decoder_4/decoder_block_30/point_wise_feed_forward_network_64/dense_389/kernel:0\nse_transformer_4/decoder_4/decoder_block_30/point_wise_feed_forward_network_64/dense_389/bias:0\nse_transformer_4/decoder_4/decoder_block_30/layer_normalization_64/gamma:0\nse_transformer_4/decoder_4/decoder_block_30/layer_normalization_64/beta:0\nse_transformer_4/decoder_4/decoder_block_31/my_multi_head_attention_65/dense_390/kernel:0\nse_transformer_4/decoder_4/decoder_block_31/my_multi_head_attention_65/dense_390/bias:0\nse_transformer_4/decoder_4/decoder_block_31/my_multi_head_attention_65/dense_391/kernel:0\nse_transformer_4/decoder_4/decoder_block_31/my_multi_head_attention_65/dense_391/bias:0\nse_transformer_4/decoder_4/decoder_block_31/my_multi_head_attention_65/dense_392/kernel:0\nse_transformer_4/decoder_4/decoder_block_31/my_multi_head_attention_65/dense_392/bias:0\nse_transformer_4/decoder_4/decoder_block_31/my_multi_head_attention_65/dense_393/kernel:0\nse_transformer_4/decoder_4/decoder_block_31/my_multi_head_attention_65/dense_393/bias:0\nse_transformer_4/decoder_4/decoder_block_31/point_wise_feed_forward_network_65/dense_394/kernel:0\nse_transformer_4/decoder_4/decoder_block_31/point_wise_feed_forward_network_65/dense_394/bias:0\nse_transformer_4/decoder_4/decoder_block_31/point_wise_feed_forward_network_65/dense_395/kernel:0\nse_transformer_4/decoder_4/decoder_block_31/point_wise_feed_forward_network_65/dense_395/bias:0\nse_transformer_4/decoder_4/decoder_block_31/layer_normalization_65/gamma:0\nse_transformer_4/decoder_4/decoder_block_31/layer_normalization_65/beta:0\nse_transformer_4/decoder_4/decoder_block_32/my_multi_head_attention_66/dense_396/kernel:0\nse_transformer_4/decoder_4/decoder_block_32/my_multi_head_attention_66/dense_396/bias:0\nse_transformer_4/decoder_4/decoder_block_32/my_multi_head_attention_66/dense_397/kernel:0\nse_transformer_4/decoder_4/decoder_block_32/my_multi_head_attention_66/dense_397/bias:0\nse_transformer_4/decoder_4/decoder_block_32/my_multi_head_attention_66/dense_398/kernel:0\nse_transformer_4/decoder_4/decoder_block_32/my_multi_head_attention_66/dense_398/bias:0\nse_transformer_4/decoder_4/decoder_block_32/my_multi_head_attention_66/dense_399/kernel:0\nse_transformer_4/decoder_4/decoder_block_32/my_multi_head_attention_66/dense_399/bias:0\nse_transformer_4/decoder_4/decoder_block_32/point_wise_feed_forward_network_66/dense_400/kernel:0\nse_transformer_4/decoder_4/decoder_block_32/point_wise_feed_forward_network_66/dense_400/bias:0\nse_transformer_4/decoder_4/decoder_block_32/point_wise_feed_forward_network_66/dense_401/kernel:0\nse_transformer_4/decoder_4/decoder_block_32/point_wise_feed_forward_network_66/dense_401/bias:0\nse_transformer_4/decoder_4/decoder_block_32/layer_normalization_66/gamma:0\nse_transformer_4/decoder_4/decoder_block_32/layer_normalization_66/beta:0\nse_transformer_4/decoder_4/decoder_block_33/my_multi_head_attention_67/dense_402/kernel:0\nse_transformer_4/decoder_4/decoder_block_33/my_multi_head_attention_67/dense_402/bias:0\nse_transformer_4/decoder_4/decoder_block_33/my_multi_head_attention_67/dense_403/kernel:0\nse_transformer_4/decoder_4/decoder_block_33/my_multi_head_attention_67/dense_403/bias:0\nse_transformer_4/decoder_4/decoder_block_33/my_multi_head_attention_67/dense_404/kernel:0\nse_transformer_4/decoder_4/decoder_block_33/my_multi_head_attention_67/dense_404/bias:0\nse_transformer_4/decoder_4/decoder_block_33/my_multi_head_attention_67/dense_405/kernel:0\nse_transformer_4/decoder_4/decoder_block_33/my_multi_head_attention_67/dense_405/bias:0\nse_transformer_4/decoder_4/decoder_block_33/point_wise_feed_forward_network_67/dense_406/kernel:0\nse_transformer_4/decoder_4/decoder_block_33/point_wise_feed_forward_network_67/dense_406/bias:0\nse_transformer_4/decoder_4/decoder_block_33/point_wise_feed_forward_network_67/dense_407/kernel:0\nse_transformer_4/decoder_4/decoder_block_33/point_wise_feed_forward_network_67/dense_407/bias:0\nse_transformer_4/decoder_4/decoder_block_33/layer_normalization_67/gamma:0\nse_transformer_4/decoder_4/decoder_block_33/layer_normalization_67/beta:0\nse_transformer_4/embedding_9/embeddings:0\n<tf.Variable 'se_transformer_4/encoder_4/encoder_block_26/my_multi_head_attention_52/dense_312/kernel:0' shape=(256, 256) dtype=float32, numpy=\narray([[-0.12695359, -0.04644895, -0.09041218, ..., -0.00736434,\n        -0.08400926, -0.02830921],\n       [-0.03846622, -0.01010104, -0.05575245, ..., -0.02522293,\n         0.14587478, -0.03220825],\n       [-0.00284962, -0.1463792 , -0.08028531, ..., -0.04677516,\n        -0.07775746,  0.05254555],\n       ...,\n       [ 0.03242698, -0.04623955, -0.01100036, ..., -0.02395525,\n         0.09743593,  0.13477185],\n       [-0.05266959, -0.09714931,  0.01227705, ..., -0.0106305 ,\n         0.03264393, -0.01598896],\n       [-0.09226196, -0.05241366, -0.14366268, ..., -0.00956151,\n        -0.00238797, -0.07577495]], dtype=float32)>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training helper functions","metadata":{"id":"5gjCRldwckzR"}},{"cell_type":"code","source":"@tf.function\ndef contains_pad(inp: tf.Tensor):\n    bool_ten = tf.math.equal(inp, pad_ten)\n    nonzero_count = tf.math.count_nonzero(bool_ten)\n    return nonzero_count > 0","metadata":{"id":"HZf9y_1nCtEa","execution":{"iopub.status.busy":"2022-07-11T13:44:53.150385Z","iopub.execute_input":"2022-07-11T13:44:53.151498Z","iopub.status.idle":"2022-07-11T13:44:53.540900Z","shell.execute_reply.started":"2022-07-11T13:44:53.151427Z","shell.execute_reply":"2022-07-11T13:44:53.539567Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{"id":"iRP0mGA1NjXi"}},{"cell_type":"code","source":"@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\ndef train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    with tf.GradientTape() as tape:\n        pred: tf.Tensor = model([inp, outp], training=True) \n        loss_val: tf.Tensor = loss_func(y_true = outp, y_pred = pred)\n    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    return tf.math.reduce_mean(loss_val)","metadata":{"id":"UR6qYltHhh_t","execution":{"iopub.status.busy":"2022-07-11T13:44:56.447453Z","iopub.execute_input":"2022-07-11T13:44:56.448894Z","iopub.status.idle":"2022-07-11T13:44:56.837349Z","shell.execute_reply.started":"2022-07-11T13:44:56.448844Z","shell.execute_reply":"2022-07-11T13:44:56.836373Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\ndef train(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()):\n    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=tf.keras.backend.floatx())\n    i = 0\n    while i < num_sets:\n        # The input is of size set_size-TAKE_TO_ACCOUNT\n        already_predicted: int = i * (set_size + 1)\n        start_from: int = max(0, already_predicted - max_seq_len)\n        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n            break\n        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n        loss_val: tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()) = train_step(inp, outp)\n        one_hot: tf.TensorSpec(shape=[num_sets], dtype=tf.keras.backend.floatx())\n        one_hot = tf.one_hot([i], num_sets, dtype=tf.keras.backend.floatx()) * loss_val\n        per_generation_loss += one_hot\n        i += 1\n    return tf.math.reduce_mean(per_generation_loss[:i])\n    ","metadata":{"id":"sSQ8nWVwMuG_","execution":{"iopub.status.busy":"2022-07-11T14:04:54.064243Z","iopub.execute_input":"2022-07-11T14:04:54.064892Z","iopub.status.idle":"2022-07-11T14:04:54.564595Z","shell.execute_reply.started":"2022-07-11T14:04:54.064833Z","shell.execute_reply":"2022-07-11T14:04:54.563017Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"### Validate","metadata":{"id":"N9vu221UNjXk"}},{"cell_type":"code","source":"@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\ndef val_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    pred = model([inp, outp], training=False)\n    loss_val = loss_func(y_true = outp, y_pred = pred)\n    return tf.math.reduce_mean(loss_val)","metadata":{"id":"gPYi_RT4Ytv_","execution":{"iopub.status.busy":"2022-07-11T13:45:01.927042Z","iopub.execute_input":"2022-07-11T13:45:01.927569Z","iopub.status.idle":"2022-07-11T13:45:02.435004Z","shell.execute_reply.started":"2022-07-11T13:45:01.927515Z","shell.execute_reply":"2022-07-11T13:45:02.433610Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\ndef validate(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()):\n    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=tf.keras.backend.floatx())\n    i = 0\n    while i < num_sets:\n        # The input is of size set_size-TAKE_TO_ACCOUNT\n        already_predicted: int = i * (set_size + 1)\n        start_from: int = max(0, already_predicted - max_seq_len)\n        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n            break\n        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n        loss_val: tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()) = val_step(inp, outp)\n        one_hot: tf.TensorSpec(shape=[num_sets], dtype=tf.keras.backend.floatx())\n        one_hot = tf.one_hot([i], num_sets, dtype=tf.keras.backend.floatx()) * loss_val\n        per_generation_loss += one_hot\n        i += 1\n    return tf.math.reduce_mean(per_generation_loss[:i])","metadata":{"id":"7NpdkcUkckzT","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T14:04:57.808261Z","iopub.execute_input":"2022-07-11T14:04:57.808725Z","iopub.status.idle":"2022-07-11T14:04:58.367172Z","shell.execute_reply.started":"2022-07-11T14:04:57.808691Z","shell.execute_reply":"2022-07-11T14:04:58.365691Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"## Chackpoints","metadata":{"id":"cEcysTey1bEk"}},{"cell_type":"code","source":"date: str = datetime.datetime.now().strftime('/%d/%m/%Y')\nbest_loss = float(\"inf\")\nbest_model = None","metadata":{"id":"Iuce_A94ckzU","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:45:06.805236Z","iopub.execute_input":"2022-07-11T13:45:06.805745Z","iopub.status.idle":"2022-07-11T13:45:07.350351Z","shell.execute_reply.started":"2022-07-11T13:45:06.805704Z","shell.execute_reply":"2022-07-11T13:45:07.348810Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"def check_point(folder_path: str, model: SeTransformer, val_loss: float, train_loss: float, test_loss = None):\n    \"\"\"Saves the model at the end of each epoch\"\"\"\n    # (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * \n    # * (3 for forward and backward pass) * (number of examples in dataset) \n    num_ops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n    tf.keras.models.save_model(model = model, filepath = \"model.pb\", save_format='tf', overwrite=True)\n    log_model(model)\n    art = wandb.Artifact(f\"{wandb.run.id}-best model\", type='my_model', description = f\"the model after {num_ops:,} operations\")\n    art.add_file(\"model.pb\")\n    run.log_artifact(artifact)\n    print(\"Saved checkpoint\")","metadata":{"id":"LZsEGz_vckzT","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-11T13:45:09.004742Z","iopub.execute_input":"2022-07-11T13:45:09.005973Z","iopub.status.idle":"2022-07-11T13:45:09.420387Z","shell.execute_reply.started":"2022-07-11T13:45:09.005903Z","shell.execute_reply":"2022-07-11T13:45:09.419386Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"def on_val_batch_end(epoch_ndx: int, batch_ndx: int, train_loss: float, val_loss: float) -> bool:\n    \"\"\"A call back after every \"\"\"\n    global best_loss\n    global best_model\n    wandb.log({\n        \"epoch_ndx\": epoch_ndx,\n        \"batch_ndx\": batch_ndx,\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss\n    })\n    if time.time() - last_save_time > 1800.0 and val_loss < math.log(vocab_size) and val_loss < best_loss:\n        best_loss = val_loss\n        # If the last save is more than a half hour (1800 sec) ago\n        # and if the predictions are not random\n        check_point(check_points_path, model, val_loss, train_loss)\n        last_save_time = time.time()\n        return False\n    elif train_loss < 0.01:\n        title: str = \"Over fitting or data leak\"\n        message = f\"Training loss is {train_loss} and val loss is {val_loss} in the latest batch\"\n        wandb.alert(title=title, text=message)\n        print(title)\n        print(message)\n        return True\n    elif time.time() - last_save_time > 18000.0 and train_loss >= math.log(vocab_size):\n        # if the prob of every token is 1/vocab_size, the loss is\n        # -ln(1/vocab_size) = ln(vocab_size) \n        # by the logrithem rule log(a^x)=xlog(a) where x = -1\n        # if after 5 hours of training, the model predictions are still random\n        title: str = \"Under fitting\"\n        message = f\"training loss is {train_loss} and val loss is {val_loss} in the latest batch\"\n        wandb.alert(title=title, text=message)\n        print(title)\n        print(message)\n        return True","metadata":{"execution":{"iopub.status.busy":"2022-07-11T13:45:15.113182Z","iopub.execute_input":"2022-07-11T13:45:15.114615Z","iopub.status.idle":"2022-07-11T13:45:15.674578Z","shell.execute_reply.started":"2022-07-11T13:45:15.114547Z","shell.execute_reply":"2022-07-11T13:45:15.673313Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"def loss_to_prob(loss: float) -> float:\n    return math.exp(-loss)","metadata":{"id":"VQENw4qSEssJ","execution":{"iopub.status.busy":"2022-07-11T13:45:19.011955Z","iopub.execute_input":"2022-07-11T13:45:19.012942Z","iopub.status.idle":"2022-07-11T13:45:19.559760Z","shell.execute_reply.started":"2022-07-11T13:45:19.012894Z","shell.execute_reply":"2022-07-11T13:45:19.555752Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"## The actual training loop!","metadata":{"id":"RN8rmbrNW0-Z"}},{"cell_type":"code","source":"def train_loop():\n    epochs: int = 1000000  # Train until the cloud disconnects or the model stops improving\n    per_epoch_train_loss: List[float] = []\n    per_epoch_val_loss: List[float] = []\n    print(f\"number of train batches per epoch: {len(list_train_set)}\")\n    last_save_time = time.time()\n    epoch: int = 0\n    batch_num: int = 0\n    for epoch in range(epochs):\n        print(f\"epoch number: {epoch}\")\n        per_batch_train_loss: List[float] = []\n        per_batch_val_loss: List[float] = []\n        for batch_num in tqdm.tqdm(range(len(list_train_set))):  # tqdm is a progress bar\n            train_loss: tf.Tensor = train(list_train_set[batch_num])\n            float_val_loss = tf.keras.backend.eval(train_loss).item()\n            per_batch_train_loss.append(train_loss)\n            if batch_num % 8 == 0:  # 8 is number of training batches/number of val batches\n                # because training set is 80% of the data and val set is 10%\n                next_val_batch: tf.Tensor = list_val_set[batch_num // 8]\n                val_loss: tf.Tensor = validate(next_val_batch)\n                float_val_loss = tf.keras.backend.eval(val_loss).item()\n                per_batch_val_loss.append(val_loss)\n                if on_val_batch_end(epoch, batch_num, train_loss, val_loss): return train_loss, val_loss\n        per_epoch_train_loss.append(statistics.mean(per_batch_train_loss))\n        per_epoch_val_loss.append(statistics.mean(per_batch_val_loss))\n        print(f\"train_loss: {per_epoch_train_loss[-1]}\")\n        print(f\"prob of right ans train: {loss_to_prob(per_epoch_train_loss[-1])}\")\n        print(f\"val_loss: {per_epoch_val_loss[-1]}\")\n        print(f\"prob of right ans val: {loss_to_prob(per_epoch_val_loss[-1])}\")\n        if len(per_epoch_val_loss) > 1:\n            if per_epoch_val_loss[-1] >= per_epoch_val_loss[-2]:\n                print(\"Validation loss increased. Stopped training\")\n                return per_epoch_train_loss[-1], per_epoch_val_loss[-1]\n        check_point(check_points_path, model, per_epoch_val_loss[-1], per_epoch_train_loss[-1])\n        last_save_time = time.time()\n        print(\"Saved checkpoint\")","metadata":{"id":"nHktVH98kp4B","pycharm":{"name":"#%%\n"},"scrolled":true,"execution":{"iopub.status.busy":"2022-07-11T14:05:04.584160Z","iopub.execute_input":"2022-07-11T14:05:04.584641Z","iopub.status.idle":"2022-07-11T14:05:05.003889Z","shell.execute_reply.started":"2022-07-11T14:05:04.584604Z","shell.execute_reply":"2022-07-11T14:05:05.002086Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"!export AUTOGRAPH_VERBOSITY=10\ntrain_loss, val_loss = train_loop()","metadata":{"id":"g5MukK3ynG7v","outputId":"dca87c06-0140-41d9-c5bd-1e71ea9c9289","execution":{"iopub.status.busy":"2022-07-11T14:05:33.522378Z","iopub.execute_input":"2022-07-11T14:05:33.522832Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"number of train batches per epoch: 504\nepoch number: 0\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/504 [00:00<?, ?it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"## reproduce error","metadata":{"id":"i48dY189QR1i"}},{"cell_type":"code","source":"exm_model = tf.keras.Sequential([\n    tf.keras.layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n    tf.keras.layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n    tf.keras.layers.Dense(1, name=\"layer3\"),\n])\nexm_optimizer = tf.keras.optimizers.Adam()\nmse = tf.keras.losses.MeanSquaredError()\nexm_model.compile(optimizer = exm_optimizer, loss = mse)\n\n@tf.function(input_signature=(tf.TensorSpec(shape=[3,3], dtype=tf.int32),\n                              tf.TensorSpec(shape=[3], dtype=tf.int32)))\ndef exm_train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    with tf.GradientTape() as tape:\n        pred: tf.Tensor = exm_model(inp) \n        loss_val: tf.Tensor = mse(outp, pred)\n    grads: tf.RaggedTensor = tape.gradient(loss_val, exm_model.trainable_weights)\n    exm_optimizer.apply_gradients(zip(grads, exm_model.trainable_weights))\n    return tf.math.reduce_mean(loss_val)\n\n\nexm_inp = tf.random.uniform(shape=[3, 3], dtype=tf.int32, maxval=100, minval=0)\nexm_out = tf.random.uniform(shape=[3], dtype=tf.int32, maxval=100, minval=0)\n\n\nexm_train_step(exm_inp, exm_out).numpy()","metadata":{"id":"6EondR_UQUe4","execution":{"iopub.status.busy":"2022-07-11T13:53:52.334204Z","iopub.execute_input":"2022-07-11T13:53:52.334709Z","iopub.status.idle":"2022-07-11T13:53:53.197501Z","shell.execute_reply.started":"2022-07-11T13:53:52.334670Z","shell.execute_reply":"2022-07-11T13:53:53.196178Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"2343.0"},"metadata":{}}]},{"cell_type":"markdown","source":"## After training","metadata":{"id":"Z-iopedNNjXs"}},{"cell_type":"code","source":"test_loss = statistics.mean([validate(test_batch) for test_batch in tqdm.tqdm(list_test_set)])\nprint(f\"Test loss: {test_loss}\")\ncheck_point(check_points_path, model, val_loss, train_loss, test_loss)","metadata":{"id":"1VtC8sP_NjXu","execution":{"iopub.status.busy":"2022-07-11T13:36:38.723714Z","iopub.execute_input":"2022-07-11T13:36:38.725245Z","iopub.status.idle":"2022-07-11T13:36:39.066223Z","shell.execute_reply.started":"2022-07-11T13:36:38.725168Z","shell.execute_reply":"2022-07-11T13:36:39.064319Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":76,"outputs":[{"name":"stderr","text":"  0%|          | 0/128 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function validate at 0x7f07b41bb5f0> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: name 'fscope' is not defined\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_8498/156358468.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test loss: {test_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheck_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_points_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_8498/156358468.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test loss: {test_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheck_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_points_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_8498/2192025570.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mstart_from\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malready_predicted\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_from\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mset_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mhave_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontains_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_output_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_static_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhave_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'contains_pad' is not defined"],"ename":"NameError","evalue":"name 'contains_pad' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}