{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"ZZf-fbj50Kqc"}},{"cell_type":"code","source":"import sys\ndevice: str\nif 'google.colab' in sys.modules.keys():\n    device = 'colab'\nif 'kaggle_web_client' in sys.modules.keys():\n    device = 'kaggle'\nelse:\n    device = 'locally'","metadata":{"execution":{"iopub.status.busy":"2022-07-12T18:47:15.409525Z","iopub.execute_input":"2022-07-12T18:47:15.411349Z","iopub.status.idle":"2022-07-12T18:47:15.420404Z","shell.execute_reply.started":"2022-07-12T18:47:15.411266Z","shell.execute_reply":"2022-07-12T18:47:15.419070Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def install_libaries():\n    if device == 'kaggle':\n        !conda create -y -q -d --name create_models_venv numpy tensorflow=2.9.1 pandas seaborn\n        # create a conda virtual env with the liberies: numpy tensorflow pandas matplotlib seaborn\n        # -y is for dont ask if Im sure\n        !pip install -q tensorflow-text  # some might not be avilable using conda install so pip install them\n        !pip install -q tqdm\n    elif device == 'colab':\n        %pip install -q tensorflow-text\n        %pip install -q tqdm\n        %pip install -q wandb --upgrade\n        # q for quiet","metadata":{"execution":{"iopub.status.busy":"2022-07-12T18:49:49.844120Z","iopub.execute_input":"2022-07-12T18:49:49.844616Z","iopub.status.idle":"2022-07-12T18:49:49.862552Z","shell.execute_reply.started":"2022-07-12T18:49:49.844580Z","shell.execute_reply":"2022-07-12T18:49:49.861220Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# standart liberies:\nfrom typing import Optional, List, Set, Dict, Tuple\nimport datetime\nimport os\nimport random\nimport statistics\nimport math\nimport time\nimport copy\n# NON-standart liberies:\n# from flopco_keras import FlopCoKeras  # flop counter for keras at https://github.com/evgps/flopco-keras\ntry:\n    import wandb\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers, Model\n    import pandas as pd\n    import seaborn as sns\n    import numpy as np\n    import tqdm\n    from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n    from tensorflow_text import FastWordpieceTokenizer\nexcept ModuleNotFoundError:\n    install_libaries()\n    from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n    from tensorflow_text import FastWordpieceTokenizer\nexcept ImportError:\n    install_libaries()\n    from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n    from tensorflow_text import FastWordpieceTokenizer","metadata":{"gather":{"logged":1644854036589},"id":"MsfMr-Qod_nl","outputId":"e09897f8-e611-4818-b6b6-adf1e370cccf","execution":{"iopub.status.busy":"2022-07-12T18:49:52.716633Z","iopub.execute_input":"2022-07-12T18:49:52.717792Z","iopub.status.idle":"2022-07-12T18:50:15.973609Z","shell.execute_reply.started":"2022-07-12T18:49:52.717719Z","shell.execute_reply":"2022-07-12T18:50:15.971517Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nCondaValueError: Cannot `create --dry-run` with an existing conda environment\n\n\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_38905/1761662653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.tools'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_38905/1761662653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0minstall_libaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_vocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbert_vocab_from_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastWordpieceTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"Various tensorflow ops related to text-processing.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_undocumented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_recalculate\u001b[0;34m(self)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_parent_path\u001b[0;34m(self)\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'tensorflow'"],"ename":"KeyError","evalue":"'tensorflow'","output_type":"error"}]},{"cell_type":"code","source":"from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\nfrom tensorflow_text import FastWordpieceTokenizer\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Python version: {sys.version}\")\nprint(f\"Tensorflow version: {tf.__version__}\")","metadata":{"gather":{"logged":1644854037100},"id":"SzaglEQ-kp2n","outputId":"5b7fd3b9-3369-431c-b516-bde95519988f","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Settings","metadata":{"id":"91ZJ0-RZIx6j"}},{"cell_type":"code","source":"tf.random.set_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\nkeras.utils.set_random_seed(0)\ntf.config.experimental.enable_op_determinism()\n# keras.backend.set_floatx('float16')\nf_type = keras.backend.floatx()  # either tf.float16 or tf.float32","metadata":{"gather":{"logged":1644854038983},"id":"2mp2yFTEkp2u","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a strategy - Accelerator optimization ","metadata":{"id":"QlubPo7LRyNm"}},{"cell_type":"code","source":"try:\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver();\n    tf.config.experimental_connect_to_cluster(resolver);\n    tf.tpu.experimental.initialize_tpu_system(resolver);\n    strategy = tf.distribute.TPUStrategy(resolver);\nexcept ValueError:\n    pass","metadata":{"id":"s0yqg12zqSt9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loading","metadata":{"id":"yVR1oJRD0NtO","_kg_hide-input":true}},{"cell_type":"code","source":"if device == 'colab':  # If notebook is ran on colab\n    from google.colab import drive\n    drive.mount('/drive')\n    df: pd.DataFrame = pd.read_csv('/drive/MyDrive/final_project/wikipedia_articles.csv')\nelif device == 'kaggle':\n    df: pd.DataFrame = pd.read_csv('../input/wikipedia-promotional-articles/promotional.csv')\nelse:  # If notebook is ran on my laptop\n    df: pd.DataFrame = pd.read_csv('wiki_data/articles.csv')\nprint(f\"the shape of the dataframe: {df.shape}\")","metadata":{"gather":{"logged":1644857858611},"id":"pn1UfAXQwlQX","outputId":"af20cbcb-3d71-4d4b-be9a-5850b9f9741c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df: pd.Series = df['text']\ndata_list: List[str] = df.to_list()\nDATA_SIZE = len(data_list)\nprint(f\"There are {DATA_SIZE} data points\")\nstring_lengths: List[int] = [len(data_point) for data_point in data_list]\nmax_string_len = max(string_lengths)\nprint(f\"The length of the longest text IN CHARACTERS is: {max_string_len}\")\nmin_string_len = min(string_lengths)\nprint(f\"The length of the shortest text IN CHARACTERS is: {min_string_len}\")","metadata":{"id":"IW43lG6iUV6G","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"outputId":"3fad9123-c6f0-4cf6-f625-6d22d0241bda","collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the vocabulary and tokenizer","metadata":{"id":"87Putqz6kp20"}},{"cell_type":"code","source":"# %%time\nbert_tokenizer_params: dict = dict(lower_case=True)\nVOCAB_SIZE: int = 8192  # Always the same for all models\n\nif device == 'colab':  # If notebook is ran on colab\n    path = '/drive/MyDrive/final_project/vocab.txt'\nif device == 'kaggle':\n    path = \"../input/my-lookup-table/look_up_table.txt\"\nelse:  # If notebook is ran on my laptop\n    path = 'C:/yoni/final_project/model/vocab.txt'\n\n\nreserved_tokens: List[str] = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", \"[MASK]\"]\n\nif os.path.exists(path):\n    with open(path, 'r') as f:\n        vocab: List[str] = f.read().split()\nelse:\n    bert_vocab_args: dict = dict(\n        # The target vocabulary size\n        vocab_size = VOCAB_SIZE,\n        # Reserved tokens that must be included in the vocabulary\n        reserved_tokens=reserved_tokens,\n        # Arguments for `tf_text.BertTokenizer`\n        bert_tokenizer_params=bert_tokenizer_params,\n        # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n        learn_params={},\n    )\n    tensor_list: list = [tf.convert_to_tensor(data_point) for data_point in data_list]\n    data_set: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tensor_list)\n    # I already ran this code and saved the file to C:/yoni/final_project/model/vocab.txt\n    vocab: List[str] = bert_vocab_from_dataset(\n        data_set,\n        **bert_vocab_args,)\n    with open('C:/yoni/final_project/model/vocab.txt', 'w') as f:\n        for token in vocab:\n            f.write(token + ' ')\nprint(f\"the type of the items in vocab: {type(vocab[0])}\")\nprint(f\"the first 15 items in vocab: {vocab[:15]}\")\nvocab_size = len(vocab)\nprint(f\" the length of vocab: {vocab_size}\")\ntokenizer = FastWordpieceTokenizer(vocab, support_detokenization=True, token_out_type=tf.int32)","metadata":{"id":"KBhC3SPyMi9n","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf.Ragged([[\"this is my string\"]])\nprint(x)\nprint(tokenizer.tokenize(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the data","metadata":{"id":"A73Lbgg6kp3R"}},{"cell_type":"code","source":"# START: int = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")  # The value of the start token\n# END: int = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")  # The value of the end token\n# starts = tf.cast(tf.Variable([START]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\n# ends = tf.cast(tf.Variable([END]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\nstarts = tf.constant([2], dtype=tf.int32)\nends = tf.constant([3], dtype=tf.int32)\npad_int: int = int(tf.argmax(tf.constant(reserved_tokens) == \"[PAD]\"))\npad_ten: tf.TensorSpec(dtype=tf.int32, shape=()) = tf.constant([pad_int], dtype=tf.int32)","metadata":{"id":"mLxV5l562-x2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_string(text: str) -> tf.Tensor:\n    \"\"\"Converts string to tensor\"\"\"\n    ragged: tf.RaggedTensor = tokenizer.tokenize(text)\n    eager: tf.Tensor = ragged.to_tensor(default_value=0, shape=[None, 1])  # 0 is the value of the padding token\n    sqeezed: tf.Tensor = tf.squeeze(eager, axis=1)\n    typed: tf.Tensor = tf.cast(sqeezed, tf.int32)\n    return edited","metadata":{"id":"4gPxVc7tkp3X","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_data: List[tf.Tensor] = [tokenize_string(data_point) for data_point in data_list] \nprint(f\"An example of a shape of tokens tensor: {tokenized_data[0].shape}\")\nprint(f\"examples of tokens tensor: {tokenized_data[0][:10]}\")\nlengths_tokenized: List[int] = [text.shape[0] for text in tokenized_data]\nprint(f\"The maximum length of a tokens tensor: {max(lengths_tokenized)}\")\nprint(f\"The minimum length of a tokens tensor: {min(lengths_tokenized)}\")\nsns.displot(lengths_tokenized);","metadata":{"id":"UzHmzHSukp3a","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### chunk too long texts","metadata":{"id":"EtjxgLWMkp3l"}},{"cell_type":"code","source":"max_seq_len: int = 256\ndef chunk_tensor(tensor: tf.Tensor, max_len: int = max_seq_len) -> List[tf.Tensor]:\n    \"\"\"Splits 1d tensor to chunks (1d tensors) of maximum size: max_len\"\"\"\n    return [tensor[i*max_len:(i+1)*max_len] for i in range(tensor.shape[0] // max_len)]","metadata":{"id":"MubdHBEMkJNC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunked_data: List[tf.Tensor] = []\nfor tensor in tokenized_data:\n    chunks = chunk_tensor(tensor, max_seq_len)\n    for chunk in chunks:\n        chunked_data.append(chunk)\nDATA_SIZE: int = len(chunked_data)\nprint(f\"The data set size after chunking {DATA_SIZE}\")\nprint(f\"An example of a chunked shape{chunked_data[0].shape}\")","metadata":{"id":"9DgU2Or2kp3m","outputId":"596803b2-a306-4ef7-92c0-a497ecc507e3","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Padding","metadata":{"id":"R8Z2_P2Lnke8"}},{"cell_type":"code","source":"def pad(tensor: tf.Tensor, pad_int: int) -> tf.Tensor:\n    \"\"\"Pads the tensor to the length of the longest text in the data set\"\"\"\n    padded: tf.Tensor = tf.pad(tensor=tensor, paddings=[[pad_int, max_seq_len - tensor.shape[0]]], mode='CONSTANT', constant_values=0)\n    # 0 is the padding token\n    return padded","metadata":{"id":"XUOVoGBfkp3p","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_data: List[tf.Tensor] = [pad(text, pad_int) for text in chunked_data]\nchunked_data.sort(key = lambda t: t.shape[0])  # sorting so that every batch will have similar sized texts, used when training","metadata":{"id":"nRa9_ryHkp3p","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train test val split","metadata":{"id":"gbnub0UZkp3s"}},{"cell_type":"code","source":"batch_size: int = 128\n\ndef list_to_dataset(tokenized_list: List[tf.Tensor]) -> tf.data.Dataset:\n    \"\"\"Converts a list of tokenized texts after all preprocessing to a tf.data.Dataset\"\"\"\n    dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tokenized_list)\n    batched: tf.data.Dataset = dataset.batch(batch_size)\n    return batched\n\nbatched_data_ten = list_to_dataset(padded_data)\nbatched_data_list = list(batched_data_ten)\nrandom.shuffle(batched_data_list)\nif batched_data_list[-1].shape[0] != batch_size:\n    batched_data_list = batched_data_list[:-1]\ndata_size = len(batched_data_list)\ntrain_size: int = int(data_size * 0.8) \nval_test_size: int = int(data_size * 0.1)  # Both validation and test get 10% of the data\nlist_train_set: List[tf.Tensor] = batched_data_list[:train_size]\nlist_val_set: List[tf.Tensor] = batched_data_list[train_size:(train_size + val_test_size)]\nlist_test_set: List[tf.Tensor] = batched_data_list[(train_size + val_test_size)]","metadata":{"id":"pTW48b8Qr6sv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clear memory","metadata":{"id":"HzS72rcIvL0N"}},{"cell_type":"code","source":"del batched_data_list, train_size, val_test_size, data_size\ndel padded_data, chunked_data, tokenized_data, data_list, df, lengths_tokenized, reserved_tokens\ndel bert_tokenizer_params, ends, starts, vocab\ndel chunk, chunks","metadata":{"id":"VlkhQigKvOQT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n","metadata":{"id":"5vBLynXu0QI1"}},{"cell_type":"markdown","source":"## Positional encoding","metadata":{"id":"A9qUAZ00xiVd"}},{"cell_type":"markdown","source":"The formula for calculating the positional encoding is as follows:\n\n$${PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n$${PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n\nwhere $d_{model}$ is the model dimension, $pos$ is the position and $i$ is the index of the embedding.\nthis is taken from the paper: attention is all you need.","metadata":{"id":"_V1b_16-kp3v"}},{"cell_type":"code","source":"def create_positional_encoding(max_len: int, d_model: int) -> tf.Tensor:\n    \"\"\"Returns the positional encoding for a given a maximal sequence length and model dimension.\n    used in SeTransformer.__init__()\n    inputs: max_len: int, d_model: int\n    returns: tf.Tensor of shape (1, max_len, d_model) and dtype f_type\n    The 1 is for the batch dimension, the place in the batch dimension does not matter\"\"\"\n\n    def get_angles(positions: np.ndarray, timestamps: np.ndarray, d_model: int) -> np.ndarray:\n        \"\"\"Returns the angle in radians for given positions, timestamps and the dimension of the model\n        input: positions: np.ndarray of shape (max_len, 1), timestamps: np.ndarray of shape (1, d_model), d_model: int\n        output: np.ndarray of shape (max_len, d_model)\"\"\"\n        if f_type == \"float32\":\n            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float32(d_model)))\n        else:\n            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float16(d_model)))\n\n        return positions * angle_rates\n    \n    angle_rads = get_angles(np.arange(max_len)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)  # (max_len, d_model)\n\n    # apply sin to even indices in the array; 2i for i in range(d_model // 2)\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # (max_len, d_model)\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # (max_len, d_model)\n\n    pos_encode = angle_rads[np.newaxis, ...]  # (1, max_len, d_model)\n\n    return tf.cast(pos_encode, dtype=f_type)","metadata":{"id":"Wx4hze4k0yls","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Masking","metadata":{"id":"6ib5F3hnxrE9"}},{"cell_type":"markdown","source":"Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.","metadata":{"id":"3TXw70UlzcVi"}},{"cell_type":"markdown","source":"The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n\nThis means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on.","metadata":{"id":"4w68l26GzrqG"}},{"cell_type":"code","source":"def create_masks(inp: tf.Tensor, tar: tf.Tensor, pad_ten: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n        \"\"\"Creates all the masks needed for the model\n        input: inp: tf.Tensor of shape (batch_size, seq_len), tar: tf.Tensor of shape (batch_size, set_size)\n        Returns: tuple of (padding_mask, look_ahead_mask)\n        padding_mask, look_ahead_mask: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n        \n        def create_padding_mask(seq: tf.Tensor) -> tf.Tensor:\n                \"\"\"Returns a padding mask for the given sequence.\n                input: seq: tf.Tensor of shape (batch_size, seq_len)\n                Returns: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n                seq = tf.cast(tf.math.equal(seq, pad_ten), f_type)  \n                # For every item in the sequence, 1 if it is a padding token, 0 if it is not \n\n                # add extra dimensions to add the padding\n                \n                return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n        \n        # Encoder padding mask\n        padding_mask: tf.Tensor = create_padding_mask(inp)  # (batch_size, 1, 1, seq_len)\n\n        # Used in the 1st attention block in the decoder.\n        # It is used to pad and mask future tokens in the input received by\n        # the decoder.\n        set_size: int = tar.shape[1]\n\n        def create_look_ahead_mask(set_size: int) -> tf.Tensor:\n                mask = 1 - tf.linalg.band_part(tf.ones((set_size, set_size)), -1, 0)\n                mask = tf.cast(mask, dtype=f_type)\n                return mask  # (seq_len, seq_len)\n\n        look_ahead_mask = create_look_ahead_mask(set_size)  # (seq_len, seq_len)\n        dec_target_padding_mask = create_padding_mask(tar)  # (batch_size, 1, 1, seq_len)\n        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) # (batch_size, 1, 1, seq_len)\n\n        return padding_mask, look_ahead_mask","metadata":{"id":"BQkjgUVVckzJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Layers and blocks","metadata":{"id":"9KHKaxz3xumc"}},{"cell_type":"code","source":"class ScaledDotProductAttention(layers.Layer):\n    def __init__(self, d_model: int, **kwargs):\n        super(ScaledDotProductAttention, self).__init__(**kwargs)\n        # scale = 1 / sqrt(d_model)\n        self.scale = tf.math.pow(tf.cast(d_model, f_type), -0.5)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n        \"\"\"Scaled Dot-Product Attention\n        input: \n        q: tf.Tensor of shape (batch_size, seq_len, d_model), \n        k: tf.Tensor of shape (batch_size, seq_len, d_model), \n        v: tf.Tensor of shape (batch_size, seq_len, d_model), \n        mask: Optional[tf.Tensor] of shape (batch_size, 1, 1, seq_len)\n        output: tf.Tensor of shape (batch_size, seq_len, d_model)\"\"\"\n        matmul_qk: tf.Tensor = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n        # q @ transpose(k)\n\n        # Scaled Dot-Product Attention\n        scaled_attention_logits: tf.Tensor = matmul_qk * self.scale  # (..., seq_len_q, seq_len_k)\n        # matmul_qk / sqrt(d_model)\n\n        # Masking\n        if mask is not None:\n            # noinspection PyTypeChecker\n            if f_type == 'float16':\n                # tf.float16.min is minus infinity\n                scaled_attention_logits += (mask * tf.float16.min)  # changed from -1e9 to prevent nan's\n            else:\n                scaled_attention_logits += (mask * -1e9) \n\n        # Normalize\n        attention_weights = self.softmax(scaled_attention_logits)\n        # (..., seq_len_q, seq_len_k)\n\n        # Output\n        output = tf.matmul(attention_weights, v)\n\n        return output","metadata":{"id":"gRXst-o2NjXL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyMultiHeadAttention(keras.Model):\n    \"\"\"U can use the built-in layers.multihead_attention but is caused a bug for me\"\"\"\n    def __init__(self, num_heads: int, d_model: int, **kwargs):\n        super(MyMultiHeadAttention, self).__init__(**kwargs)\n        if d_model % num_heads != 0:\n            raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\")\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.depth = d_model // self.num_heads\n\n        self.wq = layers.Dense(d_model)\n        self.wk = layers.Dense(d_model)\n        self.wv = layers.Dense(d_model)\n\n        self.dense = layers.Dense(d_model)\n        self.sdpa = ScaledDotProductAttention(d_model)\n\n        \n    def split_heads(self, x: tf.Tensor, batch_size: int) -> tf.Tensor:\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    \n    def call(self, v_k: tf.Tensor, q: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n        \"\"\"inputs:\n        v_k: tf.Tensor of shape (batch_size, seq_len, d_model) in self attention keys and values are the same\n        q: tf.Tensor of shape (batch_size, seq_len, d_model)\n        mask: Optional[tf.Tensor] of shape (batch_size, seq_len)\"\"\"\n        batch_size = tf.shape(q)[0]\n\n        q: tf.Tensor = self.wq(q)  # (batch_size, seq_len, d_model)\n        k: tf.Tensor = self.wk(v_k)  # (batch_size, seq_len, d_model)\n        v: tf.Tensor = self.wv(v_k)  # (batch_size, seq_len, d_model)\n\n        q: tf.Tensor = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k: tf.Tensor = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v: tf.Tensor = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape should be (batch_size, num_heads, seq_len_q, depth)\n        scaled_attention = self.sdpa(q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n         # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n          # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output","metadata":{"id":"_0I9SZI0kp31","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PointWiseFeedForward(keras.Model):\n    def __init__(self, d_model: int, dff: int, **kwargs): \n        super(PointWiseFeedForward, self).__init__(**kwargs)\n        self.layer1 = layers.Dense(dff, activation='relu')  # (batch_size, seq_len, dff)\n        self.layer2 = layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    \n    def call(self, x: tf.Tensor) -> tf.Tensor:\n        \"\"\"Gets and returns tensor of shape (batch_size, seq_len, d_model) and dtype keras.beckend.floatx()\"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return x","metadata":{"id":"QOVbpMxtNjXQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(keras.Model):\n    def __init__(self, d_model: int, num_heads: int, dff: int, drop_out_rate: float, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n\n        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n        self.ffn = PointWiseFeedForward(d_model, dff)\n\n        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout = layers.Dropout(drop_out_rate)\n\n    def call(self, x: tf.Tensor, training: bool, mask: tf.Tensor) -> tf.Tensor:\n        \n        attn_output = self.mha(x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout(attn_output, training=training)  # (batch_size, input_seq_len, d_model)\n        # out1 = self.layer_norm(x + attn_output)  # (batch_size, input_seq_len, d_model)\n        # might be data leak\n        out1 = self.layer_norm(attn_output)  # (batch_size, input_seq_len, d_model)\n        \n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout(ffn_output, training=training)  # (batch_size, input_seq_len, d_model)\n        out2 = self.layer_norm(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2","metadata":{"id":"8mEP3-Jx2n39","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(keras.Model):\n    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float, **kwargs):\n        super(DecoderBlock, self).__init__(**kwargs)\n\n        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n\n        self.ffn = PointWiseFeedForward(d_model, dff)\n\n        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout = layers.Dropout(rate)\n\n    def call(self, x: tf.Tensor, enc_output: tf.Tensor, look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor, training):\n        # enc_output.shape should be (batch_size, input_seq_len, d_model)\n\n        attn1 = self.mha(x, x, look_ahead_mask)  # (batch_size, set_size, d_model)\n        attn1 = self.dropout(attn1, training=training)  # (batch_size, set_size, d_model)\n        # out1 = self.layer_norm(attn1 + x)\n        # might be data leak\n        out1 = self.layer_norm(attn1)  # (batch_size, set_size, d_model)\n\n        attn2 = self.mha(enc_output, out1, padding_mask)  # (batch_size, set_size, d_model)\n        attn2 = self.dropout(attn2, training=training)  # (batch_size, set_size, d_model)\n        out2 = self.layer_norm(attn2 + out1)  # (batch_size, set_size, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, set_size, d_model)\n        ffn_output = self.dropout(ffn_output, training=training)\n        out3 = self.layer_norm(ffn_output + out2)  # (batch_size, set_size, d_model)\n\n        return out3 ","metadata":{"id":"xJWFrv0g281G","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(keras.Model):\n    def __init__(self, pos_encoding: tf.Tensor, num_blocks: int, d_model: int, num_heads: int, dff: int, rate=0.1, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n        self.d_model = d_model\n        self.num_blocks = num_blocks\n        self.pos_encoding = pos_encoding\n\n        self.enc_blocks = [EncoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n        # the encoder \n        self.dropout = layers.Dropout(rate)\n        self.scale = tf.math.sqrt(tf.cast(self.d_model, f_type))\n\n    def call(self, x: tf.Tensor, training, mask: tf.Tensor) -> tf.Tensor:\n\n        seq_len = tf.shape(x)[1]\n\n        # adding position encoding.\n        # assert not tf.math.is_nan(x[0][0][0])\n        x *= self.scale\n        # assert not tf.math.is_nan(x[0][0][0])\n        \n        x += self.pos_encoding[:, :seq_len, :]  # (batch_size, input_seq_len, d_model)\n        # assert not tf.math.is_nan(x[0][0][0])\n        x = self.dropout(x, training=training)  # (batch_size, input_seq_len, d_model)\n        # assert not tf.math.is_nan(x[0][0][0])\n\n        for block in self.enc_blocks:\n            x = block(x, training, mask)  # (batch_size, input_seq_len, d_model)\n            # assert not tf.math.is_nan(x[0][0][0])\n\n        return x  # (batch_size, input_seq_len, d_model)  ","metadata":{"id":"NdlS1kfM3k5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(keras.Model):\n    def __init__(self, pos_encoding, num_blocks: int, d_model: int, num_heads: int, dff: int,\n                 vocab_size: int, rate: float, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n\n        self.scale = tf.math.sqrt(tf.cast(d_model, f_type))\n        self.num_blocks = num_blocks\n        self.pos_encoding = pos_encoding\n\n        self.embedding = layers.Embedding(vocab_size, d_model)\n        self.dec_blocks = [DecoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n        self.dropout = layers.Dropout(rate)\n\n    def call(self, tar: tf.Tensor, enc_output: tf.Tensor, training: bool,\n             look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor) -> tf.Tensor:\n\n        seq_len = tf.shape(tar)[1]\n\n        x = self.embedding(tar)  # (batch_size, set_size, d_model)\n        x *= self.scale\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for block in self.dec_blocks:\n            x = block(x=x, enc_output=enc_output, look_ahead_mask=look_ahead_mask,\n                      padding_mask=padding_mask, training=training)\n\n        # x.shape should be (batch_size, set_size, d_model)\n        return x","metadata":{"id":"QLE8js6O3p5-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EmbeddingTransposed(layers.Layer):\n    def __init__(self, tied_to: layers.Embedding = None, activation: Optional[str] = None, **kwargs):\n        super(EmbeddingTransposed, self).__init__(**kwargs)\n        self.tied_to = tied_to\n        self.activation = keras.activations.get(activation)\n\n    def build(self, input_shape):\n        self.custom_weights = self.tied_to.weights[0]\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], keras.backend.int_shape(self.tied_to.weights[0])[0]\n\n    def call(self, inputs, mask=None):\n        output = keras.backend.dot(inputs, keras.backend.transpose(self.custom_weights))\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def get_config(self):\n        config = {\"activation\": keras.activations.serialize(self.activation)}\n        base_config = super(EmbeddingTransposed, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"id":"6uaYfyRdEtBN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The full model","metadata":{"id":"1-xe3K7LyD2l"}},{"cell_type":"code","source":"def count_layers(my_model) -> int:\n    \"\"\"Counts the layers of a keras model recursizely\"\"\"\n    if not isinstance(my_model, keras.Model): \n        if isinstance(my_model, layers.Layer):\n            return 1\n        return 0\n    return sum([count_layers(sub_model) for sub_model in my_model.layers])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SeTransformer(keras.Model):\n    \"\"\"The base architecture of my models in this project.\"\"\"\n    def __init__(self, num_blocks: int, d_model: int, num_heads: int, dff: int,\n                 vocab_size: int, max_len: int, rate: float, pad_int: int, **kwargs):\n        super(SeTransformer, self).__init__(**kwargs)  # calls keras.Model's __init__ method with kwarg as key worg arguments\n        self.pad_int = pad_int\n        self.vocab_size = vocab_size\n        pos_encoding = create_positional_encoding(max_len, d_model)\n        self.d_model = d_model\n        self.encoder = Encoder(pos_encoding, num_blocks, d_model, num_heads, dff, rate)\n        self.decoder = Decoder(pos_encoding, num_blocks, d_model, num_heads, dff, vocab_size, rate)\n        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n        self.emb_trans = EmbeddingTransposed(self.embedding, \"softmax\")\n\n\n    def get_layer_count(self) -> int:\n        return count_layers(self)\n    \n    \n    def summary(self, **kwargs) -> None:\n        super().summary(**kwargs)\n        print(f\"The model have {self.get_layer_count()} layers\")\n        \n    \n    def count_params(self) -> int:\n        \"\"\"counts trainable parameters\n        Raises an error if caleed before building the model\"\"\"\n        param_count: int = self.encoder.count_params() + self.decoder.count_params() + self.embedding.count_params()\n        return param_count\n    \n    \n    def build_graph(self) -> keras.Model:\n        \"\"\"Returns a functional keras model identical to the model\"\"\"\n        inp = layers.Input(shape=(batch_size, max_seq_len))\n        tar = layers.Input(shape=(batch_size, set_size))\n        return keras.Model(inputs=[[inp, tar], True], outputs=self.call([inp, tar], True))\n    \n    \n    def call(self, inputs: List[tf.Tensor], training: bool) -> tf.Tensor:\n        inp, tar = inputs\n        # inp.shape should be (batch_size, max_seq_len)\n        # tar.shape should be (batch_size, set_size)\n        x = self.embedding(inp)  # (batch_size, max_seq_len, d_model)\n        padding_mask, look_ahead_mask = create_masks(inp, tar, self.pad_int)\n        enc_output = self.encoder(x, training, padding_mask)  # (batch_size, max_seq_len, d_model)\n        dec_output = self.decoder(tar, enc_output, training, look_ahead_mask, padding_mask)  # (batch_size, set_size, d_model)\n        final_output = self.emb_trans(dec_output)  # (batch_size, set_size, vocab_size)\n        return final_output","metadata":{"id":"4NLhyE0T3tUs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_transformer = SeTransformer(\n    num_blocks=2, d_model=32, num_heads=4, dff=128,\n    vocab_size=7000,\n    max_len=1200, pad_int=0, rate=0.1)\n\ntemp_input = tf.random.uniform((1, 200), dtype=tf.int32, minval=5, maxval=6999)\ntemp_target = tf.random.uniform((1, 8), dtype=tf.int32, minval=5, maxval=6999)\ntemp_target2 = temp_target + 3\n\ntrain_out = sample_transformer([temp_input, temp_target], training=True)\ntrain_out2 = sample_transformer([temp_input, temp_target2], training=True)\n\ntry:\n    tf.debugging.assert_equal(train_out, train_out2)\nexcept tf.errors.InvalidArgumentError:\n    print(\"WARNING: model output might depends on the target\")\n\ndel sample_transformer, train_out, temp_target2, train_out2","metadata":{"id":"3WZ41cb-1B_f","outputId":"36a9e5ad-0737-4a6c-ef71-b85b208ca4e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{"id":"hFxiRfCekp38"}},{"cell_type":"code","source":"# model_art = wandb.use_artifact(f\"{model_collection_name}:latest\")\n# model_path = model_art.get_path(\"model.pb\").download()\n# model = tf.saved_model.load(model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyper-Parameters","metadata":{"id":"B5rtigmHckyv"}},{"cell_type":"code","source":"set_size: int = 2\nlearning_rate: float = 0.005\n\nnum_sets: int = (max_seq_len // set_size) - 1 # Because we dont predict the first set\n# number of sets in each sequence\n\nnum_blocks: int = 16\nd_model: int = 512\ndff: int = 1024\nnum_heads: int = 32\ndropout_rate: float = 0.1","metadata":{"id":"bWmh89nVckyw","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weights and Biases","metadata":{}},{"cell_type":"code","source":"if device == 'kaggle':\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_API_KEY\")\n    except Exception:\n        print(\"please enter your weights and biases API key\")\n!wandb login","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try: \n#     artifect = use_artifact(artifact, use_as=None)\n#     art = wandb.use_artifact(...)\n#     wandb.run.link_artifact(art, 'yonikremer/final_project_owned/version0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not 'run' in globals():\n    run = wandb.init(\n        project=\"final_project_owned\",\n        entity=\"yonikremer\",\n        name=datetime.datetime.today().strftime(f\"run from %d/%m/%Y\"),\n        settings=wandb.Settings(start_method=\"thread\"),\n        config = {\"set size\": set_size,\n                  \"batch size\": batch_size,\n                  \"learning rate\": learning_rate,\n                  \"max seq len\": max_seq_len,\n                  \"num blocks\": num_blocks,\n                  \"model dimention\": d_model,\n                  \"dff\": dff,\n                  \"num heads\": num_heads,\n                  \"dropout rate\": dropout_rate\n                  })\n    config = wandb.config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the model","metadata":{"id":"O783T1xMckzR"}},{"cell_type":"code","source":"model = SeTransformer(\n    num_blocks=num_blocks,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    vocab_size=vocab_size,\n    max_len=max_seq_len,\n    rate=dropout_rate,\n    pad_int=pad_int)\n\noptimizer = keras.optimizers.Adam(learning_rate, epsilon=keras.backend.epsilon())\nloss_func = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nacc = keras.metrics.SparseCategoricalAccuracy(dtype=f_type)\n\nmodel.compile(optimizer=optimizer,\n    loss=loss_func,\n    metrics=[acc]\n )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_input = tf.random.uniform((batch_size, max_seq_len), dtype=tf.int32, minval=5, maxval=6999)\ntemp_target = tf.random.uniform((batch_size, set_size), dtype=tf.int32, minval=5, maxval=6999)\ntemp_pred = model([temp_input, temp_target], False)\n\nparam_count: int = model.count_params()\nprint(f\"The model has {param_count:,} = {round(param_count * (10**-6), 1)}M trainable parameters\")\nrun.config[\"parameters\"] = param_count\n\n# stats = FlopCoKeras(model)\n# flops_per_call: int = stats.total_flops\n# macs_per_call: int = stats.total_macs\n\n# # (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * (3 for forward and backward pass) * (number of examples in dataset) \n# training_flops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n# print(f\"FLOPs per call: {flops_per_call:,} = {(flops_per_call * (10 ** -6)):,}M\")\n# print(f\"MACs per call: {macs_per_call:,} = {(macs_per_call * (10 ** -6)):,}M\")\ndel temp_input, temp_target, temp_pred","metadata":{"id":"-uAxXGBtkp4A","pycharm":{"name":"#%%\n"},"outputId":"898c89bb-118b-4c84-e398-4171a2904b42","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keras.utils.plot_model(\n#     model.build_graph(),\n#     show_shapes=True,\n#     show_dtype=True,\n#     show_layer_names=False,\n#     expand_nested=True,\n#     layer_range=None,\n#     show_layer_activations=True\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary(line_length=125, positions=[0.5, 0.66, 0.83, 1], expand_nested=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training helper functions","metadata":{"id":"5gjCRldwckzR"}},{"cell_type":"code","source":"@tf.function\ndef contains_pad(inp: tf.Tensor):\n    bool_ten = tf.math.equal(inp, pad_ten)\n    nonzero_count = tf.math.count_nonzero(bool_ten)\n    return nonzero_count > 0","metadata":{"id":"HZf9y_1nCtEa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{"id":"iRP0mGA1NjXi"}},{"cell_type":"code","source":"# @tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n#                               tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\ndef train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    with tf.GradientTape() as tape:\n        pred: tf.Tensor = model([inp, outp], training=True) \n        loss_val: tf.Tensor = loss_func(y_true = outp, y_pred = pred)\n    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    return tf.math.reduce_mean(loss_val), acc(outp, pred)","metadata":{"id":"UR6qYltHhh_t","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @tf.function\n# def test_train_step():\n#     exm_inp = tf.random.uniform(shape=[batch_size, 32], dtype=tf.int32, maxval=7000, minval=0)\n#     exm_inp2 = tf.random.uniform(shape=[batch_size, 64], dtype=tf.int32, maxval=7000, minval=0)\n#     exm_tar = tf.random.uniform(shape=[batch_size, set_size], dtype=tf.int32, maxval=7000, minval=0)\n#     train_step(exm_inp, exm_tar)\n#     train_step(exm_inp2, exm_tar)\n    \n# test_train_step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\ndef train(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=f_type):\n    per_gen_loss: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    per_gen_acc: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    i = 0\n    while i < num_sets:\n        # The input is of size set_size-TAKE_TO_ACCOUNT\n        already_predicted: int = i * (set_size + 1)\n        start_from: int = max(0, already_predicted - max_seq_len)\n        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n            break\n        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n        loss_val, acc_val = train_step(inp, outp)\n        one_hot_loss = tf.one_hot([i], num_sets, dtype = f_type) * loss_val\n        one_hot_acc = tf.one_hot([i], num_sets, dtype = f_type) * acc_val\n        per_gen_loss += one_hot_loss\n        per_gen_acc += one_hot_acc\n        i += 1\n    return tf.math.reduce_mean(per_gen_loss[:i]), tf.math.reduce_mean(per_gen_acc[:i])\n    ","metadata":{"id":"sSQ8nWVwMuG_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validate","metadata":{"id":"N9vu221UNjXk"}},{"cell_type":"code","source":"@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\ndef val_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    pred = model([inp, outp], training=False)\n    loss_val = loss_func(y_true = outp, y_pred = pred)\n    return tf.math.reduce_mean(loss_val), acc(outp, pred)","metadata":{"id":"gPYi_RT4Ytv_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\ndef validate(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=f_type):\n    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    per_gen_acc: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    i = 0\n    while i < num_sets:\n        # The input is of size set_size-TAKE_TO_ACCOUNT\n        already_predicted: int = i * (set_size + 1)\n        start_from: int = max(0, already_predicted - max_seq_len)\n        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n            break\n        loss_val, acc_val = val_step(inp, outp)\n        one_hot_loss = tf.one_hot([i], num_sets, dtype=f_type) * loss_val\n        one_hot_acc = tf.one_hot([i], num_sets, dtype=f_type) * acc_val\n        per_gen_loss += one_hot_loss\n        per_gen_acc += one_hot_acc\n        i += 1\n    return tf.math.reduce_mean(per_gen_loss[:i]), tf.math.reduce_mean(per_gen_acc[:i])","metadata":{"id":"7NpdkcUkckzT","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Callbacks","metadata":{"id":"cEcysTey1bEk"}},{"cell_type":"code","source":"best_loss = float(\"inf\")\nbest_model = None","metadata":{"id":"Iuce_A94ckzU","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_point(model: SeTransformer, train_loss, train_acc, val_loss, val_acc, test_loss = None, test_acc = None):\n    \"\"\"Saves the model at the end of each epoch\"\"\"\n    # (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * \n    # * (3 for forward and backward pass) * (number of examples in dataset)\n    global best_model\n    global last_save_time\n    last_save_time = time.time()\n    best_model = model\n    num_ops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n    keras.models.save_model(model = model, filepath = \"model.pb\", save_format='tf', overwrite=True)\n    wandb.log({\"model train loss\": train_loss, \"model train acc\": train_acc, \"model val loss\": val_loss, \"model val acc\": val_acc, \n              \"model test loss\": test_loss, \"model test acc\": test_acc})\n    art = wandb.Artifact(f\"{wandb.run.id}-best model\", type='my_model', description = f\"the model after {num_ops:,} operations\")\n    art.add_file(\"model.pb\")\n    run.log_artifact(artifact)\n    print(\"Saved checkpoint\")","metadata":{"id":"LZsEGz_vckzT","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def on_val_batch_end(train_loss: float, train_acc: float ,val_loss: float, val_acc: float) -> bool:\n    \"\"\"A callback after every val batch\n    returns True if the model should stop training and False else\"\"\"\n    global best_loss\n    global best_model\n    global last_save_time\n    if time.time() - last_save_time > 1800.0 and val_loss < math.log(vocab_size) and val_loss < best_loss:\n        best_loss = val_loss\n        # If the last save is more than a half hour (1800 sec) ago\n        # and if the predictions are better than randon and \n        check_point(model, train_loss, train_acc, val_loss, val_acc)\n        return False\n    elif train_loss < 0.01:\n        title: str = \"Over fitting or data leak\"\n        message = f\"Training loss is {train_loss} and val loss is {val_loss} in the latest batch\"\n        wandb.alert(title=title, text=message)\n        print(title)\n        print(message)\n        return True\n    elif time.time() - last_save_time > 18000.0 and train_loss >= math.log(vocab_size):\n        # if the prob of every token is 1/vocab_size, the loss is\n        # -ln(1/vocab_size) = ln(vocab_size) \n        # by the logrithem rule log(a^x)=xlog(a) where x = -1\n        # if after 5 hours of training, the model predictions are still random\n        title: str = \"Under fitting\"\n        message = f\"train loss: {train_loss} train acc: {train_acc}, val loss: {val_loss}, val acc: {val_acc} in the latest batch\"\n        wandb.alert(title=title, text=message)\n        print(title)\n        print(message)\n        return True\n    return Flase","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The actual training loop!","metadata":{"id":"RN8rmbrNW0-Z"}},{"cell_type":"code","source":"def train_loop():\n    epochs: int = 1000000  # Train until the cloud disconnects or the model stops improving\n    per_epoch_train_loss: List[float] = []\n    per_epoch_val_loss: List[float] = []\n    per_epoch_train_acc: List[float] = []\n    per_epoch_val_acc: List[float] = []\n    print(f\"number of train batches per epoch: {len(list_train_set)}\")\n    last_save_time = time.time()\n    for epoch in range(epochs):\n        print(f\"epoch number: {epoch}\")\n        per_batch_train_loss: List[float] = []\n        per_batch_val_loss: List[float] = []\n        per_batch_train_acc: List[float] = []\n        per_batch_val_acc: List[float] = []\n        for batch_num in tqdm.tqdm(range(len(list_train_set))):  # tqdm is a progress bar\n            train_loss, train_acc = train(list_train_set[batch_num])\n            float_train_loss = keras.backend.eval(train_loss).item()\n            per_batch_train_loss.append(float_train_loss)\n            float_train_acc = keras.backend.eval(train_acc).item()\n            per_batch_train_acc.append(float_train_acc)\n            wandb.log({\"epoch\": epoch, \"batch\": batch_num, \"batch train loss\": float_train_loss, \"batch train_acc\": float_train_acc})\n            if batch_num % 8 == 0:  # 8 = #training batches/#val batches\n                # because training set is 80% of the data and val set is 10%\n                next_val_batch: tf.Tensor = list_val_set[batch_num // 8]\n                val_loss, val_acc = validate(next_val_batch)\n                float_val_loss = keras.backend.eval(val_loss).item()\n                per_batch_val_loss.append(float_val_loss)\n                float_val_acc = keras.backend.eval(val_acc).item()\n                per_batch_val_acc.append(float_val_acc)\n                wandb.log({\"epoch\": epoch, \"batch\": batch_num, \"batch val loss\": float_val_loss, \"batch train_acc\": float_val_acc})\n                on_val_batch_end(float_train_loss, float_train_acc, float_val_loss, float_val_acc)\n        epoch_train_loss = statistics.mean(per_batch_train_loss)\n        epoch_train_acc = statistics.mean(per_batch_train_acc)\n        epoch_val_loss = statistics.mean(per_batch_val_loss)\n        epoch_val_acc = statistics.mean(per_batch_val_acc)\n        per_epoch_train_loss.append(epoch_train_loss)\n        per_epoch_train_acc.append(epoch_train_acc)\n        per_epoch_val_loss.append(epoch_val_loss)\n        per_epoch_val_acc.append(epoch_val_acc)\n        print(f\"train loss: {epoch_train_loss}\")\n        print(f\"train acc: {epoch_train_acc}\")\n        print(f\"val loss: {epoch_val_loss}\")\n        print(f\"val acc: {epoch_val_acc}\")\n        if len(per_epoch_val_loss) > 1:\n            if epoch_val_loss >= per_epoch_val_loss[-2]:\n                print(\"Validation loss increased. Stopped training\")\n                return epoch_train_loss, epoch_val_loss, epoch_train_acc, epoch_val_acc\n        print(\"Saved checkpoint\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export AUTOGRAPH_VERBOSITY=10\ntrain_loss, val_loss, train_acc, val_acc = train_loop()","metadata":{"id":"g5MukK3ynG7v","outputId":"dca87c06-0140-41d9-c5bd-1e71ea9c9289","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## After training","metadata":{"id":"Z-iopedNNjXs"}},{"cell_type":"code","source":"if best_model:\n    global model\n    model = tf.lite.TFLiteConverter.from_saved_model(\"model.pb\")\n    test_loss, test_acc = statistics.mean([validate(test_batch) for test_batch in tqdm.tqdm(list_test_set)])\n    print(f\"Test loss: {test_loss}\")\n    print(f\"Test acc: {test_acc}\")\n    wandb.log({\"test loss\": test_loss, \"text acc\": test_acc})","metadata":{"id":"1VtC8sP_NjXu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}